{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgXxoDhMAiti"
      },
      "source": [
        "# $ \\text{Video Killed The Radio Star}$ $\\color{red}{...Diffusion}$\n",
        "\n",
        "Notebook by David Marx ([@DigThatData](https://twitter.com/digthatdata))\n",
        "\n",
        "Shared under MIT license\n",
        "\n",
        "\n",
        "# $\\text{FAQ}$\n",
        "\n",
        "**What is this?**\n",
        "\n",
        "Point this notebook at a youtube url and it'll make a music video for you.\n",
        "\n",
        "**How does this animation technique work?**\n",
        "\n",
        "For each text prompt you provide, the notebook will...\n",
        "\n",
        "1. Generate an image based on that text prompt (using stable diffusion)\n",
        "2. Use the generated image as the `init_image` to recombine with the text prompt to generate variations similar to the first image. This produces a sequence of extremely similar images based on the original text prompt\n",
        "3. Images are then intelligently reordered to find the smoothest animation sequence of those frames\n",
        "3. This image sequence is then repeated to pad out the animation duration as needed\n",
        "\n",
        "The technique demonstrated in this notebook was inspired by a [video](https://www.youtube.com/watch?v=WJaxFbdjm8c) created by Ben Gillin.\n",
        "\n",
        "**How are lyrics transcribed?**\n",
        "\n",
        "This notebook uses openai's recently released 'whisper' model for performing automatic speech recognition. \n",
        "OpenAI was kind enough to offer several different sizes of this model which each have their own pros and cons. \n",
        "This notebook uses the largest whisper model for transcribing the actual lyrics. Additionally, we use the \n",
        "smallest model for performing the lyric segmentation. Neither of these models is perfect, but the results \n",
        "so far seem pretty decent.\n",
        "\n",
        "The first draft of this notebook relied on subtitles from youtube videos to determine timing, which was\n",
        "then aligned with user-provided lyrics. Youtube's automated captions are powerful and I'll update the\n",
        "notebook shortly to leverage those again, but for the time being we're just using whisper for everything\n",
        "and not referencing user-provided captions at all.\n",
        "\n",
        "**Something didn't work quite right in the transcription process. How do fix the timing or the actual lyrics?**\n",
        "\n",
        "The notebook is divided into several steps. Between each step, a \"storyboard\" file is updated. If you want to\n",
        "make modifications, you can edit this file directly and those edits should be reflected when you next load the\n",
        "file. Depending on what you changed and what step you run next, your changes may be ignored or even overwritten.\n",
        "Still playing with different solutions here.\n",
        "\n",
        "**Can I provide my own images to 'bring to life' and associate with certain lyrics/sequences?**\n",
        "\n",
        "Yes, you can! As described above: you just need to modify the storyboard. Will describe this functionality in\n",
        "greater detail after the implementation stabilizes a bit more.\n",
        "\n",
        "**This gave me an idea and I'd like to use just a part of your process here. What's the best way to reuse just some of the machinery you've developed here?**\n",
        "\n",
        "Most of the functionality in this notebook has been offloaded to library I published to pypi called `vktrs`. I strongly encourage you to import anything you need \n",
        "from there rather than cutting and pasting function into a notebook. Similarly, if you have ideas for improvements, please don't hesitate to submit a PR!\n",
        "\n",
        "**How can I support your work or work like it?**\n",
        "\n",
        "This notebook was made possible thanks to ongoing support from [stability.ai](https://stability.ai/). The best way to support my work is to share it with your friends, [report bugs](https://github.com/dmarx/video-killed-the-radio-star/issues/new), [suggest features](https://github.com/dmarx/video-killed-the-radio-star/discussions) or to donate to open source non-profits :) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM147HP4kAdY"
      },
      "source": [
        "## $0.$ Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZnTe8clZuZuj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title # ðŸ“Š Check GPU Status\n",
        "\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "\n",
        "def gpu_info():\n",
        "    outv = subprocess.run([\n",
        "        'nvidia-smi',\n",
        "            # these lines concatenate into a single query string\n",
        "            '--query-gpu='\n",
        "            'timestamp,'\n",
        "            'name,'\n",
        "            'utilization.gpu,'\n",
        "            'utilization.memory,'\n",
        "            'memory.used,'\n",
        "            'memory.free,'\n",
        "            ,\n",
        "        '--format=csv'\n",
        "        ],\n",
        "        stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "    header, rec = outv.split('\\n')[:-1]\n",
        "    return pd.DataFrame({' '.join(k.strip().split('.')).capitalize():v for k,v in zip(header.split(','), rec.split(','))}, index=[0]).T\n",
        "\n",
        "gpu_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OrHUOTwdgCfK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "\n",
        "# @title # ðŸ› ï¸ Setup: dependencies, imports, definitions\n",
        "\n",
        "# Install dependencies\n",
        "\n",
        "try: \n",
        "    import google.colab\n",
        "    local=False\n",
        "except:\n",
        "    local=True\n",
        "\n",
        "# TODO: pin versions\n",
        "    \n",
        "# local only additional dependencies\n",
        "if local:\n",
        "    %pip install pandas torch pillow beautifulsoup4 scipy toolz numpy lxml librosa scikit-learn\n",
        "\n",
        "# dependencies for both colab and local\n",
        "%pip install yt-dlp python-tsp stability-sdk[anim_ui] diffusers transformers ftfy accelerate omegaconf\n",
        "%pip install openai-whisper panel huggingface_hub ipywidgets safetensors keyframed demucs parse\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "#  Definitions and imports\n",
        "\n",
        "import copy\n",
        "import datetime as dt\n",
        "import gc\n",
        "import io\n",
        "from itertools import chain, cycle\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import subprocess\n",
        "from subprocess import Popen, PIPE\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from bokeh.models.widgets.tables import (\n",
        "    NumberFormatter, \n",
        "    BooleanFormatter,\n",
        "    CheckboxEditor,\n",
        ")\n",
        "from diffusers import (\n",
        "    StableDiffusionImg2ImgPipeline,\n",
        "    StableDiffusionPipeline,\n",
        ")\n",
        "from IPython.display import display\n",
        "import keyframed\n",
        "import keyframed as kf # TODO...\n",
        "import keyframed.dsl\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "import pandas as pd\n",
        "import panel as pn\n",
        "import parse\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from python_tsp.exact import solve_tsp_dynamic_programming\n",
        "from safetensors.numpy import save_file as save_safetensors\n",
        "from safetensors.numpy import load_file as load_safetensors\n",
        "import scipy\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "import sklearn.cluster\n",
        "import textwrap\n",
        "from tqdm.autonotebook import tqdm\n",
        "import torch\n",
        "from torch import autocast\n",
        "import whisper\n",
        "\n",
        "from stability_sdk.api import Context\n",
        "from stability_sdk.animation import AnimationArgs, Animator\n",
        "\n",
        "from stability_sdk.animation import (\n",
        "    AnimationArgs,\n",
        "    Animator,\n",
        "    AnimationSettings,\n",
        "    BasicSettings,\n",
        "    CoherenceSettings,\n",
        "    ColorSettings,\n",
        "    DepthSettings,\n",
        "    InpaintingSettings,\n",
        "    Rendering3dSettings,\n",
        "    CameraSettings,\n",
        "    VideoInputSettings,\n",
        "    VideoOutputSettings,\n",
        ")\n",
        "\n",
        "try: \n",
        "    import google.colab\n",
        "    local=False\n",
        "except:\n",
        "    local=True\n",
        "\n",
        "    \n",
        "def sanitize_folder_name(fp):\n",
        "    outv = ''\n",
        "    whitelist = string.ascii_letters + string.digits + '-_'\n",
        "    for token in str(fp):\n",
        "        if token not in whitelist:\n",
        "            token = '-'\n",
        "        outv += token\n",
        "    return outv\n",
        "\n",
        "# to do: is there a way to check if this is in the env already?\n",
        "#pn.extension('tabulator')\n",
        "\n",
        "\n",
        "def establish_workspace(\n",
        "    use_stability_api,\n",
        "    mount_gdrive,\n",
        "    application_name=\"VideoKilledTheRadioStar\",\n",
        "    active_project=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    This function constructs a local file called `config.yaml` that maintains state that will be used elsewhere.\n",
        "    It mostly sets the names of project folders and a handful of settings. The reason for doing things this way\n",
        "    is to facilitate \"resume\" functionality and creating new projects without overwriting previously created assets.\n",
        "\n",
        "    By convention, when loaded the config.yaml is referred to as the `workspace` object.\n",
        "    \n",
        "    Most project-specific content will be located in a project-specific config -- `storyboard.yaml` -- which should be\n",
        "    located in the folder path given by `workspace.project_root`. By convention, when loaded this is referred to as the\n",
        "    `storyboard` object. \n",
        "\n",
        "    If everything is set up correctly, you should be able to load the currently configured workspace and storyboard via:\n",
        "\n",
        "        workspace, storyboard = load_storyboard()\n",
        "    \"\"\"\n",
        "    # yeah... so... this shouldn't be necessary....\n",
        "    import os\n",
        "\n",
        "    # infer if we're on colab or not, since this impacts gdrive mounting\n",
        "    try: \n",
        "        import google.colab\n",
        "        local=False\n",
        "    except:\n",
        "        local=True\n",
        "\n",
        "    if local:\n",
        "        mount_gdrive=False\n",
        "\n",
        "    # Infer directory locations\n",
        "    os.environ['XDG_CACHE_HOME'] = os.environ.get(\n",
        "        'XDG_CACHE_HOME',\n",
        "        str(Path('~/.cache').expanduser())\n",
        "    )\n",
        "    if mount_gdrive:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        Path('/content/drive/MyDrive/AI/models/.cache/').mkdir(parents=True, exist_ok=True) \n",
        "        os.environ['XDG_CACHE_HOME']='/content/drive/MyDrive/AI/models/.cache'\n",
        "\n",
        "    model_dir_str=str(Path(os.environ['XDG_CACHE_HOME']))\n",
        "    proj_root_str = '${active_project}'\n",
        "    application_root = str(Path('.').absolute())\n",
        "    if mount_gdrive:\n",
        "        application_root = '/content/drive/MyDrive/AI/VideoKilledTheRadioStar'\n",
        "\n",
        "\n",
        "    # Build config file that defines the \"workspace\" abstraction\n",
        "    workspace = OmegaConf.create({\n",
        "        'active_project': active_project if active_project else str(time.time()),\n",
        "        'application_root':application_root,\n",
        "        'project_root':\"${application_root}/${active_project}\",\n",
        "        'shared_assets_root':\"${application_root}/shared_assets\",\n",
        "        'gdrive_mounted':mount_gdrive,\n",
        "        'use_stability_api':use_stability_api,\n",
        "        'model_dir':model_dir_str,\n",
        "        'output_dir':'${project_root}/frames'\n",
        "    })\n",
        "\n",
        "    Path(workspace.project_root).mkdir(parents=True, exist_ok=True)\n",
        "    Path(workspace.model_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(workspace.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ###################\n",
        "\n",
        "    # Assign tracking locations for A/V assets and generally useful outputs\n",
        "    \n",
        "    assets_dir = Path(workspace.shared_assets_root)\n",
        "    assets_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # TODO: yaml -> jsonl ?\n",
        "    video_assets_meta_fname = assets_dir / 'video_assets_meta.yaml'\n",
        "    if not video_assets_meta_fname.exists():\n",
        "        video_assets_meta = OmegaConf.create()\n",
        "        video_assets_meta.videos = []\n",
        "        with video_assets_meta_fname.open('w') as fp:\n",
        "            OmegaConf.save(config=video_assets_meta, f=fp.name)\n",
        "    else:\n",
        "        video_assets_meta = OmegaConf.load(video_assets_meta_fname)\n",
        "\n",
        "    audio_assets_meta_fname = assets_dir / 'audio_assets_meta.yaml'\n",
        "    if not audio_assets_meta_fname.exists():\n",
        "        audio_assets_meta = OmegaConf.create()\n",
        "        audio_assets_meta.content = []\n",
        "        with audio_assets_meta_fname.open('w') as fp:\n",
        "            OmegaConf.save(config=audio_assets_meta, f=fp.name)\n",
        "    else:\n",
        "        audio_assets_meta = OmegaConf.load(audio_assets_meta_fname)\n",
        "\n",
        "    ###################\n",
        "\n",
        "    # Request user provide credentials as needed\n",
        "\n",
        "    # if use_stability_api:\n",
        "    #     import os, getpass\n",
        "    #     if not os.environ.get('STABILITY_KEY'):\n",
        "    #         os.environ['STABILITY_KEY'] = getpass.getpass('Enter your Stability API Key, then press enter to continue')\n",
        "    # else:\n",
        "    #     # TODO: check for HF token in environment\n",
        "    #     if not local:\n",
        "    #         from google.colab import output\n",
        "    #         output.enable_custom_widget_manager()\n",
        "            \n",
        "    #     from huggingface_hub import notebook_login\n",
        "    #     notebook_login()\n",
        "    \n",
        "    ###################\n",
        "\n",
        "    with open('config.yaml','w') as fp:\n",
        "        OmegaConf.save(config=workspace, f=fp.name)\n",
        "\n",
        "    return workspace\n",
        "\n",
        "########################\n",
        "\n",
        "# wrap some of the loading logic for portability\n",
        "\n",
        "def save_storyboard(storyboard):\n",
        "    root = Path(load_workspace().project_root)\n",
        "    storyboard_fname = root / 'storyboard.yaml'\n",
        "    with open(storyboard_fname) as fp:\n",
        "        OmegaConf.save(config=storyboard, f=fp.name)\n",
        "\n",
        "def load_workspace():\n",
        "    return OmegaConf.load('config.yaml')\n",
        "        \n",
        "def load_storyboard():\n",
        "    workspace = load_workspace()\n",
        "    root = Path(workspace.project_root)\n",
        "    storyboard_fname = root / 'storyboard.yaml'\n",
        "    storyboard = OmegaConf.load(storyboard_fname)\n",
        "    return workspace, storyboard\n",
        "\n",
        "def load_audio_meta(workspace, storyboard):\n",
        "    assets_dir = Path(workspace.shared_assets_root)\n",
        "    audio_assets_meta_fname = assets_dir / 'audio_assets_meta.yaml'\n",
        "    audio_assets_meta = OmegaConf.load(audio_assets_meta_fname)\n",
        "    audio_meta=dict()\n",
        "    for idx, rec in enumerate(audio_assets_meta.content):\n",
        "        if rec.audio_fpath == storyboard.params.audio_fpath:\n",
        "            audio_meta = rec\n",
        "            break\n",
        "    return audio_meta\n",
        "\n",
        "\n",
        "\n",
        "#######################\n",
        "\n",
        "# EXTRA SEGMENTATION STUFF\n",
        "\n",
        "\n",
        "def calculate_interword_gaps(segment):\n",
        "    end_prev = -1\n",
        "    gaps = []\n",
        "    for word in segment['words']:\n",
        "        if end_prev < 0:\n",
        "            end_prev = word['end']\n",
        "            continue \n",
        "        gap = word['start'] - end_prev\n",
        "        gaps.append(gap)\n",
        "        end_prev = word['end']\n",
        "    return gaps \n",
        "\n",
        "def trivial_subsegmentation(segment, threshold=0, gaps=None):\n",
        "    \"\"\"\n",
        "    split on gaps in detected vocal activity. \n",
        "    Contiguity = gap between adjacent tokens is less than the input threshold.\n",
        "    \"\"\"\n",
        "    if gaps is None:\n",
        "        gaps = calculate_interword_gaps(seg)\n",
        "    out_segments = []\n",
        "    this_segment = [seg['words'][0]]\n",
        "    for word, preceding_pause in zip(seg['words'][1:], gaps):\n",
        "        if preceding_pause <= threshold:\n",
        "            this_segment.append(word)\n",
        "        else:\n",
        "            out_segments.append(this_segment)\n",
        "            this_segment = [word]\n",
        "    out_segments.append(this_segment)\n",
        "\n",
        "    outv = [dict(\n",
        "        start=seg[0]['start'],\n",
        "        end=seg[-1]['end'],\n",
        "        text=''.join([w['word'] for w in seg]).strip(),\n",
        "    ) for seg in out_segments]\n",
        "\n",
        "    return outv\n",
        "\n",
        "##############################################################\n",
        "\n",
        "# audio processing\n",
        "\n",
        "\n",
        "def analyze_audio_structure(\n",
        "    audio_fpath,\n",
        "    BINS_PER_OCTAVE = 12 * 3, # should be a multiple of twelve: https://github.com/MTG/essentia/blob/master/src/examples/python/tutorial_spectral_constantq-nsg.ipynb\n",
        "    N_OCTAVES = 7,\n",
        "):\n",
        "    \"\"\"\n",
        "    via librosa docs\n",
        "    https://librosa.org/doc/latest/auto_examples/plot_segmentation.html#sphx-glr-auto-examples-plot-segmentation-py\n",
        "    cites: McFee and Ellis, 2014 - https://brianmcfee.net/papers/ismir2014_spectral.pdf\n",
        "    \"\"\"\n",
        "    y, sr = librosa.load(audio_fpath)\n",
        "\n",
        "    C = librosa.amplitude_to_db(np.abs(librosa.cqt(y=y, sr=sr,\n",
        "                                            bins_per_octave=BINS_PER_OCTAVE,\n",
        "                                            n_bins=N_OCTAVES * BINS_PER_OCTAVE)),\n",
        "                                ref=np.max)\n",
        "\n",
        "    # reduce dimensionality via beat-synchronization\n",
        "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr, trim=False)\n",
        "    Csync = librosa.util.sync(C, beats, aggregate=np.median)\n",
        "    \n",
        "    # I have concerns about this frame fixing operation\n",
        "    beat_times = librosa.frames_to_time(librosa.util.fix_frames(beats, x_min=0), sr=sr)\n",
        "\n",
        "    # width=3 prevents links within the same bar \n",
        "    # mode=â€™affinityâ€™ here implements S_rep (after Eq. 8)\n",
        "    R = librosa.segment.recurrence_matrix(Csync, width=3, mode='affinity', sym=True)\n",
        "    # Enhance diagonals with a median filter (Equation 2)\n",
        "    df = librosa.segment.timelag_filter(scipy.ndimage.median_filter)\n",
        "    Rf = df(R, size=(1, 7))\n",
        "    # build the sequence matrix (S_loc) using mfcc-similarity\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "    Msync = librosa.util.sync(mfcc, beats)\n",
        "    path_distance = np.sum(np.diff(Msync, axis=1)**2, axis=0)\n",
        "    sigma = np.median(path_distance)\n",
        "    path_sim = np.exp(-path_distance / sigma)\n",
        "    R_path = np.diag(path_sim, k=1) + np.diag(path_sim, k=-1)\n",
        "    # compute the balanced combination\n",
        "    deg_path = np.sum(R_path, axis=1)\n",
        "    deg_rec = np.sum(Rf, axis=1)\n",
        "    mu = deg_path.dot(deg_path + deg_rec) / np.sum((deg_path + deg_rec)**2)\n",
        "    A = mu * Rf + (1 - mu) * R_path\n",
        "\n",
        "    # compute normalized laplacian and its spectrum\n",
        "    L = scipy.sparse.csgraph.laplacian(A, normed=True)\n",
        "    evals, evecs = scipy.linalg.eigh(L)\n",
        "    # clean this up with a median filter. can help smooth over discontinuities\n",
        "    evecs = scipy.ndimage.median_filter(evecs, size=(9, 1))\n",
        "    return dict(\n",
        "        y=y, \n",
        "        sr=np.array(sr).astype(np.uint32),\n",
        "        tempo=tempo,\n",
        "        beats=beats,\n",
        "        beat_times=beat_times,\n",
        "        evecs=evecs,\n",
        "    )\n",
        "\n",
        "    \n",
        "def laplacian_segmentation(\n",
        "    audio_fpath=None,\n",
        "    evecs=None,\n",
        "    n_clusters = 5,\n",
        "    n_spectral_features = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    segment audio by clustering a self-similarity matrix.\n",
        "    via librosa docs\n",
        "    https://librosa.org/doc/latest/auto_examples/plot_segmentation.html#sphx-glr-auto-examples-plot-segmentation-py\n",
        "    cites: McFee and Ellis, 2014 - https://brianmcfee.net/papers/ismir2014_spectral.pdf\n",
        "    \"\"\"\n",
        "    if evecs is None:\n",
        "        if audio_fpath is None:\n",
        "            raise Exception(\"One of `audio_fpath` or `evecs` must be provided\")\n",
        "        features = analyze_audio_structure(audio_fpath)\n",
        "        evecs = features['evecs']\n",
        "        \n",
        "    if n_clusters < 2:\n",
        "        seg_ids = np.zeros(evecs.shape[0], dtype=int)\n",
        "        return seg_ids\n",
        "    \n",
        "    if n_spectral_features is None:\n",
        "        n_spectral_features = n_clusters\n",
        "\n",
        "    # cumulative normalization is needed for symmetric normalize laplacian eigenvectors\n",
        "    Cnorm = np.cumsum(evecs**2, axis=1)**0.5\n",
        "    k = n_spectral_features\n",
        "    X = evecs[:, :k] / Cnorm[:, k-1:k]\n",
        "\n",
        "\n",
        "    # use these k components to cluster beats into segments\n",
        "    KM = sklearn.cluster.KMeans(n_clusters=n_clusters, n_init=\"auto\")\n",
        "    seg_ids = KM.fit_predict(X)\n",
        "\n",
        "    return seg_ids #, beat_times, tempo\n",
        "\n",
        "\n",
        "# for video duration\n",
        "def get_audio_duration_seconds(audio_fpath):\n",
        "    outv = subprocess.run([\n",
        "        'ffprobe'\n",
        "        ,'-i',audio_fpath\n",
        "        ,'-show_entries', 'format=duration'\n",
        "        ,'-v','quiet'\n",
        "        ,'-of','csv=p=0'\n",
        "        ],\n",
        "        stdout=subprocess.PIPE\n",
        "        ).stdout.decode('utf-8')\n",
        "    return float(outv.strip())\n",
        "\n",
        "\n",
        "##########################################\n",
        "\n",
        "# animation stuff\n",
        "\n",
        "# TODO: update this stuff to reflect updates to API/sdk\n",
        "def get_image_for_prompt_sai(prompt, max_retries=5, **kargs):\n",
        "    stability_api = client.StabilityInference(\n",
        "        key=os.environ['STABILITY_KEY'], \n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    # auto-retry if mitigation triggered\n",
        "    while max_retries:\n",
        "        try:\n",
        "            answers = stability_api.generate(prompt=prompt, **kargs)\n",
        "            response = process_response(answers)\n",
        "            for img in response:\n",
        "                yield img\n",
        "            break\n",
        "\n",
        "        # TODO: better regen handling\n",
        "        except RuntimeError:\n",
        "            print(\"runtime error\")\n",
        "            max_retries -= 1\n",
        "            warnings.warn(f\"mitigation triggered, retries remaining: {max_retries}\")\n",
        "\n",
        "def process_response(answers):\n",
        "    for resp in answers:\n",
        "        for artifact in resp.artifacts:\n",
        "            if artifact.finish_reason == generation.FILTER:\n",
        "                warnings.warn(\n",
        "                    \"Your request activated the API's safety filters and could not be processed.\"\n",
        "                    \"Please modify the prompt and try again.\")\n",
        "                raise RuntimeError\n",
        "            if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "                img = Image.open(io.BytesIO(artifact.binary))\n",
        "                yield img\n",
        "\n",
        "\n",
        "########################################\n",
        "\n",
        "# misc utils\n",
        "\n",
        "def rand_str(n_char=5):\n",
        "    return ''.join(random.choice(string.ascii_lowercase) for i in range(n_char))\n",
        "\n",
        "def save_frame(\n",
        "    img: Image,\n",
        "    idx:int=0,\n",
        "    root_path=Path('./frames'),\n",
        "    name=None,\n",
        "):\n",
        "    root_path.mkdir(parents=True, exist_ok=True)\n",
        "    if name is None:\n",
        "        name = rand_str()\n",
        "    outpath = root_path / f\"{idx}-{name}.png\"\n",
        "    img.save(outpath)\n",
        "    return str(outpath)\n",
        "\n",
        "def get_image_sequence(idx, root, init_first=True):\n",
        "    root = Path(root)\n",
        "    images = (root / 'frames' ).glob(f'{idx}-*.png')\n",
        "    images = [str(fp) for fp in images]\n",
        "    if init_first:\n",
        "        init_image = None\n",
        "        images2 = []\n",
        "        for i, fp in enumerate(images):\n",
        "            if 'anchor' in fp:\n",
        "                init_image = fp\n",
        "            else:\n",
        "                images2.append(fp)\n",
        "        if not init_image:\n",
        "            try:\n",
        "                init_image, images2 = images2[0], images2[1:]\n",
        "                images = [init_image] + images2\n",
        "            except IndexError:\n",
        "                images = images2\n",
        "    return images\n",
        "\n",
        "def archive_images(idx, root, archive_root = None):\n",
        "    root = Path(root)\n",
        "    if archive_root is None:\n",
        "        archive_root = root / 'archive'\n",
        "    archive_root = Path(archive_root)\n",
        "    archive_root.mkdir(parents=True, exist_ok=True)\n",
        "    old_images = get_image_sequence(idx, root=root)\n",
        "    if not old_images:\n",
        "        return\n",
        "    print(f\"moving {len(old_images)} old images for scene {idx} to {archive_root}\")\n",
        "    for old_fp in old_images:\n",
        "        old_fp = Path(old_fp)\n",
        "        im_name = Path(old_fp.name)\n",
        "        new_path = archive_root / im_name\n",
        "        if new_path.exists():\n",
        "            im_name = f\"{im_name.stem}-{time.time()}{im_name.suffix}\"\n",
        "            new_path = archive_root / im_name\n",
        "        old_fp.rename(new_path)\n",
        "\n",
        "\n",
        "############################\n",
        "\n",
        "# video compilation stuff\n",
        "\n",
        "# TODO: Sorting algorithm that can tolerate more than 15-ish frames (GPU?)\n",
        "def tsp_sort(frames):\n",
        "    frames_m = np.array([np.array(f).ravel() for f in frames])\n",
        "    dmat = pdist(frames_m, metric='cosine')\n",
        "    dmat = squareform(dmat)\n",
        "    permutation, _ = solve_tsp_dynamic_programming(dmat)\n",
        "    return permutation\n",
        "\n",
        "def add_caption2image(\n",
        "      image, \n",
        "      caption, \n",
        "      text_font='LiberationSans-Regular.ttf', \n",
        "      font_size=20,\n",
        "      fill_color=(255, 255, 255),\n",
        "      stroke_color=(0, 0, 0), #stroke_fill\n",
        "      stroke_width=2,\n",
        "      align='center',\n",
        "      ):\n",
        "    # via https://stackoverflow.com/a/59104505/819544\n",
        "    wrapper = textwrap.TextWrapper(width=50) \n",
        "    word_list = wrapper.wrap(text=caption) \n",
        "    caption_new = ''\n",
        "    for ii in word_list[:-1]:\n",
        "        caption_new = caption_new + ii + '\\n'\n",
        "    caption_new += word_list[-1]\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Download the Font and Replace the font with the font file. \n",
        "    font = ImageFont.truetype(text_font, size=font_size)\n",
        "    w,h = draw.textsize(caption_new, font=font, stroke_width=stroke_width)\n",
        "    W,H = image.size\n",
        "    x,y = 0.5*(W-w),0.90*H-h\n",
        "    draw.text(\n",
        "        (x,y), \n",
        "        caption_new,\n",
        "        font=font,\n",
        "        fill=fill_color, \n",
        "        stroke_fill=stroke_color,\n",
        "        stroke_width=stroke_width,\n",
        "        align=align,\n",
        "    )\n",
        "\n",
        "    return image\n",
        "\n",
        "##########################################################\n",
        "\n",
        "# audioreactivity stuff\n",
        "\n",
        "\n",
        "def full_width_plot():\n",
        "    ax = plt.gca()\n",
        "    ax.figure.set_figwidth(20)\n",
        "    plt.show()\n",
        "\n",
        "def display_signal(y, sr, show_spec=True, title=None):\n",
        "    \n",
        "#     if show_spec:\n",
        "#         frame_time = librosa.samples_to_time(np.arange(len(normalized_signal)), sr=sr)\n",
        "#     else:\n",
        "#         frame_time = librosa.frames_to_time(np.arange(len(normalized_signal)), sr=sr)\n",
        "    \n",
        "    if show_spec:\n",
        "        librosa.display.waveshow(y, sr=sr)\n",
        "        if title:\n",
        "            plt.title(title)\n",
        "        full_width_plot()\n",
        "        \n",
        "        try:\n",
        "            times = librosa.times_like(y, sr=sr)\n",
        "            M = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "            librosa.display.specshow(librosa.power_to_db(M, ref=np.max),\n",
        "                             y_axis='mel', x_axis='time')\n",
        "            full_width_plot()\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "    else:\n",
        "        frame_time = librosa.frames_to_time(np.arange(len(y)), sr=sr)\n",
        "        plt.plot(frame_time, y)\n",
        "        if title:\n",
        "            plt.title(title)\n",
        "        full_width_plot()\n",
        "\n",
        "\n",
        "# https://github.com/pytti-tools/pytti-core/blob/9e8568365cfdc123d2d2fbc20d676ca0f8715341/src/pytti/AudioParse.py#L95\n",
        "from scipy.signal import butter, sosfilt, sosfreqz\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    sos = butter(order, [low, high], analog=False, btype='bandpass', output='sos')\n",
        "    return sos\n",
        "\n",
        "def butter_bandpass_filter(y, sr, lowcut, highcut, order=10):\n",
        "    sos = butter_bandpass(lowcut, highcut, sr, order=order)\n",
        "    y = sosfilt(sos, y)\n",
        "    return y\n",
        "\n",
        "########################################################################\n",
        "\n",
        "\n",
        "def show_storyboard(storyboard=None):\n",
        "    if storyboard is None:\n",
        "        workspace, storyboard = load_storyboard()\n",
        "    reactive_signal_map = {}\n",
        "    if storyboard.get('audioreactive'):\n",
        "        reactive_signal_map = storyboard.audioreactive.get('reactive_signal_map')\n",
        "        \n",
        "    for idx, rec in enumerate(storyboard.prompt_starts):\n",
        "        report = f\"scene: {idx}\\t start: {rec['start']:.2f}\" \n",
        "        if rec.get('duration_'):\n",
        "            report += f\"\\t duration: {rec.get('duration_'):.2f}\"\n",
        "        report += f\"\\nspoken text: {rec.get('text')}\\n\"\n",
        "        \n",
        "        # TODO: wrap prompt construction logic in a function (better yet use omegaconf substitution variables)\n",
        "        \n",
        "        #'_theme':'theme', 'structural_segmentation_label':\n",
        "        if rec.get('_theme'):\n",
        "            report += f\"theme prompt: {rec['_theme']}\\n\"\n",
        "        #f\"image prompt: {rec['_prompt']}\\n\"\n",
        "        prompt = rec.get('prompt')\n",
        "        #if not prompt:\n",
        "        #    prompt = ...\n",
        "        if prompt:\n",
        "            report += f\"image prompt: {rec['_prompt']}\\n\"\n",
        "        \n",
        "        if rec.get('animation_mode'):\n",
        "            report += f\"animation mode: {rec['animation_mode']}\"\n",
        "        print(report)\n",
        "        im_path = rec.get('frame0_fpath')\n",
        "        if im_path and Path(im_path).exists():\n",
        "            display(Image.open(rec['frame0_fpath']))\n",
        "\n",
        "        if reactive_signal_map:\n",
        "            n = rec['frames']\n",
        "            if n <1:\n",
        "                continue\n",
        "            for signal_name in reactive_signal_map.keys():\n",
        "                if signal_name in rec:\n",
        "                    curve = kf.dsl.curve_from_cn_string(rec[signal_name])\n",
        "                    xs = [i for i in range(n)]\n",
        "                    ys = [curve[i] for i in xs]\n",
        "                    plt.plot(xs, ys, label=signal_name)\n",
        "                plt.title(f\"scene {idx}\")\n",
        "                plt.xlabel(\"frame index within scene\")\n",
        "                plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "#########################################\n",
        "\n",
        "def get_path_to_stems():\n",
        "    workspace, storyboard = load_storyboard()\n",
        "    assets_root = Path(workspace.application_root) / 'shared_assets'\n",
        "    #stems_path = root / \"stems\"\n",
        "    stems_path = assets_root / \"stems\"\n",
        "    stems_outpath = stems_path / 'htdemucs_ft' / Path(storyboard.params.audio_fpath).stem\n",
        "    return stems_outpath\n",
        "\n",
        "def ensure_stems_separated():\n",
        "    stems_outpath = get_path_to_stems()\n",
        "    stems_path = str(stems_outpath.parent.parent)\n",
        "    if not stems_outpath.exists():    \n",
        "        !demucs -n htdemucs_ft -o \"{stems_path}\" \"{storyboard.params.audio_fpath}\"\n",
        "\n",
        "def get_stem(instrument_name):\n",
        "    ensure_stems_separated()\n",
        "    stems_outpath = get_path_to_stems()\n",
        "    stem_fpaths  = list(stems_outpath.glob('*.wav'))\n",
        "\n",
        "    for stem_fpath in stem_fpaths:\n",
        "        if instrument_name in str(stem_fpath):\n",
        "            y, sr = librosa.load(stem_fpath)\n",
        "            return y, sr\n",
        "    raise ValueError(\n",
        "        f\"Unable to locate stem for instrument: {instrument_name}\\n\"\n",
        "        f\"in folder: {stems_outpath}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cM8cux9b7F4v",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title # ðŸ“‹ Attach Storyboard (create or resume project)\n",
        "\n",
        "# @markdown Depending on your settings and environment, running this cell may prompt you to enter one or more API Keys below. \n",
        "# @markdown Don't forget to press \"enter\" after providing a requested key.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "use_stability_api = False # @param {type:'boolean'}\n",
        "mount_gdrive = True # @param {type:'boolean'}\n",
        "resume=True # @param {type:'boolean'}\n",
        "\n",
        "# TODO: add support for whisper API\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "\n",
        "# @markdown To create a new project, enter a unique project name.\n",
        "# @markdown If you leave `project_name` blank, the current unix timestamp will be used\n",
        "# @markdown  (seconds since 1970-01-01 00:00).\n",
        "\n",
        "# @markdown If you use the name of an existing project, the workspace will switch to that project.\n",
        "\n",
        "# @markdown Non-alphanumeric characters (excluding '-' and '_') will be replaced with hyphens.\n",
        "\n",
        "project_name = '' # @param {type:'string'}\n",
        "\n",
        "##########################\n",
        "\n",
        "try: \n",
        "    import google.colab\n",
        "    local=False\n",
        "except:\n",
        "    local=True\n",
        "\n",
        "if local:\n",
        "    mount_gdrive=False\n",
        "\n",
        "\n",
        "##################################################################\n",
        "\n",
        "resuming = False\n",
        "if resume:\n",
        "    try:\n",
        "        workspace, storyboard = load_storyboard()\n",
        "        print(\"loading storyboard\")\n",
        "        resuming=True\n",
        "    except:\n",
        "        resuming = False\n",
        "\n",
        "if not resuming:\n",
        "    if not project_name:\n",
        "        project_name = str(time.time())\n",
        "    project_name = sanitize_folder_name(project_name)\n",
        "    \n",
        "    print(\"creating workspace\")\n",
        "\n",
        "    workspace = establish_workspace(\n",
        "        use_stability_api=use_stability_api,\n",
        "        mount_gdrive=mount_gdrive,\n",
        "        application_name=\"VideoKilledTheRadioStar\",\n",
        "        active_project=project_name,\n",
        "    )\n",
        "\n",
        "    print(\"creating new storyboard\")\n",
        "    storyboard = OmegaConf.create()\n",
        "    storyboard.params = {}\n",
        "\n",
        "if workspace.use_stability_api:\n",
        "    import os, getpass\n",
        "    if not os.environ.get('STABILITY_KEY'):\n",
        "        os.environ['STABILITY_KEY'] = getpass.getpass('Enter your Stability API Key, then press enter to continue')\n",
        "else:\n",
        "    # TODO: check for HF token in environment\n",
        "    if not local:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "        \n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()\n",
        "\n",
        "##################################################################\n",
        "\n",
        "root = Path(workspace.project_root)\n",
        "\n",
        "assets_dir = Path(workspace.shared_assets_root)\n",
        "\n",
        "video_assets_meta_fname = assets_dir / 'video_assets_meta.yaml'\n",
        "audio_assets_meta_fname = assets_dir / 'audio_assets_meta.yaml'\n",
        "\n",
        "video_assets_meta = OmegaConf.load( assets_dir/'video_assets_meta.yaml' )\n",
        "audio_assets_meta = OmegaConf.load( assets_dir/'audio_assets_meta.yaml' )\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SyIJRMhEgCfL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Set Audio Source\n",
        "\n",
        "d_ = dict(\n",
        "    # all the underscore does is make it so each of the following lines can be preceded with a comma\n",
        "    # otw the first parameter would be offset from the other in the colab form\n",
        "    _=\"\"\n",
        "\n",
        "    , video_url = 'https://www.youtube.com/watch?v=4XNXLasyAKE' # @param {type:'string'}\n",
        "    , audio_fpath = '' # @param {type:'string'}\n",
        ")\n",
        "d_.pop('_')\n",
        "\n",
        "# @markdown `video_url` - URL of a youtube video to download as a source for audio and potentially for text transcription as well.\n",
        "\n",
        "# @markdown `audio_fpath` - Optionally provide an audio file instead of relying on a youtube download. Name it something other than 'audio.mp3', \n",
        "# @markdown                 otherwise it might get overwritten accidentally.\n",
        "\n",
        "\n",
        "storyboard.params = d_\n",
        "\n",
        "storyboard_fname = root / 'storyboard.yaml'\n",
        "with open(storyboard_fname,'wb') as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)\n",
        "    \n",
        "\n",
        "###############################\n",
        "# Download audio from youtube #\n",
        "###############################\n",
        "\n",
        "# this should modify the existing record for the URL rather than creating a new one...\n",
        "force_redownload=False\n",
        "\n",
        "video_url = storyboard.params.video_url\n",
        "download_video=True\n",
        "\n",
        "if not force_redownload:\n",
        "    for rec in video_assets_meta.videos:\n",
        "        if rec.video_url == video_url:\n",
        "            if rec.get('video_fpath'):\n",
        "                print(\"previously downloaded video detected\")\n",
        "                download_video=False\n",
        "                # populate storyboard with previous processing results\n",
        "                if rec.get('audio_fpath'):\n",
        "                    storyboard.params.audio_fpath = rec.get('audio_fpath')\n",
        "            else:\n",
        "                download_video=True # should be redundant?\n",
        "            break\n",
        "            \n",
        "\n",
        "\n",
        "if download_video:\n",
        "    # check if user provided an audio filepath (or we already have one from youtube) before attempting to download\n",
        "    video_assets_meta_record = {}\n",
        "    video_assets_meta_record['video_url'] = video_url\n",
        "\n",
        "    ytdl_prefix = \"DOWNLOADED__\"\n",
        "    ytdl_fname = f\"{str(assets_dir / ytdl_prefix)}%(title)s.%(ext)s\"\n",
        "\n",
        "    !yt-dlp -o \"{ytdl_fname}\" {video_url}\n",
        "\n",
        "    matched_files = assets_dir.glob(ytdl_prefix+\"*\")\n",
        "    most_recent_file = max(matched_files, key=os.path.getctime)\n",
        "    print(f\"downloaded: {most_recent_file}\")\n",
        "    ytdl_fname = most_recent_file\n",
        "\n",
        "    video_assets_meta_record['video_fpath'] = str(ytdl_fname.absolute())\n",
        "    \n",
        "    # TODO: right here, we should be adding this to the audio meta\n",
        "    audio_fpath = ytdl_fname.with_suffix('.m4a')\n",
        "    input_audio = ytdl_fname\n",
        "    !ffmpeg -y -i \"{input_audio}\" -vn -c:a aac \"{audio_fpath}\"\n",
        "\n",
        "    storyboard.params.audio_fpath = audio_fpath\n",
        "    video_assets_meta_record['audio_fpath'] = str(audio_fpath.absolute())\n",
        "    \n",
        "    video_assets_meta.videos.append(video_assets_meta_record)\n",
        "\n",
        "\n",
        "save_storyboard(storyboard)\n",
        "\n",
        "with open(video_assets_meta_fname, 'wb') as fp:\n",
        "    OmegaConf.save(config=video_assets_meta, f=fp.name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9zT0u4-q_fMF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @markdown # ðŸ”Š ðŸ’¬ Audio Processing\n",
        "\n",
        "# @markdown * Transcribe and segment speech using whisper\n",
        "\n",
        "# @markdown If this audio source has been previoussly processed by this notebook, that should be detected processing won't be repeated.\n",
        "\n",
        "##################################################\n",
        "# ðŸ’¬ Transcribe and segment speech using whisper #\n",
        "##################################################\n",
        "\n",
        "# TODO: combine this with previous cell, doesn't need to be separate step.\n",
        "\n",
        "#audio_fpath = Path(storyboard.params.audio_fpath)\n",
        "audio_fpath = str(storyboard.params.audio_fpath)\n",
        "\n",
        "separate_stems = True # @param {type:'boolean'}\n",
        "\n",
        "force_retranscription = False # @param {type:'boolean'}\n",
        "override_storyboard_transcription = False\n",
        "\n",
        "whisper_seg = None\n",
        "#audio_meta={}\n",
        "\n",
        "# this is silly.\n",
        "# for audio_meta in audio_assets_meta.content:\n",
        "#     #if (audio_fpath) and (audio_meta.audio_fpath == audio_fpath):\n",
        "#     if (audio_fpath) and (audio_meta.audio_fpath == audio_fpath):\n",
        "\n",
        "if audio_fpath in audio_assets_meta:\n",
        "    print(\"previously processed audio detected\")\n",
        "    # TODO: add processed audio structure features/stems as well\n",
        "    audio_meta = audio_assets_meta[audio_fpath]\n",
        "else:\n",
        "    audio_assets_meta[audio_fpath] = {}\n",
        "    audio_meta = audio_assets_meta[audio_fpath]\n",
        "\n",
        "if (not force_retranscription) and audio_meta.get('whisper_segmentation') and Path(audio_meta.whisper_segmentation).exists():\n",
        "    print(\"Using pre-existing whisper transcription\")\n",
        "    whisper_seg_fpath = Path(audio_meta.whisper_segmentation)\n",
        "    with whisper_seg_fpath.open() as f:\n",
        "        timings = json.load(f)\n",
        "    whisper_seg = timings['segments']\n",
        "\n",
        "\n",
        "if force_retranscription or (whisper_seg is None):\n",
        "    print(\"Transcribing...\")\n",
        "    #audio_meta['audio_fpath'] = storyboard.params.audio_fpath redundant\n",
        "    # outputs text files as audio.* locally\n",
        "    !whisper --model large --word_timestamps True -o {str(assets_dir)} \"{storyboard.params.audio_fpath}\"\n",
        "\n",
        "    whisper_seg_fpath = Path(storyboard.params.audio_fpath).with_suffix('.json')\n",
        "    audio_meta['whisper_segmentation'] = str(whisper_seg_fpath)\n",
        "    audio_meta['duration'] = get_audio_duration_seconds(audio_fpath)\n",
        "    \n",
        "    with whisper_seg_fpath.open() as f:\n",
        "        timings = json.load(f)\n",
        "    whisper_seg = timings['segments']\n",
        "    \n",
        "    audio_assets_meta[audio_fpath] = audio_meta\n",
        "    with open(audio_assets_meta_fname, 'wb') as fp:\n",
        "        OmegaConf.save(config=audio_assets_meta, f=fp.name)\n",
        "    \n",
        "if not storyboard.get('prompt_starts') or override_storyboard_transcription:\n",
        "    \n",
        "    # Ta da!\n",
        "    #storyboard.prompt_starts = whisper_seg\n",
        "    storyboard.prompt_starts = [{k:rec[k] for k in ('start','end','text')} for rec in whisper_seg]\n",
        "    \n",
        "    storyboard.params['video_duration'] = audio_meta['duration']\n",
        "    # unsure if below method is reliable.\n",
        "    #storyboard.params['video_duration'] = storyboard.prompt_starts[-1]['end']\n",
        "\n",
        "\n",
        "### checkpoint the processing work we've done to this point\n",
        "\n",
        "\n",
        "# TODO: enforce scene zero starts at t=0 and last scene ends at duration\n",
        "\n",
        "#storyboard.prompt_starts = prompt_starts\n",
        "save_storyboard(storyboard)\n",
        "\n",
        "# Music Structure Analysis\n",
        "\n",
        "# Music Structure Analysis\n",
        "# - beat and tempo detection\n",
        "# - Self-similarity graph\n",
        "\n",
        "# TODO: detect previously processed audio\n",
        "\n",
        "# TODO: move stem separation here?\n",
        "\n",
        "# TODO: no decisions here, combine with other cells\n",
        "\n",
        "audio_structure_features = analyze_audio_structure(audio_fpath=storyboard.params.audio_fpath)\n",
        "\n",
        "audio_features_fpath = Path(storyboard.params.audio_fpath).with_suffix('.audio_features.safetensors')\n",
        "save_safetensors(audio_structure_features, audio_features_fpath)\n",
        "\n",
        "# TODO: fix inconsistent variable naming\n",
        "structural_features = audio_structure_features\n",
        "\n",
        "# TODO: un-\"PosixPath\" `storyboard.params.audio_fpath`\n",
        "audio_assets_meta[str(storyboard.params.audio_fpath)]['structural_features'] = audio_features_fpath\n",
        "\n",
        "\n",
        "with open(audio_assets_meta_fname, 'wb') as fp:\n",
        "    OmegaConf.save(config=audio_assets_meta, f=fp.name)\n",
        "\n",
        "if separate_stems:\n",
        "  ensure_stems_separated()\n",
        "\n",
        "# TODO: ~PLACEHOLDER~ Give user opportunity to correct the transcription at shared_asset level rather than project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B3gw1cYNgCfL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Subdivide Unusually Long Scenes\n",
        "\n",
        "# @markdown Break up \"outlier\" segments into smaller chunks\n",
        "\n",
        "# TODO: wrap these steps in functions for legibility/portability\n",
        "# TODO: make this threshold parameterizable via storyboard (and save analysis to storyboard)\n",
        "# TODO: use beat counts to estimate a smart scene duration\n",
        "\n",
        "subdivide_long_scenes = False # @param {'type':'boolean'}\n",
        "\n",
        "#TODO: merge short scenes\n",
        "\n",
        "# TODO: expose this to colab\n",
        "threshold_duration = None \n",
        "\n",
        "######################################################\n",
        "# estimate parameters of scene duration distribution #\n",
        "######################################################\n",
        "\n",
        "# TODO: use beat onsets/counts\n",
        "scene_durations = []\n",
        "scenes_ = []\n",
        "for idx, rec in enumerate(storyboard.prompt_starts):\n",
        "    rec=dict(rec)\n",
        "    if idx > 0:\n",
        "        # are we maybe doubling up 'start' time stamps? like there are more unique 'end's than 'start's?\n",
        "        duration = rec['start'] - prev['start']\n",
        "        prev['duration_'] = duration\n",
        "        scene_durations.append(duration)\n",
        "    prev = rec\n",
        "    scenes_.append(prev)\n",
        "    \n",
        "# handle last record\n",
        "else:\n",
        "    # here's the bug.\n",
        "    # TODO: swap out rec['end'] -> rec_prev['start'] here\n",
        "    rec['duration_'] = rec['end'] - rec['start']\n",
        "    scenes_.append(rec)\n",
        "\n",
        "\n",
        "mu = sum(scene_durations)/len(scene_durations)\n",
        "sigma = np.std(scene_durations)\n",
        "\n",
        "# 1sd filter to concentrate on mode\n",
        "scene_durations2 = [s for s in scene_durations if (mu - sigma) < s < (mu+sigma)]\n",
        "mu2 = sum(scene_durations2)/len(scene_durations2)\n",
        "sigma2 = np.std(scene_durations2)\n",
        "\n",
        "###########\n",
        "\n",
        "\n",
        "\n",
        "# break up \"outlier\" segments into smaller chunks\n",
        "# this heuristic could be improved with beat synchronization and onset detection.\n",
        "# ... also could probably leverage the 'end' time of the scene\n",
        "# TODO: hierarchical theme structure analysis\n",
        "# TODO: MSA segmentation for fully instrumental (i.e. arbitrary) audio\n",
        "\n",
        "if subdivide_long_scenes:\n",
        "\n",
        "    threshhold = mu2 + sigma\n",
        "    if threshold_duration is not None:\n",
        "        threshhold = threshold_duration\n",
        "\n",
        "    scenes = []\n",
        "    #for rec in storyboard.prompt_starts:\n",
        "    for rec in list(scenes_):\n",
        "        gap_remaining = rec['duration_']\n",
        "        while gap_remaining > threshhold:\n",
        "            #step = min(max(mu2-sigma, (np.random.normal() + mu2)*sigma), mu2+sigma)\n",
        "            step=mu2\n",
        "            step = float(step)\n",
        "            # TODO: move duration computation somewhere that it will happen necessarily\n",
        "            rec['duration_'] = step\n",
        "            new_rec = copy.deepcopy(rec)\n",
        "            new_rec['start'] = rec['start'] + step\n",
        "            # TODO: deal with new value here\n",
        "            #new_rec['end'] = ??\n",
        "            new_rec['duration_'] = step\n",
        "            ### maybe i could add a flag or something to clarify that this was an \"inferred\" subscene\n",
        "            #new_rec['parent_scene'] = rec.get('uid') # something like this?\n",
        "            new_rec['inferred_subscene'] = True # or this?\n",
        "            \n",
        "            scenes.append(rec)\n",
        "            rec = new_rec\n",
        "            gap_remaining -= step\n",
        "        scenes.append(rec)\n",
        "\n",
        "    storyboard.prompt_starts = scenes\n",
        "\n",
        "# TODO: get last scene \"end\" from story duration\n",
        "\n",
        "# TODO: compute duration regardless of extra segmentation\n",
        "\n",
        "# TODO: add scene indices back to rec's to facilitate editing the text file\n",
        "    \n",
        "save_storyboard(storyboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SB0X2mXGgCfL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Theme -> Scene Assignment\n",
        "\n",
        "# @markdown `theme_prompt` - Text that will be appended to the end of each lyric, useful for e.g. applying a consistent aesthetic style.\n",
        "# @markdown  To provide multiple themes (only one will be used per scene), separate theme prompts with the `|` (pipe) symbol.\n",
        "\n",
        "# @markdown `infer_thematic_structure` - if False, themes will be rotated sequentially such that no two adjacent frames\n",
        "# @markdown will use the same theme prompt (if multiple theme prompts were provided). If True, song structure analysis will cluster related scenes into as many groups as there are theme prompts to attempt to associate a visual themes with respective musical themes.\n",
        "\n",
        "# @markdown The analysis runs quick. If you didn't understand that explanation, just try it both ways and you'll probably get the idea.\n",
        "\n",
        "theme_prompt = 'Katsuhiro Otomo gundam mecha ' # @param {type:'string'}\n",
        "\n",
        "infer_thematic_structure = True # @param {type:'boolean'}\n",
        "\n",
        "storyboard.params.theme_prompt = theme_prompt\n",
        "themes = [prompt.strip() for prompt in theme_prompt.split('|') if prompt.strip()]\n",
        "\n",
        "if (len(themes) > 1):\n",
        "    if infer_thematic_structure:\n",
        "                \n",
        "        # audio_assets_meta[storyboard.params.audio_fpath]['structural_features']\n",
        "        beat_times = audio_structure_features['beat_times']\n",
        "        evecs = audio_structure_features['evecs']\n",
        "        segment_labels = laplacian_segmentation(\n",
        "            evecs=evecs,\n",
        "            \n",
        "            # TODO: publish these parameters to the user\n",
        "            n_clusters=len(themes), # be sure to explain this is an upper bound, stochastic\n",
        "            n_spectral_features=len(themes),\n",
        "        )\n",
        "        \n",
        "        # beatsynch scene start times\n",
        "\n",
        "        # TODO: swap out rec['end'] -> rec_prev['start'] here\n",
        "        for rec in storyboard.prompt_starts:\n",
        "            beat_indices = np.where((beat_times >= rec['start']) & (beat_times <= rec['end']))[0]\n",
        "            segments_this_interval = segment_labels[beat_indices]\n",
        "            if len(segments_this_interval) == 0:\n",
        "                dominant_label = 0\n",
        "            else:\n",
        "                dominant_label = int(np.argmax(np.bincount(segments_this_interval)))\n",
        "            rec['structural_segmentation_label'] = dominant_label\n",
        "            rec['_theme'] = themes[dominant_label]\n",
        "    else:\n",
        "        for rec in storyboard.prompt_starts:\n",
        "            rec['_theme'] = themes[idx % len(themes)]\n",
        "else:\n",
        "    for rec in storyboard.prompt_starts:\n",
        "        rec['_theme'] = theme_prompt\n",
        "\n",
        "\n",
        "save_storyboard(storyboard)\n",
        "show_storyboard(storyboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zQvgGq3SgCfM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @markdown (Optional) modify theme prompt without impacting structure label assignments\n",
        "\n",
        "# @markdown Themes will be assigned to the `structural_segmentation` label that maps to their ordering\n",
        "# @markdown in the theme prompt. To change which theme goes where, simply modify the order in which\n",
        "# @markdown they appear in your prompt.\n",
        "\n",
        "theme_prompt = 'rusted industrial machinery | kaiju robot CGI | paperclips! paperclips! |  robotics for beginners | rusted industrial machinery' # @param {type:'string'}\n",
        "\n",
        "#####################################################\n",
        "\n",
        "storyboard.params.theme_prompt = theme_prompt\n",
        "themes = [prompt.strip() for prompt in theme_prompt.split('|') if prompt.strip()]\n",
        "\n",
        "for rec in storyboard.prompt_starts:\n",
        "    theme_idx = rec.get('structural_segmentation_label',0)\n",
        "    rec['_theme'] = themes[theme_idx]\n",
        "\n",
        "save_storyboard(storyboard)\n",
        "show_storyboard(storyboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "edxBKv5TgCfM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title OPTIONAL: Assign Animation parameters\n",
        "\n",
        "# @markdown Run This cell to reveal a multi-tab UI for specifying `img2img` animation settings. When you are happy with your settings, \n",
        "# @markdown run the cell which follows this one to attach the settings you have selected to one or more scenes/themes.\n",
        "\n",
        "# @markdown To reset values to default, simply re-run this cell.\n",
        "\n",
        "# @markdown The `img2img` animation mode is currently only supported through the Stability.AI Animation API (i.e. DreamStudio API). \n",
        "# @markdown The `img2img` animation parameters are mostly compatible with deforum: if you don't want to animate via the Stability.AI,\n",
        "# @markdown you can still use this notebook to configure your animation and then port the relevant settings from the `storyboard.yaml`\n",
        "# @markdown to your tool of choice.\n",
        "\n",
        "if not workspace.use_stability_api:\n",
        "    warnings.warn(\"img2img animation currently only supported if you're using the stability API\")\n",
        "    arg_objs=[]\n",
        "else:\n",
        "\n",
        "\n",
        "    # @markdown To get your API key visit https://dreamstudio.ai/account\n",
        "    STABILITY_HOST = \"grpc.stability.ai:443\" #@param {type:\"string\"}\n",
        "\n",
        "    show_documentation = True # @param {type:'boolean'}\n",
        "\n",
        "\n",
        "    ###################################\n",
        "\n",
        "    STABILITY_KEY = os.environ.get('STABILITY_KEY')\n",
        "\n",
        "    # Connect to Stability API\n",
        "    context = Context(STABILITY_HOST, STABILITY_KEY)\n",
        "\n",
        "    # Test the connection\n",
        "    context.get_user_info()\n",
        "\n",
        "\n",
        "\n",
        "    ###################\n",
        "\n",
        "    args_generation = BasicSettings()\n",
        "    args_animation = AnimationSettings()\n",
        "    args_camera = CameraSettings()\n",
        "    args_coherence = CoherenceSettings()\n",
        "    args_color = ColorSettings()\n",
        "    args_depth = DepthSettings()\n",
        "    args_render_3d = Rendering3dSettings()\n",
        "    args_inpaint = InpaintingSettings()\n",
        "    args_vid_in = VideoInputSettings()\n",
        "    args_vid_out = VideoOutputSettings()\n",
        "    arg_objs = (\n",
        "        args_generation,\n",
        "        args_animation,\n",
        "        args_camera,\n",
        "        args_coherence,\n",
        "        args_color,\n",
        "        args_depth,\n",
        "        args_render_3d,\n",
        "        args_inpaint,\n",
        "        args_vid_in,\n",
        "        args_vid_out,\n",
        "    )\n",
        "\n",
        "    def _show_docs(component):\n",
        "        cols = []\n",
        "        for k, v in component.param.objects().items():\n",
        "            if k == 'name':\n",
        "                continue\n",
        "            col = pn.Column(v, v.doc)\n",
        "            cols.append(col)\n",
        "        return pn.Column(*cols)\n",
        "\n",
        "    def build(component):\n",
        "        if show_documentation:\n",
        "            component = _show_docs(component)\n",
        "        return pn.Row(component, width=1000)\n",
        "\n",
        "pn.extension()\n",
        "\n",
        "pn.Tabs(*[(a.name[:-5], build(a)) for a in arg_objs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "efgDV40OVO9S",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Attach settings to scenes\n",
        "\n",
        "# @markdown If using img2img animation, use this cell to specify which scenes the settings you provided above should apply to.\n",
        "\n",
        "# @markdown Otherwise, you can ignore the cell above and just do everything here.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown `override_storyboard` - If settings conflict with values already set on the storyboard, the values on the storyboard will take priority.\n",
        "\n",
        "# @markdown `scene_ids` - Comma separated list of integers specifying scenes to attach to.\n",
        "\n",
        "# @markdown `theme_ids` - Comma separated list of integers specifying themes the animation parameters should be associated with.\n",
        "\n",
        "# @markdown NB: both `scene_ids` and `theme_ids` are zero-indexed.\n",
        "\n",
        "\n",
        "# CURRENT ARGS APPLY TO...\n",
        "\n",
        "all_scenes = True # @param {'type':'boolean'}\n",
        "\n",
        "scene_ids = '' # @param {'type':'string'}\n",
        "theme_ids = '' # @param {'type':'string'}\n",
        "\n",
        "override_storyboard = True # @param {'type':'boolean'}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## Animation Modes. \n",
        "# @markdown * `static`: turns off animation, static image for duration of scene.\n",
        "# @markdown * `variations`: injects a small bit of \"life\" into the image. Cheap and fast.\n",
        "# @markdown * `variations tsp`: variations animation with frames reordered for smoother motion. Cheap and fast, but a tad slower.\n",
        "# @markdown * `img2img`: Fancy deforum-esque animation, only supported for stability api at present. Neither cheap nor fast.\n",
        "# @markdown * `default`: `img2img` if stability api enabled,  `variations tsp` if not. \n",
        "\n",
        "# oh baby `jittered init`\n",
        "# markdown * `jittered init`: The VKTRS special! Sample a variation and use that as an init image for img2img. Basically built for audioreactivity\n",
        "# to do --> combine this with a referring expression mask\n",
        "# make sure animation is configured so we can curve prompts\n",
        "\n",
        "animation_mode = 'default' # @param [\"default\", \"static\", \"variations\", \"variations tsp\", \"img2img\"]\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## Parameters for `variations` Animation Modes\n",
        "\n",
        "# TODO: these parameters should get set elsewhere. maybe with animation mode?\n",
        "\n",
        "# @markdown `n_variations` - How many unique variations to generate for a given text prompt. This determines the frequency of the visual \"pulsing\" effect\n",
        "\n",
        "# @markdown `image_consistency` - controls similarity between images generated by the prompt.\n",
        "# @markdown - 0: ignore the init image\n",
        "# @markdown - 1: true as possible to the init image\n",
        "\n",
        "## @markdown `max_video_duration_in_seconds` - Early stopping if you don't want to generate a video the full duration of the provided audio. Default = 5min.\n",
        "\n",
        "\n",
        "n_variations=8 # @param {type:'integer'}\n",
        "image_consistency=0.72 # @param {type:\"slider\", min:0, max:1, step:0.01}  \n",
        "\n",
        "variations_settings = {\n",
        "    'n_variations':n_variations,\n",
        "    'image_consistency':image_consistency,\n",
        "}\n",
        "\n",
        "\n",
        "# let's try this:\n",
        "#   - user specifies animation stuff in cell above\n",
        "#   - then user comes down here and persists the animation settings either to every scene, or to specific scenes.\n",
        "#   - scene specification can just be a list of numbers\n",
        "#   - feels like this should be moved after init image generation. maybe push init image generation up to run right after theme prompts and scene count is finalized?\n",
        "\n",
        "# TODO: add storyboard -> parsec converter\n",
        "\n",
        "# TODO: PR to deforum for storyboard.yaml support\n",
        "\n",
        "# TODO: allow user to constrain attention to specific parameters or sets of parameters to set\n",
        "\n",
        "def param2json(args_obj):\n",
        "    args = args_obj.param.serialize_parameters()\n",
        "    args = json.loads(args)\n",
        "    args.pop('name')\n",
        "    return args\n",
        "\n",
        "def collect_animation_args():\n",
        "    if 'arg_objs' in locals():\n",
        "        args_d = {}\n",
        "        [args_d.update(a.param.values()) for a in arg_objs]\n",
        "        args=AnimationArgs(**args_d)\n",
        "    else:\n",
        "        import warnings\n",
        "        warnings.warn(\n",
        "            \"Looks like you're animating in img2img mode \"\n",
        "            \"without having specified any animation parameters. \"\n",
        "            \"Using SDK defaults. \"\n",
        "        )\n",
        "        args = AnimationArgs()\n",
        "    return param2json(args)\n",
        "\n",
        "\n",
        "##########################\n",
        "\n",
        "scene_ids = [int(v.strip()) for v in scene_ids.split(',') if v]\n",
        "theme_ids = [int(v.strip()) for v in theme_ids.split(',') if v]\n",
        "\n",
        "# build list of scenes args will apply to\n",
        "\n",
        "applicable_scenes = []\n",
        "if all_scenes:\n",
        "    applicable_scenes = [idx for idx, _ in enumerate(storyboard.prompt_starts)]\n",
        "else:\n",
        "    applicable_scenes += scene_ids\n",
        "    applicable_scenes += theme_ids\n",
        "\n",
        "###---------------------------------###\n",
        "\n",
        "animation_mode = animation_mode.lower()\n",
        "if animation_mode == 'default':\n",
        "    animation_mode = 'img2img' if workspace.use_stability_api else 'variations tsp'\n",
        "\n",
        "###---------------------------------###\n",
        "\n",
        "# apply args to appropriate scenes\n",
        "\n",
        "for idx, rec in enumerate(storyboard.prompt_starts):\n",
        "    if idx not in applicable_scenes:\n",
        "        continue\n",
        "\n",
        "    board_args = rec.get('animation_args')\n",
        "    if board_args:\n",
        "        board_args = OmegaConf.to_container(board_args) # coerce to dict\n",
        "\n",
        "    new_args = collect_animation_args()\n",
        "    if not override_storyboard:\n",
        "        new_args.update(board_args)\n",
        "        #variations_settings\n",
        "    rec['animation_args'] = new_args\n",
        "    rec.update(variations_settings)\n",
        "\n",
        "    if override_storyboard or (not rec.get('animation_mode')):\n",
        "        rec['animation_mode'] = animation_mode\n",
        "        \n",
        "    # oooo lookat me being fancy with custom code!\n",
        "    #if (idx >=23) or idx ==16:\n",
        "    #    rec['animation_mode'] = 'img2img'\n",
        "\n",
        "save_storyboard(storyboard)\n",
        "show_storyboard()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5LrrcvX2gCfM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Math\n",
        "\n",
        "#################################################\n",
        "# Math                                          #\n",
        "#                                               #\n",
        "#    This block computes how many frames are    #\n",
        "#    needed for each segment based on the start #\n",
        "#    times for each prompt                      #\n",
        "#################################################\n",
        "\n",
        "# TODO: leverage previous beat detection, onsets, etc. for frame timings\n",
        "# - TODO: isolated cells for calculating and suggesting parameters (fps, n_variations)\n",
        "\n",
        "# TODO: experiment with tying instantaneous framerate to a music attribute (i.e. so motion can change speed mid animation)\n",
        "\n",
        "fps = 30 # @param {type:'integer'}\n",
        "storyboard.params.fps = fps\n",
        "\n",
        "ifps = 1/fps\n",
        "\n",
        "# estimate video end\n",
        "if not storyboard.params.get('video_duration'):\n",
        "    storyboard.params['video_duration'] = get_audio_duration_seconds(storyboard.params.audio_fpath)\n",
        "video_duration = storyboard.params['video_duration']\n",
        "\n",
        "# dummy prompt for last scene duration\n",
        "prompt_starts = OmegaConf.to_container(storyboard.prompt_starts)\n",
        "prompt_starts.append({'start':video_duration})\n",
        "\n",
        "# TODO: do we still need anim_start? is that used in rendering?\n",
        "# TODO: add per-frame timings to incorporate beat information\n",
        "# make sure we respect the duration of the previous phrase\n",
        "frame_start=0\n",
        "prompt_starts[0]['anim_start']=frame_start\n",
        "for i, rec in enumerate(prompt_starts[1:], start=1):\n",
        "    rec_prev = prompt_starts[i-1]\n",
        "    k=0\n",
        "    while (rec_prev['anim_start'] + k*ifps) < rec['start']:\n",
        "        k+=1\n",
        "    k-=1\n",
        "    rec_prev['frames'] = k\n",
        "    rec_prev['anim_duration'] = k*ifps\n",
        "    frame_start+=k*ifps\n",
        "    rec['anim_start']=frame_start\n",
        "\n",
        "# drop the dummy frame\n",
        "prompt_starts = prompt_starts[:-1]\n",
        "\n",
        "# TODO: given a 0 duration prompt, assume its duration is captured in the next prompt \n",
        "#        and guesstimate a corrected prompt start time and duration \n",
        "#      - or rather.. why are there ever sero duration prompts?\n",
        "\n",
        "\n",
        "storyboard.prompt_starts = prompt_starts\n",
        "\n",
        "save_storyboard(storyboard)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0twvUVRTgCfM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @markdown # Audioreactivity\n",
        "\n",
        "# @markdown ### 1. Choose a driving signal\n",
        "\n",
        "driving_signal_name = \"bass stem\" #@param ['default','user specified','vocals stem','bass stem','other stem','drum stem']\n",
        "\n",
        "# TODO: test this\n",
        "custom_signal_fpath = '' # @param {'type':'string'}\n",
        "\n",
        "def get_user_specified_signal():\n",
        "    y, sr = librosa.load(custom_signal_fpath)\n",
        "    return y, sr\n",
        "\n",
        "y = structural_features['y']\n",
        "sr = structural_features['sr']\n",
        "\n",
        "driving_signals = {\n",
        "    'default': lambda: (y, sr),\n",
        "    'user specified': get_user_specified_signal,\n",
        "    'vocals stem':lambda: get_stem('vocals'),\n",
        "    'bass stem':lambda: get_stem('bass'),\n",
        "    'other stem':lambda: get_stem('other'),\n",
        "    'drum stem':lambda: get_stem('drum'),\n",
        "}\n",
        "\n",
        "driving_signal, sr = driving_signals[driving_signal_name]()\n",
        "display_signal(driving_signal, sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvJLNpHpgCfM"
      },
      "source": [
        "### Audio EQ cheat sheet\n",
        "\n",
        "![Audio EQ cheat sheet](https://i.pinimg.com/736x/4f/28/5e/4f285e3fbc5b6b6ea78638e58b2e3052.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hFOIMefxgCfM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title 2. Manipulate the signal\n",
        "\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "from inspect import signature\n",
        "from functools import partial\n",
        "from scipy.signal import find_peaks\n",
        "from sklearn.cluster import KMeans\n",
        "#sklearn_extra.cluster.KMedoids\n",
        "\n",
        "# @markdown To apply multiple operations, separate manipulation names with a '|' or just re-run this cell.\n",
        "# @markdown When you're satisfied, run the next cell to replace the loaded driving signal with the manipulated signal.\n",
        "\n",
        "# @markdown ### Available Signal Manipulations\n",
        "# @markdown NB: if a signal's name is followed by an asterisk, this means it modifies the signal's time domain. Only one time domain transformation per sequence is supported. \n",
        "# @markdown * `raw` - no op\n",
        "# @markdown * `rms`* - Root mean squared. Converts raw signal to signal power\n",
        "# @markdown * `novelty`* - Estimates strength of sound event onsets.\n",
        "# @markdown * `predominant_pulse`* - Combined estimate of beat timing and strength\n",
        "# @markdown * `stretch` - Increases gap between high and low amplitude signals. Alias for `pow2`\n",
        "# @markdown * `smoosh` - Reduces gap between high and low amplitude signals. Alias for `sqrt`\n",
        "# @markdown * `pow(k)` - Raises signal to the power of k.\n",
        "# @markdown * `pow2` - Square the signal. Increases gap between high and low amplitude signals\n",
        "# @markdown * `sqrt` - Square root of signal. Reduces gap between high and low amplitude signals\n",
        "# @markdown * `normalize` - Transform signal to `[0,1]` range. This will always be the last step, even if you don't specify it.\n",
        "# @markdown * `smooth(k)` - Smoothes out the waveworm, using a smoothing window of `k`. Bigger `k` = flatter signal\n",
        "# @markdown * `sustain(k)` - Treats signal as a self-exciting process that decays slowly over a window of `k`. \n",
        "# @markdown * `bandpass(low, high)` - Isolate signal to frequencies between `[low, high]`\n",
        "# @markdown * `threshold(low)` - Zero the signal where amplitude is less than `low`\n",
        "# @markdown * `clamp(high)` - Clamp the signal such that no values have amplitude greater than `high`\n",
        "# @markdown * `modulo(k)` - Detects peaks in signal and returns only every Kth-peak\n",
        "# @markdown * `quantize(k)` - Discretizes the signal into K unique values after clustering amplitudes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: specify which transforms are available in spectral space vs. not\n",
        "\n",
        "\n",
        "# trim/trim(q) -> y[y>quantile(y,.1)]\n",
        "# that 4part way of parameterizing a wave... hit, sustain, decay,..?\n",
        "\n",
        "\n",
        "\n",
        "# all operations must either have signature (y, sr) or return a function which does\n",
        "\n",
        "# def rms(y, sr):\n",
        "#     return librosa.feature.rms(y=y)\n",
        "\n",
        "# def novelty(y, sr):\n",
        "#     return librosa.onset.onset_strength(y, sr)\n",
        "\n",
        "# def predominant_pulse(y, sr):\n",
        "#     return librosa.beat.plp(y, sr)\n",
        "\n",
        "def pow2(y, sr):\n",
        "    return y**2\n",
        "\n",
        "def sqrt(y, sr):\n",
        "    return y**-2\n",
        "\n",
        "\n",
        "# so... apparently `pow` is a python builtin. whoops. Meh, fuck it.\n",
        "def _pow(k):\n",
        "    def pow_(y, sr):\n",
        "        return y**k\n",
        "    return pow_\n",
        "\n",
        "def stretch(k=2):\n",
        "    return _pow(k)\n",
        "\n",
        "def smoosh(k=2):\n",
        "    return _pow(-k)\n",
        "\n",
        "def normalize(y, sr):\n",
        "    normalized_signal = np.abs(y).ravel()\n",
        "    normalized_signal /= max(normalized_signal)\n",
        "    return normalized_signal\n",
        "    \n",
        "######################################\n",
        "    \n",
        "def smooth(k=150):\n",
        "    k=int(k)\n",
        "    def smooth_(y, sr=None):\n",
        "        win_smooth = signal.windows.hann(k)\n",
        "        filtered = signal.convolve(y, win_smooth, mode='same') / sum(win_smooth)\n",
        "        return filtered\n",
        "    return smooth_\n",
        "\n",
        "def sustain(k=500):\n",
        "    k=int(k)\n",
        "    def sustain_(y, sr=None):\n",
        "        win_sustain = signal.windows.hann(2*k)\n",
        "        win_sustain[:k]=0\n",
        "        filtered = signal.convolve(y.ravel(), win_sustain, mode='same') / sum(win_sustain)\n",
        "        return filtered\n",
        "    return sustain_\n",
        "\n",
        "\n",
        "# TODO: decay() - sustain with an exponential window\n",
        "\n",
        "#####################333\n",
        "    \n",
        "def bandpass(low: float, high:float):\n",
        "    return partial(butter_bandpass_filter, lowcut=low, highcut=high)\n",
        "    \n",
        "def threshold(low):\n",
        "    def f(y, sr):\n",
        "        y[y<low] = 0\n",
        "        return y\n",
        "    return f\n",
        "\n",
        "def clamp(high):\n",
        "    def f(y, sr):\n",
        "        y[y>high] = high\n",
        "        return y\n",
        "    return f\n",
        "\n",
        "\n",
        "###############################3\n",
        "\n",
        "\n",
        "# def peak_detection(y, sr):\n",
        "#     peaks, _ = find_peaks(y)\n",
        "#     return peaks\n",
        "\n",
        "# TODO: support offset, so user could e.g. take either every even or every odd peak. \n",
        "def modulo(k=2, offset=0):\n",
        "    #k=int(k)\n",
        "    def modulo_(y, sr=None):\n",
        "        #peaks = peak_detection(y, sr)\n",
        "        peaks, _ = find_peaks(y)\n",
        "        #selected_peaks = peaks[::k]  # Select every kth peak\n",
        "        selected_peaks=[]\n",
        "        for peak_index, peak in enumerate(peaks.ravel()):\n",
        "            if (peak_index + offset) % k == 0:\n",
        "                selected_peaks.append(peak)\n",
        "        print(selected_peaks)\n",
        "        selected_peaks = np.array(selected_peaks)\n",
        "        new_signal = np.zeros_like(y)\n",
        "        new_signal[selected_peaks] = y[selected_peaks]  # Build a new signal with only the selected peaks\n",
        "        return new_signal\n",
        "    return modulo_\n",
        "\n",
        "#################################\n",
        "\n",
        "# chatgpt wrote this, needs to be tested. also, i might want to use medoids rather than means\n",
        "\n",
        "\n",
        "\n",
        "def quantize(k=1):\n",
        "    k=int(k)\n",
        "    # why doesn't it respect `k` in the closure scope? Works fine for modulo(). weird.\n",
        "    #def quantize_(y, sr=None):\n",
        "    def quantize_(y, sr=None, K=k):\n",
        "        k=K\n",
        "        # Remove zero values\n",
        "        nonzero_values = y[y > 0].reshape(-1, 1)\n",
        "        \n",
        "        # If the number of nonzero values is less than k, reduce k\n",
        "        if nonzero_values.shape[0] < k:\n",
        "            k = nonzero_values.shape[0]\n",
        "        \n",
        "        # Perform k-means clustering\n",
        "        kmeans = KMeans(n_clusters=k)\n",
        "        kmeans.fit(nonzero_values)\n",
        "\n",
        "        # Replace each value with the centroid of its cluster\n",
        "        print(f\"cluster centers: {np.unique(kmeans.cluster_centers_)}\")\n",
        "        quantized_values = kmeans.cluster_centers_[kmeans.labels_].flatten()\n",
        "\n",
        "        # Create a new signal\n",
        "        quantized_signal = np.zeros_like(y)\n",
        "        quantized_signal[y > 0] = quantized_values\n",
        "        return quantized_signal\n",
        "    return quantize_\n",
        "\n",
        "#####################################3\n",
        "\n",
        "# TODO: add ability for user to do stuff via idiomatic `keyframed` rather than convolving signals\n",
        "\n",
        "# # TODO: add operations: slice/isolate, mute, shift_y/truncate/drop (subtract some value from amplitude)\n",
        "simple_signal_operations = {\n",
        "    'raw': lambda y, sr: y,\n",
        "    ##########\n",
        "    'rms': lambda y, sr: librosa.feature.rms(y=y),\n",
        "    'novelty': librosa.onset.onset_strength,\n",
        "    'predominant_pulse': librosa.beat.plp,\n",
        "    'bandpass': bandpass,\n",
        "    'harmonic': lambda y, sr: librosa.effects.harmonic(y=y),\n",
        "    'percussive': lambda y, sr: librosa.effects.percussive(y=y),\n",
        "    ##########\n",
        "    'pow2': lambda y, sr: y**2,\n",
        "    'stretch': stretch,\n",
        "    'sqrt': sqrt,\n",
        "    'smoosh': smoosh,\n",
        "    'pow':_pow,\n",
        "    #################\n",
        "    'smooth': smooth,\n",
        "    'sustain': sustain,\n",
        "    # #########\n",
        "    'normalize': normalize,\n",
        "    'abs': lambda y, sr: np.abs(np.abs(y)), \n",
        "    'threshold': threshold,\n",
        "    'clamp': clamp,\n",
        "    'modulo': modulo,\n",
        "    'quantize':quantize,\n",
        "}\n",
        "\n",
        "def prepare_operation(op_str, operations=simple_signal_operations):\n",
        "    op_str = op_str.replace(' ','') # make sure there are no spaces separating arguments\n",
        "    for op_name, op in operations.items():\n",
        "        if op_name == op_str:\n",
        "            return op\n",
        "        \n",
        "    # if we're here, that should mean we have arguments to parse.\n",
        "    for op_name, op in operations.items():\n",
        "        if op_str.startswith(f\"{op_name}(\"):\n",
        "            break\n",
        "    else: # if we're here, it means we never broke out of the `for` loop, i.e. couldn't find a matching op\n",
        "        raise ValueError(f\"{op_str} is not a supported operation. Supported operations: {[op_name for op in operations]}\")\n",
        "    \n",
        "    arg_names = [p for p in signature(op).parameters]\n",
        "    bracketed = [\"{\" + p + \"}\" for p in arg_names]\n",
        "    template = f\"{op_name}({','.join(bracketed)})\"\n",
        "    result = parse.parse(template, op_str)\n",
        "    kargs = result.named\n",
        "    kargs = {k:float(v) for k,v in kargs.items()} # coerce strings to floats\n",
        "    return op(**kargs)\n",
        "    \n",
        "\n",
        "# signal_manipulations = 'bandpass(300, 500) | pow2 | pow2 | rms | pow2 | threshold(5e-6) ' # @param {'type':'string'}\n",
        "signal_manipulations = 'harmonic | bandpass(300, 500) | novelty |  sustain(30) |  normalize | threshold(.25) ' # @param {'type':'string'}\n",
        "\n",
        "manipulations = [m.strip() for m in signal_manipulations.split('|') if m.strip()]\n",
        "\n",
        "# reset processing\n",
        "driving_signal_massaged = driving_signal\n",
        "\n",
        "# TODO: precompute time component, modify as needed.\n",
        "#       - concretely, rms changes time domain from samples->frames.\n",
        "\n",
        "show_spec=True\n",
        "windowed_manipulations = ['rms', 'novelty']\n",
        "manipulation_history = [] # this is necessary because certain manipulations shouldn't be repeated or otw can collide w each other\n",
        "for idx, manipulation in enumerate(manipulations):\n",
        "    print(manipulation)\n",
        "    if manipulation in windowed_manipulations:\n",
        "        if show_spec:\n",
        "            show_spec=False\n",
        "        else:\n",
        "            warnings.warn(f\"transform {manipulation} collides with a transform we've already performed. Skipping.\")\n",
        "            continue\n",
        "    \n",
        "    f = prepare_operation(manipulation)\n",
        "    try:\n",
        "        driving_signal_massaged = f(y=driving_signal_massaged, sr=sr)\n",
        "    except (TypeError, librosa.ParameterError) as e: # this is sort of gross but fuck it\n",
        "        print(e)\n",
        "        f=f()\n",
        "        driving_signal_massaged = f(driving_signal_massaged, sr)\n",
        "\n",
        "    display_signal(driving_signal_massaged, sr, title=f\"{idx}, {manipulation}\", show_spec=show_spec)\n",
        "else:\n",
        "    # finalize signal\n",
        "    driving_signal_massaged = np.abs(driving_signal_massaged).ravel()\n",
        "    driving_signal_massaged /= max(driving_signal_massaged)\n",
        "    display_signal(driving_signal_massaged, sr, title=f\"Final Driving Signal\", show_spec=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XX432Y_XgCfM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title 2b. Run this cell when you're satisfied with the transformed signal\n",
        "\n",
        "use_massaged_signal = True # @param {'type':'boolean'}\n",
        "if use_massaged_signal:\n",
        "    driving_signal = driving_signal_massaged\n",
        "\n",
        "\n",
        "### Uncomment this snippet to write your driving signal to an audio file\n",
        "#import soundfile\n",
        "#soundfile.write('driving_signal.wav', driving_signal, sr)\n",
        "\n",
        "normalized_signal = np.abs(driving_signal).ravel()\n",
        "normalized_signal /= max(normalized_signal)\n",
        "\n",
        "# TODO: manipulate the `frame_time` variable in synch with the signal. maybe track a \"time_domain\" variable ('frames'|'samples')\n",
        "\n",
        "if show_spec:\n",
        "    frame_time = librosa.samples_to_time(np.arange(len(normalized_signal)), sr=sr)\n",
        "else:\n",
        "    frame_time = librosa.frames_to_time(np.arange(len(normalized_signal)), sr=sr)\n",
        "\n",
        "driving_signal_kf = kf.Curve({t:v for t,v in zip(frame_time, normalized_signal)})\n",
        "\n",
        "plt.plot(frame_time, normalized_signal)\n",
        "plt.xlabel('seconds')\n",
        "plt.ylabel('intensity')\n",
        "plt.title(\"Processed Audioreactivity Signal\")\n",
        "full_width_plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5VM7HkangCfM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title 3. Map the driving signal to parameter ranges \n",
        "\n",
        "# TODO: don't need this signal naming stuff. if anything, the signal we should be naming is the driving_signal\n",
        "\n",
        "signal_name = 'noise_reactive' # @param {'type':'string'}\n",
        "target_parameter = 'strength_curve' # @param {'type':'string'}\n",
        "attr_hi= .08 # @param {'type':'number'}\n",
        "attr_low = .02 # @param {'type':'number'}\n",
        "inverse_relationship = False # @param {'type':'boolean'}\n",
        "\n",
        "signals = [dict(\n",
        "    signal_name=signal_name,\n",
        "    attr_hi=attr_hi,\n",
        "    attr_low=attr_low,\n",
        "    inverse_relationship=inverse_relationship\n",
        ")]\n",
        "\n",
        "# signals =[\n",
        "#     dict(\n",
        "#         signal_name = 'noise_reactive',\n",
        "#         attr_hi= .08,\n",
        "#         attr_low = .02,\n",
        "#         inverse_relationship = False,\n",
        "#     ),\n",
        "#     dict(\n",
        "#         attr_hi=.8,\n",
        "#         attr_low = .5,\n",
        "#         inverse_relationship = True,\n",
        "#         signal_name =  'audio_reactive_curve',\n",
        "#     ),\n",
        "#     dict(\n",
        "#         attr_hi=30,\n",
        "#         attr_low = 1,\n",
        "#         inverse_relationship = False,\n",
        "#         signal_name =  'dynamic_fps',\n",
        "#     ),\n",
        "# ]\n",
        "\n",
        "\n",
        "# # 3b. Map the driving signal to parameter ranges\n",
        "# reactive_signal_map = {\n",
        "#     'audio_reactive_curve':'strength_curve',\n",
        "#     'noise_reactive':'noise_add_curve',\n",
        "#     'dynamic_fps':'dynamic_fps', # probably needs a better name...\n",
        "# }\n",
        "\n",
        "reactive_signal_map = {signal_name: target_parameter}\n",
        "\n",
        "# TODO: make the above part more user friendly\n",
        "\n",
        "# TODO: add the \"applicable scenes\" thing down here\n",
        "\n",
        "##################################################################################################################\n",
        "##################################################################################################################\n",
        "\n",
        "# Hi user. You've gone to far. Don't worry about this stuff. Just the bit above the line.\n",
        "\n",
        "for signal_params in signals:\n",
        "\n",
        "    attr_hi= signal_params['attr_hi']\n",
        "    attr_low = signal_params['attr_low']\n",
        "    inverse_relationship = signal_params['inverse_relationship']\n",
        "    signal_name = signal_params['signal_name']\n",
        "\n",
        "    # TODO: instead of attaching the string, let's just build a curve object and use\n",
        "    #       keyframed.Curve's native serialization. this was the whole point of doing it that way.\n",
        "\n",
        "    #print(signal_params)\n",
        "\n",
        "    for scene_idx, rec in enumerate(storyboard.prompt_starts):\n",
        "        #print(scene_idx)\n",
        "        if scene_idx == 0:\n",
        "            prev_rec = rec\n",
        "            continue\n",
        "        start, end = prev_rec['start'], rec['start']\n",
        "\n",
        "        # TODO: move this correction UP\n",
        "        if scene_idx == 1:\n",
        "          start = 0\n",
        "\n",
        "        if prev_rec['frames'] < 1:\n",
        "            continue\n",
        "            \n",
        "        curve_chunks = []        \n",
        "        for frame_idx in range(prev_rec['frames']):\n",
        "            curr_time = start + frame_idx * ifps\n",
        "            signal_value = driving_signal_kf[curr_time]\n",
        "            if inverse_relationship:\n",
        "                signal_value = 1-signal_value\n",
        "            attr_value = signal_value*(attr_hi-attr_low)+attr_low\n",
        "            curve_chunks.append(f\"{frame_idx}:({attr_value})\")\n",
        "        curve_str = ','.join(curve_chunks)\n",
        "        prev_rec[signal_name] = curve_str\n",
        "        prev_rec = rec\n",
        "    else: # hate this.\n",
        "        start, end = rec['start'], rec['end']\n",
        "        if rec['frames'] > 0:\n",
        "            curve_chunks = []\n",
        "            for frame_idx in range(rec['frames']):\n",
        "                curr_time = start + frame_idx * ifps\n",
        "                signal_value = driving_signal_kf[curr_time]\n",
        "                if inverse_relationship:\n",
        "                    signal_value = 1-signal_value\n",
        "                attr_value = signal_value*(attr_hi-attr_low)+attr_low\n",
        "                curve_chunks.append(f\"{frame_idx}:({attr_value})\")\n",
        "            curve_str = ','.join(curve_chunks)\n",
        "            rec[signal_name] = curve_str\n",
        "\n",
        "            \n",
        "# TODO: write directly into rec.animation_args['arg']\n",
        "\n",
        "# TODO: save keyframed objects for signals\n",
        "\n",
        "if not storyboard.get('audioreactive'):\n",
        "    storyboard.audioreactive = {'signals': [], 'reactive_signal_map': {}}\n",
        "\n",
        "# TODO: check if these signals are already in the board\n",
        "#       ...actually, will probably be simpler to just reshape this so each signal is a dict whose key is the name\n",
        "storyboard.audioreactive.signals.append(signals) \n",
        "\n",
        "storyboard.audioreactive.reactive_signal_map.update(reactive_signal_map)\n",
        "save_storyboard(storyboard)\n",
        "\n",
        "show_storyboard(storyboard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RTUFeyQqCfd"
      },
      "source": [
        "## $3.$ ðŸŽ¬ Animate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Sh514DGj_sua",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#####################################\n",
        "# @title ## ðŸŽ¨ Generate init images\n",
        "#####################################\n",
        "\n",
        "# @markdown If this cell throws errors, just run it again (with resume turned on)\n",
        "\n",
        "\n",
        "# TODO: add archive/don't archive toggle...\n",
        "\n",
        "prompt_starts = storyboard.prompt_starts\n",
        "use_stability_api = workspace.use_stability_api\n",
        "model_dir = workspace.model_dir\n",
        "\n",
        "device = 'cuda'\n",
        "model_id = \"CompVis/stable-diffusion-v1-5\"\n",
        "download=True # TODO: pretty sure we shouldn't need to redownload here\n",
        "\n",
        "model_dir = workspace.model_dir\n",
        "model_path= str(Path(model_dir) / 'huggingface' / 'diffusers')\n",
        "\n",
        "MAX_IM_PER_BATCH_HF = 1\n",
        "\n",
        "#if 'get_image_for_prompt' not in locals():\n",
        "\n",
        "if use_stability_api:\n",
        "    import warnings\n",
        "    from stability_sdk import client\n",
        "    import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
        "\n",
        "\n",
        "    def get_image_for_prompt(prompt, max_retries=5, **kwargs):\n",
        "        return get_image_for_prompt_sai(prompt, max_retries=5, **kwargs)\n",
        "\n",
        "\n",
        "    # leverage stability API internal parallelism for batch variation requests\n",
        "    def get_variations_w_init(prompt, init_image, n_variations=2, image_consistency=.7, **kwargs):\n",
        "          return list(\n",
        "              get_image_for_prompt(\n",
        "                  prompt=prompt, \n",
        "                  init_image=init_image, \n",
        "                  start_schedule=(1-image_consistency), \n",
        "                  #num_samples=n_variations,\n",
        "                  samples=n_variations,\n",
        "                  **kwargs,\n",
        "              )\n",
        "          )\n",
        "                    \n",
        "else:\n",
        "\n",
        "    if download:\n",
        "        img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            revision=\"fp16\", \n",
        "            torch_dtype=torch.float16,\n",
        "            use_auth_token=True\n",
        "        )\n",
        "        img2img = img2img.to(device)\n",
        "        img2img.save_pretrained(model_path)\n",
        "    else:\n",
        "        img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "            model_path,\n",
        "            local_files_only=True\n",
        "        ).to(device)\n",
        "\n",
        "    text2img = StableDiffusionPipeline(\n",
        "        vae=img2img.vae,\n",
        "        text_encoder=img2img.text_encoder,\n",
        "        tokenizer=img2img.tokenizer,\n",
        "        unet=img2img.unet,\n",
        "        feature_extractor=img2img.feature_extractor,\n",
        "        scheduler=img2img.scheduler,\n",
        "        safety_checker=img2img.safety_checker,\n",
        "    )\n",
        "    text2img.enable_attention_slicing()\n",
        "    img2img.enable_attention_slicing()\n",
        "\n",
        "\n",
        "    def get_image_for_prompt_hf(\n",
        "        prompt,\n",
        "        **kwargs\n",
        "    ):\n",
        "        if 'init_image' in kwargs:\n",
        "            kwargs['image'] = kwargs.pop('init_image')\n",
        "        if 'start_schedule' in kwargs:\n",
        "            kwargs['strength'] = kwargs.pop('start_schedule')\n",
        "        if 'image_consistency' in kwargs:\n",
        "            kwargs['strength'] = 1-kwargs.pop('image_consistency')\n",
        "        f = text2img if kwargs.get('image') is None else img2img\n",
        "        n_retries = 5\n",
        "        with autocast(device):\n",
        "            return f(prompt, **kwargs)\n",
        "            # while n_retries > 0:\n",
        "            #     n_retries-=1\n",
        "            #     result = f(prompt, **kwargs)\n",
        "            #     if not any(result.nsfw_content_detected):\n",
        "            #         return result.images\n",
        "            #     else:\n",
        "            #         print(f\"nsfw content detectected. retries remaining: {n_retries}\")\n",
        "\n",
        "    def get_image_for_prompt(*args, **kwargs):\n",
        "        # if 'init_image' in kwargs:\n",
        "        #     kwargs['image'] = kwargs.pop('init_image')\n",
        "        # if 'start_schedule' in kwargs:\n",
        "        #     kwargs['strength'] = kwargs.pop('start_schedule')\n",
        "        # if 'image_consistency' in kwargs:\n",
        "        #     kwargs['strength'] = 1-kwargs.pop('image_consistency')\n",
        "        n_retries = 5\n",
        "        while n_retries > 0:\n",
        "                n_retries-=1\n",
        "                result = get_image_for_prompt_hf(*args, **kwargs)\n",
        "                if not any(result.nsfw_content_detected):\n",
        "                    return result.images\n",
        "                else:\n",
        "                    print(f\"nsfw content detectected. retries remaining: {n_retries}\")\n",
        "\n",
        "\n",
        "    # TODO: (HF) request multiple images in single request\n",
        "    def get_variations_w_init(prompt, init_image, **kwargs):\n",
        "        #if 'n_variations' in kwargs:\n",
        "        #    kwargs['num_images_per_prompt'] = min(MAX_IM_PER_BATCH_HF, kwargs.pop('n_variations'))\n",
        "        n_variations = kwargs.pop('n_variations')\n",
        "        while n_variations > 0:\n",
        "            n_imgs_this_batch = min(MAX_IM_PER_BATCH_HF, n_variations)\n",
        "            #for response in get_image_for_prompt(\n",
        "            response = get_image_for_prompt_hf(\n",
        "                prompt=prompt, \n",
        "                init_image=init_image, \n",
        "                num_images_per_prompt=n_imgs_this_batch,\n",
        "                return_dict=True,\n",
        "                **kwargs,\n",
        "            )\n",
        "            print(response)\n",
        "            for img, nsfw in zip(response['images'], response['nsfw_content_detected']):\n",
        "                if not nsfw:\n",
        "                    yield img\n",
        "                    n_variations -= 1\n",
        "                else:\n",
        "                    warnings.warn(\"NSFW classifier triggered. Trying again.\")\n",
        "\n",
        "\n",
        "\n",
        "##################\n",
        "##  PARAMETERS  ##\n",
        "##################\n",
        "\n",
        "d_ = dict(\n",
        "    _=''\n",
        "    , height = 512 # @param {type:'integer'}\n",
        "    , width = 512 # @param {type:'integer'}\n",
        "    # TODO: pretty sure can delete this\n",
        "    #, display_frames_as_we_get_them = True # @param {type:'boolean'}\n",
        ")\n",
        "d_.pop('_')\n",
        "\n",
        "#regenerate_all_init_images = True # @param {type:'boolean'}\n",
        "regenerate_all_init_images = False # @param {type:'boolean'}\n",
        "\n",
        "# TODO: make this an integer\n",
        "prompt_lag = True # @param {type:'boolean'}\n",
        "\n",
        "# @markdown `prompt_lag` - Extend prompt with lyrics from previous frame. Can improve temporal consistency of narrative. \n",
        "# @markdown  Especially useful for lyrics segmented into short prompts.\n",
        "\n",
        "#TODO: check from storyboard\n",
        "\n",
        "# regenerate all images if the theme prompt has changed or user specifies\n",
        "#if d_['theme_prompt'] != storyboard.params.get('theme_prompt'):\n",
        "#    regenerate_all_init_images = True\n",
        "\n",
        "\n",
        "storyboard.params.update(d_)\n",
        "\n",
        "if regenerate_all_init_images:\n",
        "    for i, rec in enumerate(prompt_starts):\n",
        "        rec['frame0_fpath'] = None\n",
        "        archive_images(i, root=root)\n",
        "    print(\"archival process complete\")\n",
        "\n",
        "# anchor images will be regenerated if there's no associated frame0_fpath\n",
        "# regenerate specific images if\n",
        "# * manually tagged by user in df_regen\n",
        "# * associated fpath doesn't exist (i.e. deleted)\n",
        "# TODO: dump df_regen stuff\n",
        "# TODO: move archival stuff...\n",
        "if 'df_regen' in locals():\n",
        "    for i, _ in df_regen.iterrows():\n",
        "        rec = prompt_starts[i]\n",
        "        regen = not _['keep']\n",
        "        # need to check this elsewhere\n",
        "        if rec.get('frame0_fpath') is None:\n",
        "            regen = True\n",
        "        elif not Path(rec['frame0_fpath']).exists():\n",
        "            regen=True\n",
        "        if regen:\n",
        "            rec['frame0_fpath'] = None\n",
        "            print(rec)\n",
        "            archive_images(i, root=root)\n",
        "    print(\"archival process complete\")\n",
        "\n",
        "\n",
        "theme_prompts = storyboard.params.theme_prompt\n",
        "height = storyboard.params.height\n",
        "width = storyboard.params.width\n",
        "\n",
        "proj_name = workspace.active_project\n",
        "\n",
        "\n",
        "## Main loop ##\n",
        "\n",
        "print(\"Ensuring each prompt has an associated image\")\n",
        "for idx, rec in enumerate(prompt_starts):\n",
        "    #print(idx, rec)\n",
        "    theme = rec.get('_theme')\n",
        "    prompt = rec.get('prompt')\n",
        "    if not prompt:\n",
        "        prompt = f\"{rec['text']}, {theme}\"\n",
        "    \n",
        "        if prompt_lag and (idx > 0):\n",
        "            rec_prev = prompt_starts[idx -1]\n",
        "            prev_text = rec_prev.get('text')\n",
        "            if not prev_text:\n",
        "                prev_text = rec_prev.get('prompt').split(',')[0]\n",
        "            this_text = rec.get('text')\n",
        "            if this_text:\n",
        "                prompt = f\"{prev_text} {this_text}, {theme}\"\n",
        "            else:\n",
        "                prompt = rec_prev['_prompt']\n",
        "    rec['_prompt'] = prompt\n",
        "    \n",
        "    print(\n",
        "        f\"scene: {idx}\\t time: {rec['start']}\\n\"\n",
        "        f\"spoken text: {rec.get('text')}\\n\"\n",
        "        f\"image prompt: {rec['_prompt']}\\n\"\n",
        "    )\n",
        "    need_image = True\n",
        "    if rec.get('frame0_fpath') is not None:\n",
        "        if Path(rec['frame0_fpath']).exists():\n",
        "            need_image = False\n",
        "    if need_image:\n",
        "        \n",
        "        # TODO: get generation settings from rec.animation_args ## needs a more general name\n",
        "        init_image = list(get_image_for_prompt(\n",
        "              rec['_prompt'],\n",
        "              height=height,\n",
        "              width=width,\n",
        "              )\n",
        "          )[0]\n",
        "        \n",
        "        # TODO: save_frame doesn't need to be a function.\n",
        "        rec['frame0_fpath'] = save_frame(\n",
        "            init_image,\n",
        "            idx,\n",
        "            root_path = root / 'frames',\n",
        "            name='anchor',\n",
        "            )\n",
        "\n",
        "        print(rec.get('text'))\n",
        "        display(init_image)\n",
        "\n",
        "# TODO: regen picks up at first frame that needs it correctly, \n",
        "#       but then continues on overwriting all subsequent frames.\n",
        "#       Fix it to regain ability to delete specific init images to have just those regened\n",
        "\n",
        "##############\n",
        "# checkpoint #\n",
        "##############\n",
        "\n",
        "# TODO: recognize previous generations\n",
        "\n",
        "storyboard.prompt_starts = prompt_starts\n",
        "\n",
        "save_storyboard(storyboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S7Mr5QEzgCfN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @markdown Did one of your prompts trigger a safety filter? modify it here.\n",
        "\n",
        "scene_id = 0 # @param {type:'integer'}\n",
        "new_caption = \"\" # @param {type:'string'}\n",
        "\n",
        "###########################################################\n",
        "\n",
        "if new_caption:\n",
        "    old_caption = storyboard.prompt_starts[scene_id].text\n",
        "    storyboard.prompt_starts[scene_id].text = new_caption\n",
        "    print(f\"{old_caption} -> {new_caption}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EuTrovQqgCfR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title ## ðŸš€ Animate!\n",
        "\n",
        "# Fuck it, this should be consistent anyway, right?\n",
        "# -------------------------------------------------\n",
        "# load storyboard from disk before animating if you \n",
        "# made changes that you want to be respected\n",
        "workspace, storyboard = load_storyboard()\n",
        "\n",
        "# @markdown Negative prompt currently only used with animation mode img2img\n",
        "\n",
        "#TODO: mechanism to archive/delete just the animation frames\n",
        "\n",
        "# TODO: uh... you don't have to go home, but you can't stay here.\n",
        "negative_prompt = 'blurry, low detail, low quality, unfinished' # @param {type:'string'}\n",
        "negative_prompt_weight = -1 # @param {type:'number'}\n",
        "\n",
        "\n",
        "# ... why is this so hard. TODO: make this whole cell simpler.\n",
        "\n",
        "\n",
        "from keyframed import SmoothCurve\n",
        "\n",
        "\n",
        "# TODO: write a (different name though) scenes.txt for each scene, so we can get fancier with how we animate and generate\n",
        "\n",
        "print(\"Animating Sequence\")\n",
        "for idx, rec in enumerate(storyboard.prompt_starts):\n",
        "    \n",
        "    # at least it'll be consistent I guess...\n",
        "    if rec.get('frame0_fpath') is None:\n",
        "        print(f\"skipping scene {idx}, no init image detected\")\n",
        "    \n",
        "    print(f\"Animating scene {idx}\")\n",
        "    \n",
        "    \n",
        "    # default mode contingent on api mode\n",
        "    animation_mode = rec.get('animation_mode', 'default').lower()\n",
        "    if animation_mode == 'default':\n",
        "        animation_mode = 'img2img' if workspace.use_stability_api else 'variations tsp'\n",
        "    print(f\"animation mode: {animation_mode}\")\n",
        "        \n",
        "    # figure out how many images we've already generated\n",
        "    #TODO: \"root\" should be outpath, not project_root. \n",
        "    #      push up any \"/frames\" logic in get_image_sequence\n",
        "    images_fpaths = get_image_sequence(idx, root=workspace.project_root)\n",
        "    images_fpaths = sorted(images_fpaths, key=os.path.getmtime)\n",
        "    \n",
        "    curr_variation_count = len(images_fpaths)\n",
        "    print(f\"curr_variation_count:{curr_variation_count}\")\n",
        "\n",
        "    ##########################################################################\n",
        "    \n",
        "    ### calculate total number of images needed per scene depends on animation mode\n",
        "    ### and figure out init_image/init_image_fpath\n",
        "    \n",
        "    # `animation_mode=None`\n",
        "    tot_variations = 0\n",
        "    \n",
        "    if any(adj in animation_mode \n",
        "           for adj in ('jittered','variations')):\n",
        "        \n",
        "        init_image_path = images_fpaths[0]\n",
        "        init_image = Image.open(init_image_path)\n",
        "        \n",
        "        # TODO: generalize this, probably means pushing tot_variations into some other settings object\n",
        "        # TODO: create some sort of get_default('param_name') or get overrides that wraps the procedure of checking for a value in the rec or the sotryboard.params\n",
        "        # TODO: rename the xx_variations\" vars. `tot_variations` = total number of images that need to be generated for this scene (function of scene and animation mode)\n",
        "        \n",
        "        # permits user to specify more variations for a specific scene\n",
        "        #tot_variations = rec.get('n_variations', storyboard.params.n_variations)\n",
        "        tot_variations = rec.get('n_variations', storyboard.params.get('n_variations',5))\n",
        "        tot_variations = min(tot_variations, rec['frames'])\n",
        "\n",
        "        image_consistency = rec.get('image_consistency', storyboard.params.get('image_consistency', .75)) # 1-Curve(args.noise) or whatever\n",
        "        \n",
        "    if 'img2img' in animation_mode:\n",
        "        \n",
        "        init_image_path = images_fpaths[-1]\n",
        "        \n",
        "        tot_variations = rec['frames']\n",
        "        print(f\"tot_variations:{tot_variations}\")\n",
        "        \n",
        "        args = AnimationArgs(**rec.get('animation_args',{}))\n",
        "    \n",
        "        \n",
        "        ### inject tweaks to animation (maybe this could be a callback/hook or something?)\n",
        "\n",
        "        # TODO: expose this as an option or something\n",
        "#         sign = (-1)**(idx%2)\n",
        "#         value = sign * 0.1\n",
        "#         args.translation_x = f\"0:({value})\"\n",
        "#         args.rotation_y = f\"0:({-value})\"\n",
        "\n",
        "    ##########################################################################   \n",
        "\n",
        "    tot_variations -= curr_variation_count  # only generate variations we still need\n",
        "    \n",
        "    if tot_variations < 1:\n",
        "        print(\"No animation frames required for this scene. Checking next scene\")\n",
        "        continue\n",
        "\n",
        "    print(f\"tot_variations to request:{tot_variations}\")\n",
        "\n",
        "    prompt = rec['_prompt']\n",
        "    \n",
        "    ##########################################################################\n",
        "    \n",
        "    if 'variations' in animation_mode:\n",
        "        \n",
        "        image_variations = get_variations_w_init(\n",
        "            prompt = prompt, \n",
        "            init_image = init_image, \n",
        "            image_consistency = image_consistency, # TODO: align with animation args\n",
        "            n_variations = tot_variations,\n",
        "        )\n",
        "        for img in image_variations:\n",
        "            save_frame(\n",
        "                img,\n",
        "                idx,\n",
        "                root_path= root / 'frames',\n",
        "            )\n",
        "            display(img)\n",
        "        \n",
        "    elif 'img2img' in animation_mode:\n",
        "        from omegaconf.errors import ConfigAttributeError\n",
        "        \n",
        "        args.init_image = init_image_path\n",
        "        args.max_frames = tot_variations\n",
        "        try:\n",
        "            args.fps = storyboard.params.fps # this shit is gonna be a mess...\n",
        "        except ConfigAttributeError:\n",
        "            pass\n",
        "        \n",
        "        # TODO: make this more flexible (e.g. pull everything from animation_args...)\n",
        "        animation_prompts = {0:prompt}\n",
        "        \n",
        "        # TODO: this shouldn't be specific to SAI animation\n",
        "        #       `args` object for other anim modes as well, then wrap those modes in functions?\n",
        "        \n",
        "        # TODO: prove out generalized-ness with curved fps\n",
        "        \n",
        "        for k,v in storyboard.audioreactive.reactive_signal_map.items():\n",
        "            setattr(args, v, rec[k])\n",
        "\n",
        "        animator = Animator(\n",
        "            api_context=context,\n",
        "            animation_prompts=animation_prompts,\n",
        "            args=args,\n",
        "            out_dir=None, #out_dir,    \n",
        "            negative_prompt=negative_prompt,\n",
        "            negative_prompt_weight=negative_prompt_weight,\n",
        "            #resume=len(resume_timestring) != 0\n",
        "            resume=False,\n",
        "        )\n",
        "\n",
        "########################################################\n",
        "    \n",
        "        # TODO: push this out of the loop? I feel like what I want to be able to do\n",
        "        #       is loop over each frame in the scene, within the rec loop, in such a way that we go back up to \n",
        "        #       the top of the loop between images. At which point, it's like... sheesh.\n",
        "        #       basically what I want to be able to do is modify settings between frames, \n",
        "        #       since we're constrained by the data types of the AnimationArgs params\n",
        "        \n",
        "        for img in tqdm(animator.render(), initial=animator.start_frame_idx, total=args.max_frames):\n",
        "            # TODO: this isn't super reliable, often crashes colab\n",
        "            from IPython.display import clear_output\n",
        "            clear_output(wait=True)\n",
        "            display(img)\n",
        "            new_frame_fpath = save_frame(\n",
        "                img, idx, root_path=root/'frames',\n",
        "                name=f\"{tot_variations}\" # it'll be reversed, but it's not nothing\n",
        "            )\n",
        "            tot_variations-=1 # for naming, which needs to change anyway\n",
        "\n",
        "\n",
        "        # TODO: async this, as well as the image saving\n",
        "        # TODO: append entry to a scenes.txt for the scene here. This gets us around cluttering the storyboard with unneeded file paths.\n",
        "        #       ...I should probably stop doing everything in text files and just bite the bullet and use a database. export a text file \n",
        "        #       for the user which can be loaded, but persist the granular information I want here in relational tables.\n",
        "        \n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "# TODO: parallelize generations across scenes. \n",
        "#       i.e. maybe it's non trivial to parallelize generations within a scene,\n",
        "#       but we can do several scenes in parallel. need more control over how\n",
        "#       the animator constructs/emits requests. want it to construct requests\n",
        "#       for several scenes, then multiplex the request objects into a single\n",
        "#       larger request, which would then need to have responses demuxed\n",
        "\n",
        "# TODO: [keyframed ] max-binning/aggregation (sync) functionality\n",
        "#       ... or is this something it already supports?\n",
        "# TODO: [keyframed] lag aggregation\n",
        "# TODO: [keyframed] \"simplify\" operation\n",
        "# TODO: [keyframed] ability to specify operations relative to keyframe indices, e.g. interpolate from V to 0 over K frames for each keyframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cEwFI6kA_2SH",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title ## ðŸ“º Compile your video and enjoy your animation!\n",
        "\n",
        "# TODO: print output path\n",
        "\n",
        "# TODO: skip tsp if n_variations ==1\n",
        "\n",
        "# TODO: change frame-write names so this will be compatible with an ffmpeg one-liner to generate preview animations\n",
        "#       - mixed feelings here. \n",
        "\n",
        "# TODO: ffmpeg script conditional on animation mode\n",
        "\n",
        "########################\n",
        "# rendering parameters #\n",
        "########################\n",
        "\n",
        "# @markdown `add_caption` - Whether or not to overlay the prompt text on the image\n",
        "\n",
        "# @markdown  `upscale`: Naively (lanczos interpolation) upscale video 2x. This can be a way to force\n",
        "# @markdown  services like youtube to deliver your video without mangling it with compression\n",
        "# @markdown  artifacts. Thanks [@gandamu_ml](https://twitter.com/gandamu_ml) for this trick!\n",
        "\n",
        "output_filename = 'output.mp4' # @param {type:'string'}\n",
        "add_caption = False # @param {type:'boolean'}\n",
        "upscale = False # @param {type:'boolean'}\n",
        "download_video = True # @param {type:'boolean'}\n",
        "\n",
        "# @markdown NB: Your video will probably download way faster from https://drive.google.com\n",
        "\n",
        "#########################\n",
        "\n",
        "final_output_filename = str( root / output_filename )\n",
        "storyboard.params.output_filename = final_output_filename\n",
        "\n",
        "\n",
        "#fps = storyboard.params.fps # TODO: change ffmpeg command so we don't need this\n",
        "\n",
        "\n",
        "#####################################\n",
        "\n",
        "from keyframed import Curve # sheesh...\n",
        "\n",
        "# prep everything...\n",
        "ffmpeg_cmd_script = \"\"\n",
        "for idx, rec in enumerate(storyboard.prompt_starts):\n",
        "    scene_fps = Curve(storyboard.params.fps)\n",
        "    if rec.get('dynamic_fps'): # should probably just call this fps...\n",
        "        scene_fps = curve_from_cn_string(rec['dynamic_fps'])\n",
        "        \n",
        "    im_paths = get_image_sequence(idx, root)\n",
        "    if rec.get('animation_mode') == 'variations tsp':\n",
        "        if rec.get('frame_order'):\n",
        "            im_paths = rec['frame_order']\n",
        "        else:\n",
        "            print(f\"computing frame order for scene {idx}\")\n",
        "            images = [Image.open(fp) for fp in im_paths]\n",
        "            try:\n",
        "                frame_order = tsp_sort(images)\n",
        "                im_paths = [im_paths[j] for j in frame_order]\n",
        "                images = [images[j] for j in frame_order]\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "            # TODO: persist frame order to storyboard for variations_tsp. or not? mixed feelings again.\n",
        "            rec['frame_order'] = im_paths\n",
        "            #save_storyboard(storyboard) # make sure we aren't doing this frame_order nonsense for fancy animations\n",
        "            \n",
        "    elif rec.get('animation_mode') == 'img2img':\n",
        "        # this is sort of a hack but it works for now\n",
        "        im_paths = sorted(im_paths, key=os.path.getmtime)\n",
        "\n",
        "    images = [Image.open(fp) for fp in im_paths]\n",
        "\n",
        "    if add_caption:\n",
        "        new_paths = []\n",
        "        #images_captioned = [add_caption2image(im, rec['prompt']) for im in images]\n",
        "        #images_captioned = [add_caption2image(im, rec['text']) for im in images]\n",
        "        #for fp, im in zip(im_paths, images_captioned):\n",
        "        for fp, im in zip(im_paths, images):\n",
        "            fp = Path(fp)\n",
        "            #fp = fp.with_stem(fp.stem + '-captioned')\n",
        "            fp = fp.parent / 'captioned' / fp.name\n",
        "            fp.parent.mkdir(exist_ok=True, parents=True)\n",
        "            if not rec.get('inferred_subscene', False):\n",
        "                im = add_caption2image(im, rec['text']) # TODO: separate \"caption\" attribute on the rec\n",
        "            im.save(fp)\n",
        "            new_paths.append(fp)\n",
        "        im_paths = new_paths\n",
        "    \n",
        "    try:\n",
        "        frame_picker = cycle(im_paths)\n",
        "        for frame_idx in range(rec.frames):\n",
        "            fpath = Path(next(frame_picker))\n",
        "            fps = scene_fps[frame_idx] # ain't I fancy # ... this was a good idea but requires that I compute things differently. deal with it later.\n",
        "            ffmpeg_cmd_script += f\"file '{fpath.absolute()}'\\nduration {1/fps}\\n\"\n",
        "\n",
        "        with open(root/'scenes.txt', 'w') as f:\n",
        "            f.write(ffmpeg_cmd_script)\n",
        "    except:\n",
        "        print(\"maybe missing some frames? your video might be incomplete, sorry. try the resuming procedure.\")\n",
        "        print()\n",
        "        break\n",
        "\n",
        "if upscale:\n",
        "    height=storyboard.params.height\n",
        "    width=storyboard.params.width\n",
        "    !ffmpeg -y -f concat -safe 0 -i {root/'scenes.txt'} -i \"{storyboard.params.audio_fpath}\" -r {storyboard.params.fps} -pix_fmt yuv420p -crf 25 -preset veryslow -vf scale={2*width}x{2*height}:flags=lanczos -shortest {storyboard.params.output_filename}\n",
        "else:\n",
        "    !ffmpeg -y -f concat -safe 0 -i {root/'scenes.txt'} -i \"{storyboard.params.audio_fpath}\" -r {storyboard.params.fps} -pix_fmt yuv420p -crf 25 -preset veryfast -shortest {storyboard.params.output_filename}\n",
        "\n",
        "\n",
        "# EASTER EGG FEATURE\n",
        "#  NB: only embed short videos\n",
        "embed_video_in_notebook = False\n",
        "\n",
        "output_filename = storyboard.params.output_filename\n",
        "\n",
        "if download_video and not local:\n",
        "    from google.colab import files\n",
        "    files.download(output_filename)\n",
        "\n",
        "if embed_video_in_notebook:\n",
        "    from IPython.display import display, Video\n",
        "    display(Video(output_filename, embed=True))\n",
        "\n",
        "#!ffmpeg -y -f concat -safe 0 -i {root/'scenes.txt'} -vn -i \"{video_assets_meta_record['video_fpath']}\" -r {storyboard.params.fps} -pix_fmt yuv420p -crf 25 -preset veryfast -shortest test_alt_audio.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVu_TleBiHhY"
      },
      "source": [
        "# âš–ï¸ I put on my robe and lawyer hat\n",
        "\n",
        "### Notebook license\n",
        "\n",
        "This notebook and the accompanying [git repository](https://github.com/dmarx/video-killed-the-radio-star/) and its contents are shared under the MIT license.\n",
        "\n",
        "<!-- Note to self: lawyers should really be forced to use some sort of markup or pseudocode to eliminate ambiguity \n",
        "\n",
        "...oh shit, if laws were actually described in code, we could just run queries against it\n",
        "-->\n",
        "\n",
        "```\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2022 David Marx\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "```\n",
        "\n",
        "### DreamStudio API TOS\n",
        "\n",
        "The default behavior of this notebook uses the [DreamStudio](https://beta.dreamstudio.ai/) API to generate images. Users of the DreamStudio API are subject to the DreamStudio usage terms: https://beta.dreamstudio.ai/terms-of-service\n",
        "\n",
        "### Stable Diffusion\n",
        "\n",
        "As of the date of this writing (2022-09-29), all publicly available model checkpoints are subject to the restrictions of the Open RAIL license: https://huggingface.co/spaces/CompVis/stable-diffusion-license. \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}