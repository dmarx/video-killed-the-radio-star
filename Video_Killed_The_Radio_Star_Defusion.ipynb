{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgXxoDhMAiti"
   },
   "source": [
    "# $ \\text{Video Killed The Radio Star}$ $\\color{red}{...Diffusion}$\n",
    "\n",
    "Notebook by David Marx ([@DigThatData](https://twitter.com/digthatdata))\n",
    "\n",
    "Shared under MIT license\n",
    "\n",
    "\n",
    "# $\\text{FAQ}$\n",
    "\n",
    "**What is this?**\n",
    "\n",
    "Point this notebook at a youtube url and it'll make a music video for you.\n",
    "\n",
    "**How does this animation technique work?**\n",
    "\n",
    "For each text prompt you provide, the notebook will...\n",
    "\n",
    "1. Generate an image based on that text prompt (using stable diffusion)\n",
    "2. Use the generated image as the `init_image` to recombine with the text prompt to generate variations similar to the first image. This produces a sequence of extremely similar images based on the original text prompt\n",
    "3. Images are then intelligently reordered to find the smoothest animation sequence of those frames\n",
    "3. This image sequence is then repeated to pad out the animation duration as needed\n",
    "\n",
    "The technique demonstrated in this notebook was inspired by a [video](https://www.youtube.com/watch?v=WJaxFbdjm8c) created by Ben Gillin.\n",
    "\n",
    "**How are lyrics transcribed?**\n",
    "\n",
    "This notebook uses openai's recently released 'whisper' model for performing automatic speech recognition. \n",
    "OpenAI was kind enough to offer several different sizes of this model which each have their own pros and cons. \n",
    "This notebook uses the largest whisper model for transcribing the actual lyrics. Additionally, we use the \n",
    "smallest model for performing the lyric segmentation. Neither of these models is perfect, but the results \n",
    "so far seem pretty decent.\n",
    "\n",
    "The first draft of this notebook relied on subtitles from youtube videos to determine timing, which was\n",
    "then aligned with user-provided lyrics. Youtube's automated captions are powerful and I'll update the\n",
    "notebook shortly to leverage those again, but for the time being we're just using whisper for everything\n",
    "and not referencing user-provided captions at all.\n",
    "\n",
    "**Something didn't work quite right in the transcription process. How do fix the timing or the actual lyrics?**\n",
    "\n",
    "The notebook is divided into several steps. Between each step, a \"storyboard\" file is updated. If you want to\n",
    "make modifications, you can edit this file directly and those edits should be reflected when you next load the\n",
    "file. Depending on what you changed and what step you run next, your changes may be ignored or even overwritten.\n",
    "Still playing with different solutions here.\n",
    "\n",
    "**Can I provide my own images to 'bring to life' and associate with certain lyrics/sequences?**\n",
    "\n",
    "Yes, you can! As described above: you just need to modify the storyboard. Will describe this functionality in\n",
    "greater detail after the implementation stabilizes a bit more.\n",
    "\n",
    "**This gave me an idea and I'd like to use just a part of your process here. What's the best way to reuse just some of the machinery you've developed here?**\n",
    "\n",
    "Most of the functionality in this notebook has been offloaded to library I published to pypi called `vktrs`. I strongly encourage you to import anything you need \n",
    "from there rather than cutting and pasting function into a notebook. Similarly, if you have ideas for improvements, please don't hesitate to submit a PR!\n",
    "\n",
    "**How can I support your work or work like it?**\n",
    "\n",
    "This notebook was made possible thanks to ongoing support from [stability.ai](https://stability.ai/). The best way to support my work is to share it with your friends, [report bugs](https://github.com/dmarx/video-killed-the-radio-star/issues/new), [suggest features](https://github.com/dmarx/video-killed-the-radio-star/discussions) or to donate to open source non-profits :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM147HP4kAdY"
   },
   "source": [
    "## $0.$ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ZnTe8clZuZuj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title # ðŸ“Š Check GPU Status\n",
    "\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "def gpu_info():\n",
    "    outv = subprocess.run([\n",
    "        'nvidia-smi',\n",
    "            # these lines concatenate into a single query string\n",
    "            '--query-gpu='\n",
    "            'timestamp,'\n",
    "            'name,'\n",
    "            'utilization.gpu,'\n",
    "            'utilization.memory,'\n",
    "            'memory.used,'\n",
    "            'memory.free,'\n",
    "            ,\n",
    "        '--format=csv'\n",
    "        ],\n",
    "        stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "\n",
    "    header, rec = outv.split('\\n')[:-1]\n",
    "    return pd.DataFrame({' '.join(k.strip().split('.')).capitalize():v for k,v in zip(header.split(','), rec.split(','))}, index=[0]).T\n",
    "\n",
    "gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cADf0Y3pk5Uj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# @title # ðŸ› ï¸ Install dependencies\n",
    "\n",
    "try: \n",
    "    import google.colab\n",
    "    local=False\n",
    "except:\n",
    "    local=True\n",
    "\n",
    "# TODO: pin versions\n",
    "    \n",
    "# local only additional dependencies\n",
    "if local:\n",
    "    %pip install pandas torch pillow beautifulsoup4 scipy toolz numpy lxml librosa scikit-learn\n",
    "\n",
    "# dependencies for both colab and local\n",
    "%pip install yt-dlp python-tsp stability-sdk diffusers transformers ftfy accelerate omegaconf\n",
    "%pip install openai-whisper panel huggingface_hub ipywidgets safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cM8cux9b7F4v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title # ðŸ”‘ Provide your API Key\n",
    "\n",
    "# TODO: rework instructions/description here\n",
    "# TODO: add support for whisper API\n",
    "\n",
    "# @markdown Running this cell will prompt you to enter your API Key below. \n",
    "\n",
    "# @markdown To get your API key, visit https://beta.dreamstudio.ai/membership\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown A note on security best practices: **don't publish your API key.**\n",
    "\n",
    "# @markdown We're using a form field designed for sensitive data like passwords.\n",
    "# @markdown This notebook does not save your API key in the notebook itself,\n",
    "# @markdown but instead loads your API Key into the colab environment. This way,\n",
    "# @markdown you can make changes to this notebook and share it without concern\n",
    "# @markdown that you might accidentally share your API Key. \n",
    "# @markdown \n",
    "\n",
    "use_stability_api = True # @param {type:'boolean'}\n",
    "mount_gdrive = True # @param {type:'boolean'}\n",
    "\n",
    "try: \n",
    "    import google.colab\n",
    "    local=False\n",
    "except:\n",
    "    local=True\n",
    "\n",
    "if local:\n",
    "    mount_gdrive=False\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import string\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "os.environ['XDG_CACHE_HOME'] = os.environ.get(\n",
    "    'XDG_CACHE_HOME',\n",
    "    str(Path('~/.cache').expanduser())\n",
    ")\n",
    "if mount_gdrive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    Path('/content/drive/MyDrive/AI/models/.cache/').mkdir(parents=True, exist_ok=True) \n",
    "    os.environ['XDG_CACHE_HOME']='/content/drive/MyDrive/AI/models/.cache'\n",
    "\n",
    "model_dir_str=str(Path(os.environ['XDG_CACHE_HOME']))\n",
    "proj_root_str = '${active_project}'\n",
    "application_root = str(Path('.').absolute())\n",
    "if mount_gdrive:\n",
    "    application_root = '/content/drive/MyDrive/AI/VideoKilledTheRadioStar'\n",
    "\n",
    "\n",
    "# notebook config\n",
    "cfg = OmegaConf.create({\n",
    "    'active_project':str(time.time()),\n",
    "    'application_root':application_root,\n",
    "    'project_root':\"${application_root}/${active_project}\",\n",
    "    'shared_assets_root':\"${application_root}/shared_assets\",\n",
    "    'gdrive_mounted':mount_gdrive,\n",
    "    'use_stability_api':use_stability_api,\n",
    "    'model_dir':model_dir_str,\n",
    "    'output_dir':'${project_root}/frames'\n",
    "})\n",
    "\n",
    "with open('config.yaml','w') as fp:\n",
    "    OmegaConf.save(config=cfg, f=fp.name)\n",
    "\n",
    "# TODO: fix name consistency\n",
    "workspace = cfg\n",
    "\n",
    "###################\n",
    "\n",
    "# add some tracking to reduce duplicated processing\n",
    "assets_dir = Path(cfg.shared_assets_root)\n",
    "assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# TODO: yaml -> jsonl ?\n",
    "video_assets_meta_fname = assets_dir / 'video_assets_meta.yaml'\n",
    "if not video_assets_meta_fname.exists():\n",
    "    video_assets_meta = OmegaConf.create()\n",
    "    video_assets_meta.videos = []\n",
    "    with video_assets_meta_fname.open('w') as fp:\n",
    "        OmegaConf.save(config=video_assets_meta, f=fp.name)\n",
    "else:\n",
    "    video_assets_meta = OmegaConf.load(video_assets_meta_fname)\n",
    "\n",
    "audio_assets_meta_fname = assets_dir / 'audio_assets_meta.yaml'\n",
    "if not audio_assets_meta_fname.exists():\n",
    "    audio_assets_meta = OmegaConf.create()\n",
    "    audio_assets_meta.content = []\n",
    "    with audio_assets_meta_fname.open('w') as fp:\n",
    "        OmegaConf.save(config=audio_assets_meta, f=fp.name)\n",
    "else:\n",
    "    audio_assets_meta = OmegaConf.load(audio_assets_meta_fname)\n",
    "\n",
    "###################\n",
    "\n",
    "if use_stability_api:\n",
    "    import os, getpass\n",
    "    if not os.environ.get('STABILITY_KEY'):\n",
    "        os.environ['STABILITY_KEY'] = getpass.getpass('Enter your Stability API Key, then press enter to continue')\n",
    "else:\n",
    "    # TODO: check for HF token in environment\n",
    "    if not local:\n",
    "        from google.colab import output\n",
    "        output.enable_custom_widget_manager()\n",
    "        \n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()\n",
    "    \n",
    "##########################\n",
    "\n",
    "# more environment stuff\n",
    "# create/load remaining folders/files\n",
    "\n",
    "assets_dir = Path(workspace.shared_assets_root)\n",
    "video_assets_meta_fname = assets_dir / 'video_assets_meta.yaml'\n",
    "video_assets_meta = OmegaConf.load(video_assets_meta_fname)\n",
    "audio_assets_meta_fname = assets_dir / 'audio_assets_meta.yaml'\n",
    "audio_assets_meta = OmegaConf.load(audio_assets_meta_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rt9Mu97fk_bp"
   },
   "source": [
    "## $1.b$ ðŸ“‹ Set Project Name (create/resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "s-9xjgy0iHhS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: combine this w workspace setup cell\n",
    "\n",
    "def sanitize_folder_name(fp):\n",
    "    outv = ''\n",
    "    whitelist = string.ascii_letters + string.digits + '-_'\n",
    "    for token in str(fp):\n",
    "        if token not in whitelist:\n",
    "            token = '-'\n",
    "        outv += token\n",
    "    return outv\n",
    "\n",
    "project_name = 'humans-r-ded' # @param {type:'string'}\n",
    "if not project_name:\n",
    "    project_name = str(time.time())\n",
    "\n",
    "project_name = sanitize_folder_name(project_name)\n",
    "\n",
    "workspace.active_project = project_name\n",
    "with open('config.yaml','w') as fp:\n",
    "    OmegaConf.save(config=workspace, f=fp.name)\n",
    "\n",
    "root = Path(workspace.project_root)\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "resuming=False\n",
    "try:\n",
    "    workspace, storyboard = load_storyboard()\n",
    "    print(\"loading storyboard\")\n",
    "    resuming=True\n",
    "except:\n",
    "    print(\"creating new storyboard\")\n",
    "    storyboard = OmegaConf.create()\n",
    "    storyboard.params = {}\n",
    "    \n",
    "# @markdown To create a new project, enter a unique project name.\n",
    "# @markdown If you leave `project_name` blank, the current unix timestamp will be used\n",
    "# @markdown  (seconds since 1970-01-01 00:00).\n",
    "\n",
    "# @markdown If you use the name of an existing project, the workspace will switch to that project.\n",
    "\n",
    "# @markdown Non-alphanumeric characters (excluding '-' and '_') will be replaced with hyphens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a. DEFINE ALL THE THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime as dt\n",
    "import gc\n",
    "import io\n",
    "from itertools import chain, cycle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from bokeh.models.widgets.tables import (\n",
    "    NumberFormatter, \n",
    "    BooleanFormatter,\n",
    "    CheckboxEditor,\n",
    ")\n",
    "from diffusers import (\n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    StableDiffusionPipeline,\n",
    ")\n",
    "from IPython.display import display\n",
    "import librosa\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from python_tsp.exact import solve_tsp_dynamic_programming\n",
    "from safetensors.numpy import save_file as save_safetensors\n",
    "from safetensors.numpy import load_file as load_safetensors\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import sklearn.cluster\n",
    "import textwrap\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torch\n",
    "from torch import autocast\n",
    "import whisper\n",
    "\n",
    "\n",
    "try: \n",
    "    import google.colab\n",
    "    local=False\n",
    "except:\n",
    "    local=True\n",
    "\n",
    "    \n",
    "def sanitize_folder_name(fp):\n",
    "    outv = ''\n",
    "    whitelist = string.ascii_letters + string.digits + '-_'\n",
    "    for token in str(fp):\n",
    "        if token not in whitelist:\n",
    "            token = '-'\n",
    "        outv += token\n",
    "    return outv\n",
    "\n",
    "# to do: is there a way to check if this is in the env already?\n",
    "#pn.extension('tabulator')\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "# wrap some of the loading logic for portability\n",
    "\n",
    "def save_storyboard(storyboard):\n",
    "    with open(storyboard_fname) as fp:\n",
    "        OmegaConf.save(config=storyboard, f=fp.name)\n",
    "\n",
    "def load_workspace():\n",
    "    return OmegaConf.load('config.yaml')\n",
    "        \n",
    "def load_storyboard():\n",
    "    workspace = load_workspace()\n",
    "    root = Path(workspace.project_root)\n",
    "    storyboard_fname = root / 'storyboard.yaml'\n",
    "    storyboard = OmegaConf.load(storyboard_fname)\n",
    "    return workspace, storyboard\n",
    "\n",
    "def load_audio_meta(workspace, storyboard):\n",
    "    assets_dir = Path(workspace.shared_assets_root)\n",
    "    audio_assets_meta_fname = assets_dir / 'audio_assets_meta.yaml'\n",
    "    audio_assets_meta = OmegaConf.load(audio_assets_meta_fname)\n",
    "    audio_meta=dict()\n",
    "    for idx, rec in enumerate(audio_assets_meta.content):\n",
    "        if rec.audio_fpath == storyboard.params.audio_fpath:\n",
    "            audio_meta = rec\n",
    "            break\n",
    "    return audio_meta\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "# EXTRA SEGMENTATION STUFF\n",
    "\n",
    "\n",
    "def calculate_interword_gaps(segment):\n",
    "    end_prev = -1\n",
    "    gaps = []\n",
    "    for word in segment['words']:\n",
    "        if end_prev < 0:\n",
    "            end_prev = word['end']\n",
    "            continue \n",
    "        gap = word['start'] - end_prev\n",
    "        gaps.append(gap)\n",
    "        end_prev = word['end']\n",
    "    return gaps \n",
    "\n",
    "def trivial_subsegmentation(segment, threshold=0, gaps=None):\n",
    "    \"\"\"\n",
    "    split on gaps in detected vocal activity. \n",
    "    Contiguity = gap between adjacent tokens is less than the input threshold.\n",
    "    \"\"\"\n",
    "    if gaps is None:\n",
    "        gaps = calculate_interword_gaps(seg)\n",
    "    out_segments = []\n",
    "    this_segment = [seg['words'][0]]\n",
    "    for word, preceding_pause in zip(seg['words'][1:], gaps):\n",
    "        if preceding_pause <= threshold:\n",
    "            this_segment.append(word)\n",
    "        else:\n",
    "            out_segments.append(this_segment)\n",
    "            this_segment = [word]\n",
    "    out_segments.append(this_segment)\n",
    "\n",
    "    outv = [dict(\n",
    "        start=seg[0]['start'],\n",
    "        end=seg[-1]['end'],\n",
    "        text=''.join([w['word'] for w in seg]).strip(),\n",
    "    ) for seg in out_segments]\n",
    "\n",
    "    return outv\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# audio processing\n",
    "\n",
    "\n",
    "def analyze_audio_structure(\n",
    "    audio_fpath,\n",
    "    BINS_PER_OCTAVE = 12 * 3, # should be a multiple of twelve: https://github.com/MTG/essentia/blob/master/src/examples/python/tutorial_spectral_constantq-nsg.ipynb\n",
    "    N_OCTAVES = 7,\n",
    "):\n",
    "    \"\"\"\n",
    "    via librosa docs\n",
    "    https://librosa.org/doc/latest/auto_examples/plot_segmentation.html#sphx-glr-auto-examples-plot-segmentation-py\n",
    "    cites: McFee and Ellis, 2014 - https://brianmcfee.net/papers/ismir2014_spectral.pdf\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_fpath)\n",
    "\n",
    "    C = librosa.amplitude_to_db(np.abs(librosa.cqt(y=y, sr=sr,\n",
    "                                            bins_per_octave=BINS_PER_OCTAVE,\n",
    "                                            n_bins=N_OCTAVES * BINS_PER_OCTAVE)),\n",
    "                                ref=np.max)\n",
    "\n",
    "    # reduce dimensionality via beat-synchronization\n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr, trim=False)\n",
    "    Csync = librosa.util.sync(C, beats, aggregate=np.median)\n",
    "    \n",
    "    # I have concerns about this frame fixing operation\n",
    "    beat_times = librosa.frames_to_time(librosa.util.fix_frames(beats, x_min=0), sr=sr)\n",
    "\n",
    "    # width=3 prevents links within the same bar \n",
    "    # mode=â€™affinityâ€™ here implements S_rep (after Eq. 8)\n",
    "    R = librosa.segment.recurrence_matrix(Csync, width=3, mode='affinity', sym=True)\n",
    "    # Enhance diagonals with a median filter (Equation 2)\n",
    "    df = librosa.segment.timelag_filter(scipy.ndimage.median_filter)\n",
    "    Rf = df(R, size=(1, 7))\n",
    "    # build the sequence matrix (S_loc) using mfcc-similarity\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    Msync = librosa.util.sync(mfcc, beats)\n",
    "    path_distance = np.sum(np.diff(Msync, axis=1)**2, axis=0)\n",
    "    sigma = np.median(path_distance)\n",
    "    path_sim = np.exp(-path_distance / sigma)\n",
    "    R_path = np.diag(path_sim, k=1) + np.diag(path_sim, k=-1)\n",
    "    # compute the balanced combination\n",
    "    deg_path = np.sum(R_path, axis=1)\n",
    "    deg_rec = np.sum(Rf, axis=1)\n",
    "    mu = deg_path.dot(deg_path + deg_rec) / np.sum((deg_path + deg_rec)**2)\n",
    "    A = mu * Rf + (1 - mu) * R_path\n",
    "\n",
    "    # compute normalized laplacian and its spectrum\n",
    "    L = scipy.sparse.csgraph.laplacian(A, normed=True)\n",
    "    evals, evecs = scipy.linalg.eigh(L)\n",
    "    # clean this up with a median filter. can help smooth over discontinuities\n",
    "    evecs = scipy.ndimage.median_filter(evecs, size=(9, 1))\n",
    "    return dict(\n",
    "        y=y, \n",
    "        sr=np.array(sr).astype(np.uint32),\n",
    "        tempo=tempo,\n",
    "        beats=beats,\n",
    "        beat_times=beat_times,\n",
    "        evecs=evecs,\n",
    "    )\n",
    "\n",
    "    \n",
    "def laplacian_segmentation(\n",
    "    audio_fpath=None,\n",
    "    evecs=None,\n",
    "    n_clusters = 5,\n",
    "    n_spectral_features = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    segment audio by clustering a self-similarity matrix.\n",
    "    via librosa docs\n",
    "    https://librosa.org/doc/latest/auto_examples/plot_segmentation.html#sphx-glr-auto-examples-plot-segmentation-py\n",
    "    cites: McFee and Ellis, 2014 - https://brianmcfee.net/papers/ismir2014_spectral.pdf\n",
    "    \"\"\"\n",
    "    if evecs is None:\n",
    "        if audio_fpath is None:\n",
    "            raise Exception(\"One of `audio_fpath` or `evecs` must be provided\")\n",
    "        features = analyze_audio_structure(audio_fpath)\n",
    "        evecs = features['evecs']\n",
    "    \n",
    "    if n_spectral_features is None:\n",
    "        n_spectral_features = n_clusters\n",
    "\n",
    "    # cumulative normalization is needed for symmetric normalize laplacian eigenvectors\n",
    "    Cnorm = np.cumsum(evecs**2, axis=1)**0.5\n",
    "    k = n_spectral_features\n",
    "    X = evecs[:, :k] / Cnorm[:, k-1:k]\n",
    "\n",
    "    # use these k components to cluster beats into segments\n",
    "    KM = sklearn.cluster.KMeans(n_clusters=n_clusters, n_init=\"auto\")\n",
    "    seg_ids = KM.fit_predict(X)\n",
    "\n",
    "    return seg_ids #, beat_times, tempo\n",
    "\n",
    "\n",
    "# for video duration\n",
    "def get_audio_duration_seconds(audio_fpath):\n",
    "    outv = subprocess.run([\n",
    "        'ffprobe'\n",
    "        ,'-i',audio_fpath\n",
    "        ,'-show_entries', 'format=duration'\n",
    "        ,'-v','quiet'\n",
    "        ,'-of','csv=p=0'\n",
    "        ],\n",
    "        stdout=subprocess.PIPE\n",
    "        ).stdout.decode('utf-8')\n",
    "    return float(outv.strip())\n",
    "\n",
    "\n",
    "##########################################\n",
    "\n",
    "# animation stuff\n",
    "\n",
    "# TODO: update this stuff to reflect updates to API/sdk\n",
    "def get_image_for_prompt_sai(prompt, max_retries=5, **kargs):\n",
    "    stability_api = client.StabilityInference(\n",
    "        key=os.environ['STABILITY_KEY'], \n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # auto-retry if mitigation triggered\n",
    "    while max_retries:\n",
    "        try:\n",
    "            answers = stability_api.generate(prompt=prompt, **kargs)\n",
    "            response = process_response(answers)\n",
    "            for img in response:\n",
    "                yield img\n",
    "            break\n",
    "\n",
    "        # TODO: better regen handling\n",
    "        except RuntimeError:\n",
    "            print(\"runtime error\")\n",
    "            max_retries -= 1\n",
    "            warnings.warn(f\"mitigation triggered, retries remaining: {max_retries}\")\n",
    "\n",
    "def process_response(answers):\n",
    "    for resp in answers:\n",
    "        for artifact in resp.artifacts:\n",
    "            if artifact.finish_reason == generation.FILTER:\n",
    "                warnings.warn(\n",
    "                    \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                    \"Please modify the prompt and try again.\")\n",
    "                raise RuntimeError\n",
    "            if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "                img = Image.open(io.BytesIO(artifact.binary))\n",
    "                yield img\n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "# misc utils\n",
    "\n",
    "def rand_str(n_char=5):\n",
    "    return ''.join(random.choice(string.ascii_lowercase) for i in range(n_char))\n",
    "\n",
    "def save_frame(\n",
    "    img: Image,\n",
    "    idx:int=0,\n",
    "    root_path=Path('./frames'),\n",
    "    name=None,\n",
    "):\n",
    "    root_path.mkdir(parents=True, exist_ok=True)\n",
    "    if name is None:\n",
    "        name = rand_str()\n",
    "    outpath = root_path / f\"{idx}-{name}.png\"\n",
    "    img.save(outpath)\n",
    "    return str(outpath)\n",
    "\n",
    "def get_image_sequence(idx, root, init_first=True):\n",
    "    root = Path(root)\n",
    "    images = (root / 'frames' ).glob(f'{idx}-*.png')\n",
    "    images = [str(fp) for fp in images]\n",
    "    if init_first:\n",
    "        init_image = None\n",
    "        images2 = []\n",
    "        for i, fp in enumerate(images):\n",
    "            if 'anchor' in fp:\n",
    "                init_image = fp\n",
    "            else:\n",
    "                images2.append(fp)\n",
    "        if not init_image:\n",
    "            try:\n",
    "                init_image, images2 = images2[0], images2[1:]\n",
    "                images = [init_image] + images2\n",
    "            except IndexError:\n",
    "                images = images2\n",
    "    return images\n",
    "\n",
    "def archive_images(idx, root, archive_root = None):\n",
    "    root = Path(root)\n",
    "    if archive_root is None:\n",
    "        archive_root = root / 'archive'\n",
    "    archive_root = Path(archive_root)\n",
    "    archive_root.mkdir(parents=True, exist_ok=True)\n",
    "    old_images = get_image_sequence(idx, root=root)\n",
    "    if not old_images:\n",
    "        return\n",
    "    print(f\"moving {len(old_images)} old images for scene {idx} to {archive_root}\")\n",
    "    for old_fp in old_images:\n",
    "        old_fp = Path(old_fp)\n",
    "        im_name = Path(old_fp.name)\n",
    "        new_path = archive_root / im_name\n",
    "        if new_path.exists():\n",
    "            im_name = f\"{im_name.stem}-{time.time()}{im_name.suffix}\"\n",
    "            new_path = archive_root / im_name\n",
    "        old_fp.rename(new_path)\n",
    "\n",
    "\n",
    "############################\n",
    "\n",
    "# video compilation stuff\n",
    "\n",
    "# TODO: Sorting algorithm that can tolerate more than 15-ish frames (GPU?)\n",
    "def tsp_sort(frames):\n",
    "    frames_m = np.array([np.array(f).ravel() for f in frames])\n",
    "    dmat = pdist(frames_m, metric='cosine')\n",
    "    dmat = squareform(dmat)\n",
    "    permutation, _ = solve_tsp_dynamic_programming(dmat)\n",
    "    return permutation\n",
    "\n",
    "def add_caption2image(\n",
    "      image, \n",
    "      caption, \n",
    "      text_font='LiberationSans-Regular.ttf', \n",
    "      font_size=20,\n",
    "      fill_color=(255, 255, 255),\n",
    "      stroke_color=(0, 0, 0), #stroke_fill\n",
    "      stroke_width=2,\n",
    "      align='center',\n",
    "      ):\n",
    "    # via https://stackoverflow.com/a/59104505/819544\n",
    "    wrapper = textwrap.TextWrapper(width=50) \n",
    "    word_list = wrapper.wrap(text=caption) \n",
    "    caption_new = ''\n",
    "    for ii in word_list[:-1]:\n",
    "        caption_new = caption_new + ii + '\\n'\n",
    "    caption_new += word_list[-1]\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Download the Font and Replace the font with the font file. \n",
    "    font = ImageFont.truetype(text_font, size=font_size)\n",
    "    w,h = draw.textsize(caption_new, font=font, stroke_width=stroke_width)\n",
    "    W,H = image.size\n",
    "    x,y = 0.5*(W-w),0.90*H-h\n",
    "    draw.text(\n",
    "        (x,y), \n",
    "        caption_new,\n",
    "        font=font,\n",
    "        fill=fill_color, \n",
    "        stroke_fill=stroke_color,\n",
    "        stroke_width=stroke_width,\n",
    "        align=align,\n",
    "    )\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## download video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "d_ = dict(\n",
    "    # all the underscore does is make it so each of the following lines can be preceded with a comma\n",
    "    # otw the first parameter would be offset from the other in the colab form\n",
    "    _=\"\"\n",
    "\n",
    "    , video_url = 'https://www.youtube.com/watch?v=ATFxVB4JFpQ' # @param {type:'string'}\n",
    "    , audio_fpath = '' # @param {type:'string'}\n",
    ")\n",
    "d_.pop('_')\n",
    "\n",
    "# @markdown `video_url` - URL of a youtube video to download as a source for audio and potentially for text transcription as well.\n",
    "\n",
    "# @markdown `audio_fpath` - Optionally provide an audio file instead of relying on a youtube download. Name it something other than 'audio.mp3', \n",
    "# @markdown                 otherwise it might get overwritten accidentally.\n",
    "\n",
    "\n",
    "storyboard.params = d_\n",
    "\n",
    "storyboard_fname = root / 'storyboard.yaml'\n",
    "with open(storyboard_fname,'wb') as fp:\n",
    "    OmegaConf.save(config=storyboard, f=fp.name)\n",
    "    \n",
    "\n",
    "###############################\n",
    "# Download audio from youtube #\n",
    "###############################\n",
    "\n",
    "# this should modify the existing record for the URL rather than creating a new one...\n",
    "force_redownload=False\n",
    "\n",
    "video_url = storyboard.params.video_url\n",
    "download_video=True\n",
    "\n",
    "if not force_redownload:\n",
    "    for rec in video_assets_meta.videos:\n",
    "        if rec.video_url == video_url:\n",
    "            if rec.get('video_fpath'):\n",
    "                print(\"previously downloaded video detected\")\n",
    "                download_video=False\n",
    "                # populate storyboard with previous processing results\n",
    "                if rec.get('audio_fpath'):\n",
    "                    storyboard.params.audio_fpath = rec.get('audio_fpath')\n",
    "            else:\n",
    "                download_video=True # should be redundant?\n",
    "            break\n",
    "            \n",
    "\n",
    "\n",
    "if download_video:\n",
    "    # check if user provided an audio filepath (or we already have one from youtube) before attempting to download\n",
    "    video_assets_meta_record = {}\n",
    "    video_assets_meta_record['video_url'] = video_url\n",
    "\n",
    "    ytdl_prefix = \"DOWNLOADED__\"\n",
    "    ytdl_fname = f\"{str(assets_dir / ytdl_prefix)}%(title)s.%(ext)s\"\n",
    "\n",
    "    !yt-dlp -o \"{ytdl_fname}\" {video_url}\n",
    "\n",
    "    matched_files = assets_dir.glob(ytdl_prefix+\"*\")\n",
    "    most_recent_file = max(matched_files, key=os.path.getctime)\n",
    "    print(f\"downloaded: {most_recent_file}\")\n",
    "    ytdl_fname = most_recent_file\n",
    "\n",
    "    video_assets_meta_record['video_fpath'] = str(ytdl_fname.absolute())\n",
    "\n",
    "    audio_fpath = ytdl_fname.with_suffix('.m4a')\n",
    "    input_audio = ytdl_fname\n",
    "    !ffmpeg -y -i \"{input_audio}\" -vn -c:a aac \"{audio_fpath}\"\n",
    "\n",
    "    storyboard.params.audio_fpath = audio_fpath\n",
    "    video_assets_meta_record['audio_fpath'] = str(audio_fpath.absolute())\n",
    "    \n",
    "    video_assets_meta.videos.append(video_assets_meta_record)\n",
    "        \n",
    "with open(storyboard_fname,'wb') as fp:\n",
    "    OmegaConf.save(config=storyboard, f=fp.name)\n",
    "\n",
    "with open(video_assets_meta_fname, 'wb') as fp:\n",
    "    OmegaConf.save(config=video_assets_meta, f=fp.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eTPNhcBomtL"
   },
   "source": [
    "## $2.$ ðŸ”Š Infer speech from audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9zT0u4-q_fMF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "# ðŸ’¬ Transcribe and segment speech using whisper #\n",
    "###################################################\n",
    "\n",
    "audio_fpath = Path(storyboard.params.audio_fpath)\n",
    "\n",
    "force_retranscription = False # @param {type:'boolean'}\n",
    "override_storyboard_transcription = False\n",
    "\n",
    "whisper_seg = None\n",
    "audio_meta={}\n",
    "for audio_meta in audio_assets_meta.content:\n",
    "    if (audio_fpath) and (audio_meta.audio_fpath == audio_fpath):\n",
    "        print(\"previously processed audio detected\")\n",
    "\n",
    "        whisper_seg_fpath = Path(audio_meta.whisper_segmentation)\n",
    "\n",
    "        with whisper_seg_fpath.open() as f:\n",
    "            timings = json.load(f)\n",
    "        whisper_seg = timings['segments']\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "if force_retranscription or (whisper_seg is None):\n",
    "    audio_meta['audio_fpath'] = storyboard.params.audio_fpath\n",
    "    # outputs text files as audio.* locally\n",
    "    !whisper --model large --word_timestamps True -o {str(assets_dir)} \"{storyboard.params.audio_fpath}\"\n",
    "\n",
    "    whisper_seg_fpath = Path(storyboard.params.audio_fpath).with_suffix('.json')\n",
    "    audio_meta['whisper_segmentation'] = str(whisper_seg_fpath)\n",
    "    audio_meta['duration'] = get_audio_duration_seconds(audio_fpath)\n",
    "    \n",
    "    with whisper_seg_fpath.open() as f:\n",
    "        timings = json.load(f)\n",
    "    whisper_seg = timings['segments']\n",
    "    \n",
    "\n",
    "    # checkpoint new audio processing metadata\n",
    "    \n",
    "    new_audio_meta = copy.deepcopy(audio_meta)\n",
    "    new_content = [new_audio_meta]\n",
    "    for item in audio_assets_meta.content:\n",
    "        if item.audio_fpath != new_audio_meta['audio_fpath']:\n",
    "            new_content.append(new_audio_meta)\n",
    "\n",
    "    audio_assets_meta.content = new_content\n",
    "    with open(audio_assets_meta_fname, 'wb') as fp:\n",
    "        OmegaConf.save(config=audio_assets_meta, f=fp.name)\n",
    "    \n",
    "if not storyboard.get('prompt_starts') or override_storyboard_transcription:\n",
    "    \n",
    "    # Ta da!\n",
    "    storyboard.prompt_starts = whisper_seg\n",
    "    \n",
    "    storyboard.params['video_duration'] = audio_meta['duration']\n",
    "    # unsure if below method is reliable.\n",
    "    #storyboard.params['video_duration'] = storyboard.prompt_starts[-1]['end']\n",
    "\n",
    "prompt_starts = storyboard.prompt_starts\n",
    "\n",
    "### checkpoint the processing work we've done to this point\n",
    "\n",
    "prompt_starts_copy = copy.deepcopy(prompt_starts)\n",
    "storyboard.prompt_starts = prompt_starts_copy\n",
    "\n",
    "with open(storyboard_fname) as fp:\n",
    "    OmegaConf.save(config=storyboard, f=fp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storyboard.prompt_starts[15]['text'] = \" And we poisoned their (donkeys).\"\n",
    "\n",
    "with open(storyboard_fname) as fp:\n",
    "    OmegaConf.save(config=storyboard, f=fp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Give user opportunity to correct the transcription at shared_asset level rather than project\n",
    "\n",
    "\n",
    "###############################\n",
    "# Review/Modify transcription #\n",
    "###############################\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown NB: When this cell finishes running, a table will appear\n",
    "# @markdown at the bottom of the output window. This table is editable\n",
    "# @markdown and can be used to correct errors in the transcription.\n",
    "# @markdown\n",
    "# @markdown additionally, the `override_prompt` field can be used to provide an \n",
    "# @markdown alternative text prompt for image generation. If this feature is\n",
    "# @markdown used, both the lyric and the theme prompt (which you will specify \n",
    "# @markdown in the cell that follows this) will be ignored. If you want to use\n",
    "# @markdown an `override_prompt` and also want to stay on theme, you will have \n",
    "# @markdown to append the desired `theme_prompt` to the end of the \n",
    "# @markdown `override_prompt` manually.\n",
    "\n",
    "\n",
    "# # https://panel.holoviz.org/reference/widgets/Tabulator.html\n",
    "# pn.extension('tabulator') # I don't know that specifying 'tabulator' here is even necessary...\n",
    "\n",
    "# tabulator_formatters = {\n",
    "#     'bool': {'type': 'tickCross'}\n",
    "# }\n",
    "\n",
    "# # reset workspace\n",
    "# if 'df_regen' in locals():\n",
    "#     del df_regen\n",
    "    \n",
    "# try:\n",
    "#     prompt_starts = OmegaConf.to_container(prompt_starts)\n",
    "# except:\n",
    "#     print(\"huh. that's weird.\")\n",
    "#     print(prompt_starts)\n",
    "#     pass\n",
    "# df = pd.DataFrame(prompt_starts)[['start','end','text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Music Structure Analysis\n",
    "# - beat and tempo detection\n",
    "# - Self-similarity graph\n",
    "\n",
    "audio_structure_features = analyze_audio_structure(audio_fpath=storyboard.params.audio_fpath)\n",
    "audio_features_fpath = Path(storyboard.params.audio_fpath).with_suffix('.audio_features.safetensors')\n",
    "save_safetensors(audio_structure_features, audio_features_fpath)\n",
    "\n",
    "assets_dir = Path(workspace.shared_assets_root)\n",
    "for idx, rec in enumerate(audio_assets_meta.content):\n",
    "    if rec.audio_fpath == storyboard.params.audio_fpath:\n",
    "        audio_meta = rec\n",
    "        break\n",
    "audio_meta['structural_features'] = audio_features_fpath\n",
    "\n",
    "# TODO: Make sure we're not creating a duplicate metadata record here \n",
    "if 'audio_fpath' not in audio_meta:\n",
    "    audio_meta['audio_fpath'] = storyboard.params.audio_fpath\n",
    "    audio_assets_meta.content.append(audio_meta)\n",
    "\n",
    "\n",
    "with open(audio_assets_meta_fname, 'wb') as fp:\n",
    "    OmegaConf.save(config=audio_assets_meta, f=fp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### for adjusting start times\n",
    "\n",
    "structural_features = audio_structure_features\n",
    "\n",
    "beats = structural_features['beats']\n",
    "sr = int(structural_features['sr'])\n",
    "\n",
    "beat_times = librosa.frames_to_time(beats, sr=sr)\n",
    "\n",
    "scene_starts = [s['start'] for s in storyboard.prompt_starts]\n",
    "\n",
    "scene_starts_as_frames = librosa.time_to_frames(scene_starts, sr=sr)\n",
    "downbeat_indices = librosa.util.match_events(scene_starts_as_frames, beats)\n",
    "downbeat_times = beat_times[downbeat_indices]\n",
    "\n",
    "\n",
    "# TODO: colab params\n",
    "adjust_to_closest_beat = True \n",
    "only_adjust_down = False\n",
    "\n",
    "# TODO: markdown documentation/instructions\n",
    "# options:\n",
    "# - use closest beat for all scenes\n",
    "# - use beat if earlier than current scene start\n",
    "# - ..use onsets instead of beats for this part\n",
    "\n",
    "if adjust_to_closest_beat:\n",
    "    for idx, scene in enumerate(storyboard.prompt_starts):\n",
    "\n",
    "        if scene['start'] > downbeat_times[idx]:\n",
    "            print(f\"scene {idx}: {scene['start']} -> {downbeat_times[idx]}\")\n",
    "            scene['start'] = float(downbeat_times[idx])\n",
    "        elif (not only_adjust_down) and scene['start'] < downbeat_times[idx]:\n",
    "            print(f\"scene {idx}: {scene['start']} <-> {downbeat_times[idx]}\")\n",
    "            scene['start'] = float(downbeat_times[idx])\n",
    "\n",
    "save_storyboard(storyboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Subdivide unusually long scenes\n",
    "\n",
    "#TODO: uh.. probably need to do this step *before* structural segmentation\n",
    "\n",
    "# TODO: wrap these steps in functions for legibility/portability\n",
    "# TODO: make this threshold parameterizable via storyboard (and save analysis to storyboard)\n",
    "# TODO: use beat counts to estimate a smart scene duration\n",
    "\n",
    "# estimate parameters of scene duration distribution \n",
    "# TODO: use beat onsets/counts\n",
    "scene_durations = []\n",
    "scenes_ = []\n",
    "for idx, rec in enumerate(storyboard.prompt_starts):\n",
    "    rec=dict(rec)\n",
    "    if idx > 0:\n",
    "        # are we maybe doubling up 'start' time stamps? like there are more unique 'end's than 'start's?\n",
    "        duration = rec['start'] - prev['start']\n",
    "        prev['duration_'] = duration\n",
    "        scene_durations.append(duration)\n",
    "    prev = rec\n",
    "    scenes_.append(prev)\n",
    "    \n",
    "# handle last record\n",
    "else:\n",
    "    # here's the bug.\n",
    "    # TODO: swap out rec['end'] -> rec_prev['start'] here\n",
    "    rec['duration_'] = rec['end'] - rec['start']\n",
    "    scenes_.append(rec)\n",
    "\n",
    "\n",
    "mu = sum(scene_durations)/len(scene_durations)\n",
    "sigma = np.std(scene_durations)\n",
    "\n",
    "# 1sd filter to concentrate on mode\n",
    "scene_durations2 = [s for s in scene_durations if (mu - sigma) < s < (mu+sigma)]\n",
    "mu2 = sum(scene_durations2)/len(scene_durations2)\n",
    "sigma2 = np.std(scene_durations2)\n",
    "\n",
    "###########\n",
    "\n",
    "# break up \"outlier\" segments into smaller chunks\n",
    "# this heuristic could be improved with beat synchronization and onset detection.\n",
    "# ... also could probably leverage the 'end' time of the scene\n",
    "# TODO: hierarchical theme structure analysis\n",
    "# TODO: MSA segmentation for fully instrumental (i.e. arbitrary) audio\n",
    "threshhold = mu2 + sigma\n",
    "scenes = []\n",
    "#for rec in storyboard.prompt_starts:\n",
    "for rec in list(scenes_):\n",
    "    gap_remaining = rec['duration_']\n",
    "    while gap_remaining > threshhold:\n",
    "        #step = min(max(mu2-sigma, (np.random.normal() + mu2)*sigma), mu2+sigma)\n",
    "        step=mu2\n",
    "        step = float(step)\n",
    "        rec['duration_'] = step\n",
    "        ######### TODO: infer appropriate structural segmentation for new subscene\n",
    "        # Better yet: move structural analysis until after scene count has stabilized\n",
    "        new_rec = copy.deepcopy(rec)\n",
    "        new_rec['start'] = rec['start'] + step\n",
    "        # TODO: deal with new value here\n",
    "        #new_rec['end'] = ??\n",
    "        new_rec['duration_'] = step\n",
    "        ### maybe i could add a flag or something to clarify that this was an \"inferred\" subscene\n",
    "        new_rec['parent_scene'] = rec.get('uid') # something like this?\n",
    "        new_rec['inferred_subscene'] = True # or this?\n",
    "         \n",
    "        scenes.append(rec)\n",
    "        rec = new_rec\n",
    "        gap_remaining -= step\n",
    "    scenes.append(rec)\n",
    "\n",
    "    \n",
    "# TODO: Adjust to beats\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "storyboard.prompt_starts = scenes\n",
    "\n",
    "storyboard_fname = Path(workspace.project_root) / 'storyboard.yaml'\n",
    "with open(storyboard_fname,'wb') as fp:\n",
    "    OmegaConf.save(config=storyboard, f=fp.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theme -> Scene Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: rework instructions\n",
    "\n",
    "# @markdown if False, themes will be rotated sequentially such that no two adjacent frames\n",
    "# @markdown will use the same theme prompt (if multiple theme prompts were provided).\n",
    "infer_thematic_structure = True # @param {type:'boolean'}\n",
    "\n",
    "# @markdown `theme_prompt` - Text that will be appended to the end of each lyric, useful for e.g. applying a consistent aesthetic style\n",
    "\n",
    "# @param {type:'string'}\n",
    "theme_prompt = ( \n",
    "    \", Katsuhiro Otomo gundam mecha |  \"\n",
    "    \", 1980s transformers autobots | \"\n",
    "    \", paperclips! paperclips! | \"\n",
    "    \", robotics for beginners | \"\n",
    "    \", rusted industrial machinery \" # TODO: test with fewer than 5 themes\n",
    "    #\"abstract art, inspired by peter gabriel | \"\n",
    "    #\"abstract art, inspired by radiohead | \"\n",
    "    #\"abstract art, inspired by coldplay | \"\n",
    "    #\"abstract art, inspired by david bowie | \"\n",
    "    #\"abstract art, inspired by talking heads |\"\n",
    "    #\"abstract art, inspired by kusama | \"\n",
    "    #\"abstract art, inspired by alex grey \"\n",
    "    \n",
    ")\n",
    "storyboard.params.theme_prompt = theme_prompt\n",
    "themes = [prompt.strip() for prompt in theme_prompt.split('|') if prompt.strip()]\n",
    "\n",
    "if (len(themes) > 1):\n",
    "    if infer_thematic_structure:\n",
    "                \n",
    "        beat_times = audio_structure_features['beat_times']\n",
    "        evecs = audio_structure_features['evecs']\n",
    "        segment_labels = laplacian_segmentation(\n",
    "            evecs=evecs,\n",
    "            # TODO: publish these parameters to the user\n",
    "            n_clusters=len(themes), # be sure to explain this is an upper bound, stochastic\n",
    "            n_spectral_features=len(themes),\n",
    "        )\n",
    "        \n",
    "        # beatsynch scene start times\n",
    "\n",
    "        # TODO: swap out rec['end'] -> rec_prev['start'] here\n",
    "        for rec in storyboard.prompt_starts:\n",
    "            beat_indices = np.where((beat_times >= rec['start']) & (beat_times <= rec['end']))[0]\n",
    "            segments_this_interval = segment_labels[beat_indices]\n",
    "            if len(segments_this_interval) == 0:\n",
    "                dominant_label = 0\n",
    "            else:\n",
    "                dominant_label = int(np.argmax(np.bincount(segments_this_interval)))\n",
    "            rec['structural_segmentation_label'] = dominant_label\n",
    "            rec['_theme'] = themes[dominant_label]\n",
    "    else:\n",
    "        for rec in storyboard.prompt_starts:\n",
    "            rec['_theme'] = themes[idx % len(themes)]\n",
    "else:\n",
    "    for rec in storyboard.prompt_starts:\n",
    "        rec['_theme'] = theme_prompt\n",
    "\n",
    "## checkpoint\n",
    "with open(storyboard_fname) as fp:\n",
    "    OmegaConf.save(config=storyboard, f=fp.name)\n",
    "\n",
    "    \n",
    "df_themes = pd.DataFrame(storyboard.prompt_starts).rename(columns={'_theme':'theme', 'structural_segmentation_label':'theme_id'})[['start','end','text','theme_id', 'theme']]\n",
    "df_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @markdown (Optional) modify theme prompt without impacting structure label assignments\n",
    "\n",
    "# @markdown Themes will be assigned to the `structural_segmentation` label that maps to their ordering\n",
    "#           in the theme prompt. To change which theme goes where, simply modify the order in which\n",
    "#           they appear in your prompt.\n",
    "\n",
    "theme_prompt = ( \n",
    "    \", Katsuhiro Otomo gundam mecha |  \"\n",
    "    \", autobots 80s transformers | \"\n",
    "    \", paperclips! paperclips! | \"\n",
    "    \", robotics for beginners | \"\n",
    "    \", rusted industrial machinery \"     \n",
    ")\n",
    "storyboard.params.theme_prompt = theme_prompt\n",
    "themes = [prompt.strip() for prompt in theme_prompt.split('|') if prompt.strip()]\n",
    "\n",
    "# update theme-scene assignments\n",
    "for rec in storyboard.prompt_starts:\n",
    "    theme_idx = rec['structural_segmentation_label']\n",
    "    rec['_theme'] = themes[theme_idx]\n",
    "\n",
    "## checkpoint\n",
    "with open(storyboard_fname) as fp:\n",
    "    OmegaConf.save(config=storyboard, f=fp.name)\n",
    "\n",
    "# share results\n",
    "df_themes = pd.DataFrame(storyboard.prompt_starts).rename(columns={'_theme':'theme', 'structural_segmentation_label':'theme_id'})[['start','end','text','theme_id', 'theme']]\n",
    "df_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: give user opportunity to edit prompts/themes here\n",
    "\n",
    "# - separately edit caption/prompt\n",
    "# - give user opportunity to modify theme-structure mapping\n",
    "\n",
    "# pn.extension('tabulator') # I don't know that specifying 'tabulator' here is even necessary...\n",
    "# tabulator_formatters = {\n",
    "#     'bool': {'type': 'tickCross'}\n",
    "# }\n",
    "        \n",
    "#outv = pn.widgets.Tabulator(df_themes, formatters=tabulator_formatters)\n",
    "#outv\n",
    "    \n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install stability-sdk[anim_ui]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Assign ANIMATION parameters!\n",
    "\n",
    "# Run This cell to turn on the animation UI.\n",
    "\n",
    "from stability_sdk.api import Context\n",
    "from stability_sdk.animation import AnimationArgs, Animator\n",
    "\n",
    "# @markdown To get your API key visit https://dreamstudio.ai/account\n",
    "STABILITY_HOST = \"grpc.stability.ai:443\" #@param {type:\"string\"}\n",
    "STABILITY_KEY = os.environ.get('STABILITY_KEY')\n",
    "\n",
    "# Connect to Stability API\n",
    "context = Context(STABILITY_HOST, STABILITY_KEY)\n",
    "\n",
    "# Test the connection\n",
    "context.get_user_info()\n",
    "#print(\"Connection successful!\")\n",
    "\n",
    "###################################\n",
    "\n",
    "# @title ANIMATION Settings\n",
    "\n",
    "# @markdown Run this cell to reveal the settings UI grouped across several tabs. After entering values, move on to the next step.\n",
    "\n",
    "# @markdown To reset values to default, simply re-run this cell.\n",
    "\n",
    "show_documentation = True # @param {type:'boolean'}\n",
    "\n",
    "# #@markdown ####**Resume:**\n",
    "resume_timestring = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ####**Override Settings:**\n",
    "override_settings_path = \"\" #@param {type:\"string\"}\n",
    "\n",
    "###################\n",
    "\n",
    "from stability_sdk.animation import (\n",
    "    AnimationArgs,\n",
    "    Animator,\n",
    "    AnimationSettings,\n",
    "    BasicSettings,\n",
    "    CoherenceSettings,\n",
    "    ColorSettings,\n",
    "    DepthSettings,\n",
    "    InpaintingSettings,\n",
    "    Rendering3dSettings,\n",
    "    CameraSettings,\n",
    "    VideoInputSettings,\n",
    "    VideoOutputSettings,\n",
    ")\n",
    "\n",
    "args_generation = BasicSettings()\n",
    "args_animation = AnimationSettings()\n",
    "args_camera = CameraSettings()\n",
    "args_coherence = CoherenceSettings()\n",
    "args_color = ColorSettings()\n",
    "args_depth = DepthSettings()\n",
    "args_render_3d = Rendering3dSettings()\n",
    "args_inpaint = InpaintingSettings()\n",
    "args_vid_in = VideoInputSettings()\n",
    "args_vid_out = VideoOutputSettings()\n",
    "arg_objs = (\n",
    "    args_generation,\n",
    "    args_animation,\n",
    "    args_camera,\n",
    "    args_coherence,\n",
    "    args_color,\n",
    "    args_depth,\n",
    "    args_render_3d,\n",
    "    args_inpaint,\n",
    "    args_vid_in,\n",
    "    args_vid_out,\n",
    ")\n",
    "\n",
    "def _show_docs(component):\n",
    "    cols = []\n",
    "    for k, v in component.param.objects().items():\n",
    "        if k == 'name':\n",
    "            continue\n",
    "        col = pn.Column(v, v.doc)\n",
    "        cols.append(col)\n",
    "    return pn.Column(*cols)\n",
    "\n",
    "def build(component):\n",
    "    if show_documentation:\n",
    "        component = _show_docs(component)\n",
    "    return pn.Row(component, width=1000)\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "pn.Tabs(*[(a.name[:-5], build(a)) for a in arg_objs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optional: reload storyboard from disk\n",
    "workspace, storyboard = load_storyboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RTUFeyQqCfd"
   },
   "source": [
    "## $3.$ ðŸŽ¬ Animate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Sh514DGj_sua",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# @title ## ðŸŽ¨ Generate init images\n",
    "#####################################\n",
    "\n",
    "\n",
    "prompt_starts = storyboard.prompt_starts\n",
    "use_stability_api = workspace.use_stability_api\n",
    "model_dir = workspace.model_dir\n",
    "\n",
    "device = 'cuda'\n",
    "model_id = \"CompVis/stable-diffusion-v1-5\"\n",
    "download=True\n",
    "\n",
    "model_dir = workspace.model_dir\n",
    "model_path= str(Path(model_dir) / 'huggingface' / 'diffusers')\n",
    "\n",
    "\n",
    "if 'get_image_for_prompt' not in locals():\n",
    "\n",
    "    if use_stability_api:\n",
    "        import warnings\n",
    "        from stability_sdk import client\n",
    "        import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "\n",
    "\n",
    "        def get_image_for_prompt(prompt, max_retries=5, **kargs):\n",
    "            return get_image_for_prompt_sai(prompt, max_retries=5, **kargs)\n",
    "\n",
    "\n",
    "        # leverage stability API internal parallelism for batch variation requests\n",
    "        # TODO: make sure this behaves appropriately for regeneration on NSFW trigger. only regen as needed, not whole batch\n",
    "        def get_variations_w_init(prompt, init_image, n_variations=2, image_consistency=.7, **kargs):\n",
    "             return list(\n",
    "                 get_image_for_prompt(\n",
    "                     prompt=prompt, \n",
    "                     init_image=init_image, \n",
    "                     start_schedule=(1-image_consistency), \n",
    "                     #num_samples=n_variations,\n",
    "                     samples=n_variations,\n",
    "                     **kargs,\n",
    "                 )\n",
    "             )\n",
    "                        \n",
    "    else:\n",
    "\n",
    "        if download:\n",
    "            img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                revision=\"fp16\", \n",
    "                torch_dtype=torch.float16,\n",
    "                use_auth_token=True\n",
    "            )\n",
    "            img2img = img2img.to(device)\n",
    "            img2img.save_pretrained(model_path)\n",
    "        else:\n",
    "            img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "                model_path,\n",
    "                local_files_only=True\n",
    "            ).to(device)\n",
    "\n",
    "        text2img = StableDiffusionPipeline(\n",
    "            vae=img2img.vae,\n",
    "            text_encoder=img2img.text_encoder,\n",
    "            tokenizer=img2img.tokenizer,\n",
    "            unet=img2img.unet,\n",
    "            feature_extractor=img2img.feature_extractor,\n",
    "            scheduler=img2img.scheduler,\n",
    "            safety_checker=img2img.safety_checker,\n",
    "        )\n",
    "        text2img.enable_attention_slicing()\n",
    "        img2img.enable_attention_slicing()\n",
    "\n",
    "\n",
    "        def get_image_for_prompt_hf(\n",
    "            prompt,\n",
    "            **kwargs\n",
    "        ):\n",
    "            f = text2img if kwargs.get('image') is None else img2img\n",
    "            n_retries = 5\n",
    "            with autocast(device):\n",
    "                while n_retries > 0:\n",
    "                    n_retries-=1\n",
    "                    result = f(prompt, **kwargs)\n",
    "                    if not any(result.nsfw_content_detected):\n",
    "                        return result.images\n",
    "                    else:\n",
    "                        print(f\"nsfw content detectected. retries remaining: {n_retries}\")\n",
    "\n",
    "        def get_image_for_prompt(*args, **kargs):\n",
    "            if 'init_image' in kargs:\n",
    "                kargs['image'] = kargs.pop('init_image')\n",
    "            if 'start_schedule' in kargs:\n",
    "                kargs['strength'] = kargs.pop('start_schedule')\n",
    "            return get_image_for_prompt_hf(*args, **kargs)\n",
    "\n",
    "        # TODO: (HF) request multiple images in single request\n",
    "        def get_variations_w_init(prompt, init_image, **kargs):\n",
    "            return list(get_image_for_prompt(prompt=prompt, init_image=init_image, **kargs))\n",
    "\n",
    "\n",
    "##################\n",
    "##  PARAMETERS  ##\n",
    "##################\n",
    "\n",
    "d_ = dict(\n",
    "    _=''\n",
    "    , height = 512 # @param {type:'integer'}\n",
    "    , width = 512 # @param {type:'integer'}\n",
    "    # TODO: pretty sure can delete this\n",
    "    , display_frames_as_we_get_them = True # @param {type:'boolean'}\n",
    ")\n",
    "d_.pop('_')\n",
    "\n",
    "regenerate_all_init_images = True # @param {type:'boolean'}\n",
    "#regenerate_all_init_images = False # @param {type:'boolean'}\n",
    "\n",
    "# TODO: make this an integer\n",
    "prompt_lag = True # @param {type:'boolean'}\n",
    "\n",
    "# @markdown `prompt_lag` - Extend prompt with lyrics from previous frame. Can improve temporal consistency of narrative. \n",
    "# @markdown  Especially useful for lyrics segmented into short prompts.\n",
    "\n",
    "#TODO: check from storyboard\n",
    "\n",
    "# regenerate all images if the theme prompt has changed or user specifies\n",
    "#if d_['theme_prompt'] != storyboard.params.get('theme_prompt'):\n",
    "#    regenerate_all_init_images = True\n",
    "\n",
    "\n",
    "storyboard.params.update(d_)\n",
    "\n",
    "if regenerate_all_init_images:\n",
    "    for i, rec in enumerate(prompt_starts):\n",
    "        rec['frame0_fpath'] = None\n",
    "        archive_images(i, root=root)\n",
    "    print(\"archival process complete\")\n",
    "\n",
    "# anchor images will be regenerated if there's no associated frame0_fpath\n",
    "# regenerate specific images if\n",
    "# * manually tagged by user in df_regen\n",
    "# * associated fpath doesn't exist (i.e. deleted)\n",
    "# TODO: dump df_regen stuff\n",
    "# TODO: move archival stuff...\n",
    "if 'df_regen' in locals():\n",
    "    for i, _ in df_regen.iterrows():\n",
    "        rec = prompt_starts[i]\n",
    "        regen = not _['keep']\n",
    "        # need to check this elsewhere\n",
    "        if rec.get('frame0_fpath') is None:\n",
    "            regen = True\n",
    "        elif not Path(rec['frame0_fpath']).exists():\n",
    "            regen=True\n",
    "        if regen:\n",
    "            rec['frame0_fpath'] = None\n",
    "            print(rec)\n",
    "            archive_images(i, root=root)\n",
    "    print(\"archival process complete\")\n",
    "\n",
    "\n",
    "theme_prompts = storyboard.params.theme_prompt\n",
    "height = storyboard.params.height\n",
    "width = storyboard.params.width\n",
    "\n",
    "proj_name = workspace.active_project\n",
    "\n",
    "\n",
    "## Main loop ##\n",
    "\n",
    "print(\"Ensuring each prompt has an associated image\")\n",
    "for idx, rec in enumerate(prompt_starts):\n",
    "    #print(idx, rec)\n",
    "    theme = rec.get('_theme')\n",
    "    prompt = rec.get('prompt')\n",
    "    if not prompt:\n",
    "        prompt = f\"{rec['text']}, {theme}\"\n",
    "    \n",
    "        if prompt_lag and (idx > 0):\n",
    "            rec_prev = prompt_starts[idx -1]\n",
    "            prev_text = rec_prev.get('text')\n",
    "            if not prev_text:\n",
    "                prev_text = rec_prev.get('prompt').split(',')[0]\n",
    "            this_text = rec.get('text')\n",
    "            if this_text:\n",
    "                prompt = f\"{prev_text} {this_text}, {theme}\"\n",
    "            else:\n",
    "                prompt = rec_prev['_prompt']\n",
    "    rec['_prompt'] = prompt\n",
    "    \n",
    "    print(\n",
    "        f\"scene: {idx}\\t time: {rec['start']}\\n\"\n",
    "        f\"spoken text: {rec.get('text')}\\n\"\n",
    "        f\"image prompt: {rec['_prompt']}\\n\"\n",
    "    )\n",
    "    if rec.get('frame0_fpath') is None:\n",
    "        init_image = list(get_image_for_prompt(\n",
    "              rec['_prompt'],\n",
    "              height=height,\n",
    "              width=width,\n",
    "              )\n",
    "          )[0]\n",
    "    # this shouldn't be necessary, but is a consequence of\n",
    "    # the globbing thing we're doing atm\n",
    "    if 'anchor' not in str(rec.get('frame0_fpath')):\n",
    "        # TODO: save_frame doesn't need to be a function.\n",
    "        rec['frame0_fpath'] = save_frame(\n",
    "            init_image,\n",
    "            idx,\n",
    "            root_path = root / 'frames',\n",
    "            name='anchor',\n",
    "            )\n",
    "\n",
    "        print(rec.get('text'))\n",
    "        display(init_image)\n",
    "\n",
    "\n",
    "##############\n",
    "# checkpoint #\n",
    "##############\n",
    "\n",
    "# TODO: recognize previous generations\n",
    "\n",
    "storyboard.prompt_starts = prompt_starts\n",
    "\n",
    "with open(storyboard_fname) as fp:\n",
    "    OmegaConf.save(config=storyboard, f=fp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For outputting a storyboard for ChatGPT prompting\n",
    "\n",
    "# import rich\n",
    "# import json\n",
    "# #rich.print(dict(storyboard))\n",
    "\n",
    "# outter_cols = ['audio_fpath','video_duration']\n",
    "\n",
    "# cols = ['start','text', 'structural_segmentation_label'] #,'no_speech_prob','avg_logprob','compression_ratio']\n",
    "# recs = [{k:rec[k] for k in cols} for rec in storyboard['prompt_starts'] if rec.get('inferred_subscene') is None]\n",
    "# recs = [{k:storyboard.params[k]} for k in outter_cols] + recs\n",
    "# rich.print(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#################################################\n",
    "# Math                                          #\n",
    "#                                               #\n",
    "#    This block computes how many frames are    #\n",
    "#    needed for each segment based on the start #\n",
    "#    times for each prompt                      #\n",
    "#################################################\n",
    "\n",
    "# TODO: leverage previous beat detection, onsets, etc. for frame timings\n",
    "# - TODO: isolated cells for calculating and suggesting parameters (fps, n_variations)\n",
    "\n",
    "# TODO: experiment with tying instantaneous framerate to a music attribute (i.e. so motion can change speed mid animation)\n",
    "\n",
    "fps = 16 # @param {type:'integer'}\n",
    "storyboard.params.fps = fps\n",
    "\n",
    "ifps = 1/fps\n",
    "\n",
    "# estimate video end\n",
    "if not storyboard.params.get('video_duration'):\n",
    "    storyboard.params['video_duration'] = get_audio_duration_seconds(storyboard.params.audio_fpath)\n",
    "video_duration = storyboard.params['video_duration']\n",
    "\n",
    "# dummy prompt for last scene duration\n",
    "prompt_starts = OmegaConf.to_container(storyboard.prompt_starts)\n",
    "prompt_starts.append({'start':video_duration})\n",
    "\n",
    "# make sure we respect the duration of the previous phrase\n",
    "frame_start=0\n",
    "prompt_starts[0]['anim_start']=frame_start\n",
    "for i, rec in enumerate(prompt_starts[1:], start=1):\n",
    "    rec_prev = prompt_starts[i-1]\n",
    "    k=0\n",
    "    while (rec_prev['anim_start'] + k*ifps) < rec['start']:\n",
    "        k+=1\n",
    "    k-=1\n",
    "    rec_prev['frames'] = k\n",
    "    rec_prev['anim_duration'] = k*ifps\n",
    "    frame_start+=k*ifps\n",
    "    rec['anim_start']=frame_start\n",
    "\n",
    "# drop the dummy frame\n",
    "prompt_starts = prompt_starts[:-1]\n",
    "\n",
    "# to do: given a 0 duration prompt, assume its duration is captured in the next prompt \n",
    "#        and guesstimate a corrected prompt start time and duration \n",
    "\n",
    "\n",
    "##############\n",
    "# checkpoint #\n",
    "##############\n",
    "\n",
    "\n",
    "storyboard.prompt_starts = prompt_starts\n",
    "\n",
    "with open(storyboard_fname) as fp:\n",
    "    OmegaConf.save(config=storyboard, f=fp.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4AgMgvYusAo4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# @title ## ðŸš€ Generate animation frames\n",
    "\n",
    "# (variation animations)\n",
    "\n",
    "##################################\n",
    "# Generate animation frames #\n",
    "##################################\n",
    "\n",
    "d_ = dict(\n",
    "    _=''\n",
    "    , n_variations=8 # @param {type:'integer'}\n",
    "    , image_consistency=0.72 # @param {type:\"slider\", min:0, max:1, step:0.01}  \n",
    "    , max_video_duration_in_seconds = 300 # @param {type:'integer'}\n",
    ")\n",
    "d_.pop('_')\n",
    "\n",
    "\n",
    "# @markdown `fps` - Frames-per-second of generated animations\n",
    "\n",
    "# @markdown `n_variations` - How many unique variations to generate for a given text prompt. This determines the frequency of the visual \"pulsing\" effect\n",
    "\n",
    "# @markdown `image_consistency` - controls similarity between images generated by the prompt.\n",
    "# @markdown - 0: ignore the init image\n",
    "# @markdown - 1: true as possible to the init image\n",
    "\n",
    "# @markdown `max_video_duration_in_seconds` - Early stopping if you don't want to generate a video the full duration of the provided audio. Default = 5min.\n",
    "\n",
    "\n",
    "storyboard.params.update(d_)\n",
    "storyboard.params.max_frames = storyboard.params.fps * storyboard.params.max_video_duration_in_seconds\n",
    "\n",
    "image_consistency = storyboard.params.image_consistency\n",
    "max_frames = storyboard.params.max_frames\n",
    "n_variations = storyboard.params.n_variations\n",
    "\n",
    "\n",
    "# load init_images and generate variations as needed\n",
    "print(\"Fetching variations\")\n",
    "for idx, rec in enumerate(prompt_starts):\n",
    "    new_images = []\n",
    "    images_fpaths = get_image_sequence(idx, root=root)\n",
    "    curr_variation_count = len(images_fpaths)\n",
    "    print(f\"curr_variation_count:{curr_variation_count}\")\n",
    "    if curr_variation_count < n_variations:\n",
    "        prompt = rec['_prompt']\n",
    "\n",
    "        init_image = Image.open(rec['frame0_fpath'])\n",
    "        # TODO: user should be able to specify basically anything per-entry\n",
    "        # next line is here to permit user to specify more variations for a specific entry\n",
    "        tot_variations = rec.get('n_variations', n_variations)\n",
    "        tot_variations = min(tot_variations, rec['frames']) # don't generate variations we won't use\n",
    "        print(f\"tot_variations:{tot_variations}\")\n",
    "        tot_variations -= curr_variation_count  # only generate variations we still need\n",
    "        print(f\"tot_variations to request:{tot_variations}\")\n",
    "        \n",
    "        # why do we have a scene with 0 frames? something funny with start times overlapping i think.\n",
    "        # seems like the subsegmentation thing didn't fully update this part properly?\n",
    "        if tot_variations < 1:\n",
    "            continue\n",
    "        \n",
    "        image_variations = get_variations_w_init(\n",
    "            prompt=prompt, \n",
    "            init_image=init_image, \n",
    "            image_consistency=image_consistency,\n",
    "            n_variations=tot_variations,\n",
    "        )\n",
    "        for img in image_variations:\n",
    "            save_frame(\n",
    "                img,\n",
    "                idx,\n",
    "                root_path= root / 'frames',\n",
    "            )\n",
    "\n",
    "            display(img)\n",
    "\n",
    "                \n",
    "##############\n",
    "# checkpoint #\n",
    "##############\n",
    "\n",
    "storyboard.prompt_starts = prompt_starts\n",
    "\n",
    "with open(storyboard_fname) as fp:\n",
    "    OmegaConf.save(config=storyboard, f=fp.name)\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown Running this cell will generate as many variation frames as required \n",
    "# @markdown per `n_variations`. To trigger regeneration of images that didn't\n",
    "# @markdown generate correctly (e.g. because a nsfw classifier was triggered),\n",
    "# @markdown just delete those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fancy new animation\n",
    "\n",
    "\n",
    "#for frame in tqdm(animator.render(), initial=animator.start_frame_idx, total=args.max_frames):\n",
    "#    display.clear_output(wait=True)\n",
    "#    display.display(frame)\n",
    "    \n",
    "#animator.save_settings(f\"{timestring}_settings.txt\")\n",
    "#args.fps\n",
    "#args.seed\n",
    "#args.max_frames\n",
    "\n",
    "\n",
    "negative_prompt = ''\n",
    "negative_prompt_weight = -1\n",
    "\n",
    "\n",
    "#animator = get_animator(prompt, n_frames, init_image_fpath)\n",
    "\n",
    "from keyframed import SmoothCurve\n",
    "\n",
    "\n",
    "\n",
    "print(\"Fetching variations\")\n",
    "for idx, rec in enumerate(prompt_starts):\n",
    "    new_images = []\n",
    "    images_fpaths = get_image_sequence(idx, root=root)\n",
    "    images_fpaths = sorted(images_fpaths, key=os.path.getmtime)\n",
    "    \n",
    "    curr_variation_count = len(images_fpaths)\n",
    "    print(f\"curr_variation_count:{curr_variation_count}\")\n",
    "    n_variations = rec['frames']\n",
    "    if curr_variation_count < n_variations:\n",
    "        prompt = rec['_prompt']\n",
    "\n",
    "        #init_image = Image.open(rec['frame0_fpath'])\n",
    "        init_image_path = images_fpaths[-1]\n",
    "        \n",
    "        \n",
    "        # TODO: user should be able to specify basically anything per-entry\n",
    "        # next line is here to permit user to specify more variations for a specific entry\n",
    "        tot_variations = rec.get('n_variations', n_variations)\n",
    "        tot_variations = min(tot_variations, rec['frames']) # don't generate variations we won't use\n",
    "        print(f\"tot_variations:{tot_variations}\")\n",
    "        tot_variations -= curr_variation_count  # only generate variations we still need\n",
    "        print(f\"tot_variations to request:{tot_variations}\")\n",
    "        \n",
    "        # why do we have a scene with 0 frames? something funny with start times overlapping i think.\n",
    "        # seems like the subsegmentation thing didn't fully update this part properly?\n",
    "        if tot_variations < 1:\n",
    "            continue\n",
    "        \n",
    "        # image_variations = get_variations_w_init(\n",
    "        #     prompt=prompt, \n",
    "        #     init_image=init_image, \n",
    "        #     image_consistency=image_consistency,\n",
    "        #     n_variations=tot_variations,\n",
    "        # )\n",
    "        # for img in image_variations:\n",
    "        #     save_frame(\n",
    "        #         img,\n",
    "        #         idx,\n",
    "        #         root_path= root / 'frames',\n",
    "        #     )\n",
    "        \n",
    "        ##############################\n",
    "        \n",
    "        # grab baseline animation args from UI\n",
    "        args_d = {}\n",
    "        [args_d.update(a.param.values()) for a in arg_objs]\n",
    "        args=AnimationArgs(**args_d)\n",
    "        \n",
    "        # set major args based on theme label\n",
    "        # use modulo of idx to toggle options\n",
    "        sign = (-1)**(idx%2)\n",
    "        theme_id = rec['structural_segmentation_label']\n",
    "        value = sign * (theme_id+1) * 0.1\n",
    "        args.translation_x = f\"0:({value})\"\n",
    "        args.rotation_y = f\"0:({-value})\"\n",
    "\n",
    "        args.max_frames = tot_variations\n",
    "        args.fps = storyboard.params.fps\n",
    "        args.init_image = init_image_path\n",
    "        animation_prompts = {0:prompt}\n",
    "        \n",
    "        animator = Animator(\n",
    "            api_context=context,\n",
    "            animation_prompts=animation_prompts,\n",
    "            args=args,\n",
    "            out_dir=None, #out_dir,    \n",
    "            negative_prompt=negative_prompt,\n",
    "            negative_prompt_weight=negative_prompt_weight,\n",
    "            resume=len(resume_timestring) != 0)\n",
    "\n",
    "        #im_paths = []\n",
    "        im_paths = images_fpaths[:]\n",
    "        #j=0 # not super resume friendly\n",
    "        for img in tqdm(animator.render(), initial=animator.start_frame_idx, total=args.max_frames):\n",
    "            from IPython.display import clear_output\n",
    "            #display.clear_output(wait=True)\n",
    "            #display.display(frame)\n",
    "            clear_output(wait=True)\n",
    "            display(img)\n",
    "            new_frame_fpath = save_frame(\n",
    "                img, idx, root_path=root/'frames',\n",
    "                name=f\"{tot_variations}\" # it'll be reversed, but it's not nothing\n",
    "            )\n",
    "            tot_variations-=1\n",
    "            im_paths.append(new_frame_fpath)\n",
    "        rec['frame_order'] = im_paths\n",
    "\n",
    "        # checkpoint ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cEwFI6kA_2SH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title ## ðŸ“º Compile your video and enjoy your animation!\n",
    "\n",
    "# to do: skip tsp if n_variations ==1\n",
    "\n",
    "\n",
    "########################\n",
    "# rendering parameters #\n",
    "########################\n",
    "\n",
    "output_filename = 'output.mp4' # @param {type:'string'}\n",
    "#add_caption = False # @param {type:'boolean'}\n",
    "add_caption = True\n",
    "## TODO: DEBUGGING\n",
    "optimal_ordering = False # @param {type:'boolean'} \n",
    "#optimal_ordering = False # @param {type:'boolean'}\n",
    "#upscale = True # @param {type:'boolean'}\n",
    "upscale = True #False\n",
    "\n",
    "download_video = True # @param {type:'boolean'}\n",
    "\n",
    "use_order_images_created = True\n",
    "\n",
    "# @markdown NB: Your video will probably download way faster from https://drive.google.com\n",
    "\n",
    "\n",
    "# @markdown `add_caption` - Whether or not to overlay the prompt text on the image\n",
    "\n",
    "# @markdown `optimal_ordering` - Intelligently permutes animation frames to provide a smoother animation.\n",
    "\n",
    "# @markdown  `upscale`: Naively (lanczos interpolation) upscale video 2x. This can be a way to force\n",
    "# @markdown  services like youtube to deliver your video without mangling it with compression\n",
    "# @markdown  artifacts. Thanks [@gandamu_ml](https://twitter.com/gandamu_ml) for this trick!\n",
    "\n",
    "final_output_filename = str( root / output_filename )\n",
    "storyboard.params.output_filename = final_output_filename\n",
    "\n",
    "\n",
    "fps = storyboard.params.fps\n",
    "\n",
    "\n",
    "#####################################\n",
    "\n",
    "\n",
    "# prep everything...\n",
    "ffmpeg_cmd_script = \"\"\n",
    "for idx, rec in enumerate(storyboard.prompt_starts):\n",
    "    #if 'frame_order' not in rec:\n",
    "    if True:\n",
    "        im_paths = get_image_sequence(idx, root)\n",
    "        # order by created date for fancy animation\n",
    "        im_paths = sorted(im_paths, key=os.path.getmtime)\n",
    "\n",
    "        if optimal_ordering:\n",
    "            print(f\"computing frame order for scene {idx}\")\n",
    "            images = [Image.open(fp) for fp in im_paths]\n",
    "            try:\n",
    "                frame_order = tsp_sort(images)\n",
    "                im_paths = [im_paths[j] for j in frame_order]\n",
    "                images = [images[j] for j in frame_order]\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "        # TODO: actually persist frame order to storyboard...\n",
    "        rec['frame_order'] = im_paths\n",
    "    else:\n",
    "        im_paths = rec['frame_order']\n",
    "\n",
    "    images = [Image.open(fp) for fp in im_paths]\n",
    "\n",
    "    if add_caption:\n",
    "        new_paths = []\n",
    "        #images_captioned = [add_caption2image(im, rec['prompt']) for im in images]\n",
    "        #images_captioned = [add_caption2image(im, rec['text']) for im in images]\n",
    "        #for fp, im in zip(im_paths, images_captioned):\n",
    "        for fp, im in zip(im_paths, images):\n",
    "            fp = Path(fp)\n",
    "            #fp = fp.with_stem(fp.stem + '-captioned')\n",
    "            fp = fp.parent / 'captioned' / fp.name\n",
    "            fp.parent.mkdir(exist_ok=True, parents=True)\n",
    "            if not rec.get('inferred_subscene', False):\n",
    "                im = add_caption2image(im, rec['text'])\n",
    "            im.save(fp)\n",
    "            new_paths.append(fp)\n",
    "        im_paths = new_paths\n",
    "    \n",
    "    frame_picker = cycle(im_paths)\n",
    "    for _ in range(rec.frames):\n",
    "        fpath = Path(next(frame_picker))\n",
    "        ffmpeg_cmd_script += f\"file '{fpath.absolute()}'\\nduration {1/fps}\\n\"\n",
    "    \n",
    "    with open(root/'scenes.txt', 'w') as f:\n",
    "        f.write(ffmpeg_cmd_script)\n",
    "\n",
    "\n",
    "if upscale:\n",
    "    height=storyboard.params.height\n",
    "    width=storyboard.params.width\n",
    "    !ffmpeg -y -f concat -safe 0 -i {root/'scenes.txt'} -i \"{storyboard.params.audio_fpath}\" -r {storyboard.params.fps} -pix_fmt yuv420p -crf 25 -preset veryslow -vf scale={2*width}x{2*height}:flags=lanczos -shortest {storyboard.params.output_filename}\n",
    "else:\n",
    "    !ffmpeg -y -f concat -safe 0 -i {root/'scenes.txt'} -i \"{storyboard.params.audio_fpath}\" -r {storyboard.params.fps} -pix_fmt yuv420p -crf 25 -preset veryfast -shortest {storyboard.params.output_filename}\n",
    "\n",
    "\n",
    "# EASTER EGG FEATURE\n",
    "#  NB: only embed short videos\n",
    "embed_video_in_notebook = False\n",
    "\n",
    "output_filename = storyboard.params.output_filename\n",
    "\n",
    "if download_video and not local:\n",
    "    from google.colab import files\n",
    "    files.download(output_filename)\n",
    "\n",
    "if embed_video_in_notebook:\n",
    "    from IPython.display import display, Video\n",
    "    display(Video(output_filename, embed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ffmpeg -y -f concat -safe 0 -i {root/'scenes.txt'} -vn -i \"{video_assets_meta_record['video_fpath']}\" -r {storyboard.params.fps} -pix_fmt yuv420p -crf 25 -preset veryfast -shortest test_alt_audio.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVu_TleBiHhY"
   },
   "source": [
    "# âš–ï¸ I put on my robe and lawyer hat\n",
    "\n",
    "### Notebook license\n",
    "\n",
    "This notebook and the accompanying [git repository](https://github.com/dmarx/video-killed-the-radio-star/) and its contents are shared under the MIT license.\n",
    "\n",
    "<!-- Note to self: lawyers should really be forced to use some sort of markup or pseudocode to eliminate ambiguity \n",
    "\n",
    "...oh shit, if laws were actually described in code, we could just run queries against it\n",
    "-->\n",
    "\n",
    "```\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 David Marx\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "```\n",
    "\n",
    "### DreamStudio API TOS\n",
    "\n",
    "The default behavior of this notebook uses the [DreamStudio](https://beta.dreamstudio.ai/) API to generate images. Users of the DreamStudio API are subject to the DreamStudio usage terms: https://beta.dreamstudio.ai/terms-of-service\n",
    "\n",
    "### Stable Diffusion\n",
    "\n",
    "As of the date of this writing (2022-09-29), all publicly available model checkpoints are subject to the restrictions of the Open RAIL license: https://huggingface.co/spaces/CompVis/stable-diffusion-license. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
