{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgXxoDhMAiti"
      },
      "source": [
        "# Video Killed The Radio Star ...Diffusion.\n",
        "\n",
        "Notebook by David Marx ([@DigThatData](https://twitter.com/digthatdata))\n",
        "\n",
        "Shared under MIT license\n",
        "\n",
        "\n",
        "## FAQ\n",
        "\n",
        "**What is this?**\n",
        "\n",
        "Point this notebook at a youtube url and it'll make a music video for you.\n",
        "\n",
        "**How does this animation technique work?**\n",
        "\n",
        "For each text prompt you provide, the notebook will...\n",
        "\n",
        "1. Generate an image based on that text prompt (using stable diffusion)\n",
        "2. Use the generated image as the `init_image` to recombine with the text prompt to generate variations similar to the first image. This produces a sequence of extremely similar images based on the original text prompt\n",
        "3. Images are then intelligently reordered to find the smoothest animation sequence of those frames\n",
        "3. This image sequence is then repeated to pad out the animation duration as needed\n",
        "\n",
        "The technique demonstrated in this notebook was inspired by a [video](https://www.youtube.com/watch?v=WJaxFbdjm8c) created by Ben Gillin.\n",
        "\n",
        "**How are lyrics transcribed?**\n",
        "\n",
        "This notebook uses openai's recently released 'whisper' model for performing automatic speech recognition. \n",
        "OpenAI was kind enough to offer several different sizes of this model which each have their own pros and cons. \n",
        "This notebook uses the largest whisper model for transcribing the actual lyrics. Additionally, we use the \n",
        "smallest model for performing the lyric segmentation. Neither of these models is perfect, but the results \n",
        "so far seem pretty decent.\n",
        "\n",
        "The first draft of this notebook relied on subtitles from youtube videos to determine timing, which was\n",
        "then aligned with user-provided lyrics. Youtube's automated captions are powerful and I'll update the\n",
        "notebook shortly to leverage those again, but for the time being we're just using whisper for everything\n",
        "and not referencing user-provided captions at all.\n",
        "\n",
        "**Something didn't work quite right in the transcription process. How do fix the timing or the actual lyrics?**\n",
        "\n",
        "The notebook is divided into several steps. Between each step, a \"storyboard\" file is updated. If you want to\n",
        "make modifications, you can edit this file directly and those edits should be reflected when you next load the\n",
        "file. Depending on what you changed and what step you run next, your changes may be ignored or even overwritten.\n",
        "Still playing with different solutions here.\n",
        "\n",
        "**Can I provide my own images to 'bring to life' and associate with certain lyrics/sequences?**\n",
        "\n",
        "Yes, you can! As described above: you just need to modify the storyboard. Will describe this functionality in\n",
        "greater detail after the implementation stabilizes a bit more.\n",
        "\n",
        "**This gave me an idea and I'd like to use just a part of your process here. What's the best way to reuse just some of the machinery you've developed here?**\n",
        "\n",
        "Most of the functionality in this notebook has been offloaded to library I published to pypi called `vktrs`. I strongly encourage you to import anything you need \n",
        "from there rather than cutting and pasting function into a notebook. Similarly, if you have ideas for improvements, please don't hesitate to submit a PR!\n",
        "\n",
        "**How can I support your work or work like it?**\n",
        "\n",
        "This notebook was made possible thanks to ongoing support from [stability.ai](https://stability.ai/). The best way to support my work is to share it with your friends, [report bugs](https://github.com/dmarx/video-killed-the-radio-star/issues/new), or to donate to open source non-profits :) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oPbeyWtesAoh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title # 0. Setup\n",
        "!pip install vktrs[api,hf]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZnTe8clZuZuj"
      },
      "outputs": [],
      "source": [
        "# @markdown # Check GPU Status\n",
        "\n",
        "from vktrs.utils import gpu_info\n",
        "\n",
        "gpu_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cM8cux9b7F4v"
      },
      "outputs": [],
      "source": [
        "# @title # 1. ðŸ”‘ Provide your API Key\n",
        "# @markdown Running this cell will prompt you to enter your API Key below. \n",
        "\n",
        "# @markdown To get your API key, visit https://beta.dreamstudio.ai/membership\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown A note on security best practices: **don't publish your API key.**\n",
        "\n",
        "# @markdown We're using a form field designed for sensitive data like passwords.\n",
        "# @markdown This notebook does not save your API key in the notebook itself,\n",
        "# @markdown but instead loads your API Key into the colab environment. This way,\n",
        "# @markdown you can make changes to this notebook and share it without concern\n",
        "# @markdown that you might accidentally share your API Key. \n",
        "# @markdown \n",
        "\n",
        "use_stability_api = True # @param {type:'boolean'}\n",
        "\n",
        "if use_stability_api:\n",
        "    import os, getpass\n",
        "    os.environ['STABILITY_KEY'] = getpass.getpass('Enter your API Key')\n",
        "else:\n",
        "    # use diffusers\n",
        "    !pip install diffusers\n",
        "    !pip install \"ipywidgets>=7,<8\"\n",
        "    !pip install transformers\n",
        "\n",
        "    !sudo apt -qq install git-lfs\n",
        "    !git config --global credential.helper store\n",
        "\n",
        "    from google.colab import output\n",
        "    from huggingface_hub import notebook_login\n",
        "\n",
        "    output.enable_custom_widget_manager()\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9zT0u4-q_fMF"
      },
      "outputs": [],
      "source": [
        "# @title # 2. ðŸ“‹ Animation parameters\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import string\n",
        "from subprocess import Popen, PIPE\n",
        "import textwrap\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import tokenizations\n",
        "import webvtt\n",
        "\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from itertools import chain, cycle\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "from vktrs.youtube import (\n",
        "    YoutubeHelper,\n",
        "    parse_timestamp,\n",
        "    vtt_to_token_timestamps,\n",
        "    srv2_to_token_timestamps,\n",
        ")\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "# to do: use project name to name file\n",
        "# to do: separate global params defined here from the storyboard object.\n",
        "#        users will not anticipate that updates here will destroy their work\n",
        "storyboard = OmegaConf.create()\n",
        "\n",
        "storyboard.params = dict(\n",
        "\n",
        "     video_url = 'https://www.youtube.com/watch?v=REojIUxX4rw' # @param {type:'string'}\n",
        "    #, audio_fpath = '' # @param {type:'string'} # TO DO: drop reliance on youtube for audio\n",
        "    , audio_fpath = None\n",
        "    , theme_prompt = \"extremely detailed, painted by ralph steadman and radiohead, beautiful, wow\" # @param {type:'string'}\n",
        "\n",
        "    , n_variations=5 # @param {type:'integer'}\n",
        "    , image_consistency=0.9 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "    , fps = 12 # @param {type:\"slider\", min:4, max:60, step:1}\n",
        "    , height = 512 # @param {type:'integer'}\n",
        "    , width = 512 # @param {type:'integer'}\n",
        "\n",
        "    , output_filename = 'output.mp4' # @param {type:'string'}\n",
        "    , add_caption = True # @param {type:'boolean'}\n",
        "    , display_frames_as_we_get_them = True # @param {type:'boolean'}\n",
        "\n",
        "    , optimal_ordering = True # @param {type:'boolean'}\n",
        "    , whisper_seg = True # @param {type:'boolean'}\n",
        "    , max_video_duration_in_seconds = 300 # @param {type:'integer'}\n",
        "\n",
        "    , max_variations_per_opt_pass=15\n",
        "    , use_stability_api = use_stability_api\n",
        "\n",
        ")\n",
        "\n",
        "if not storyboard.params.audio_fpath:\n",
        "    storyboard.params.audio_fpath = None\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown `video_url` - URL of a youtube video to download as a source for audio and potentially for text transcription as well.\n",
        "\n",
        "##################\n",
        "# markdown `audio_fpath` - Optionally provide an audio file instead of relying on a youtube download. Name it something other than 'audio.mp3', \n",
        "# markdown                 otherwise it might get overwritten accidentally.\n",
        "##################\n",
        "\n",
        "# @markdown `theme_prompt` - Text that will be appended to the end of each lyric, useful for e.g. applying a consistent aesthetic style\n",
        "\n",
        "\n",
        "# @markdown `n_variations` - How many unique variations to generate for a given text prompt\n",
        "\n",
        "# @markdown `image_consistency` - controls similarity between images generated by the prompt.\n",
        "# @markdown - 0: ignore the init image\n",
        "# @markdown - 1: true as possible to the init image\n",
        "\n",
        "# @markdown `fps` - Frames-per-second of generated animations\n",
        "\n",
        "\n",
        "# @markdown `output_filename` - filename your video will be saved to\n",
        "\n",
        "# @markdown `add_caption` - Whether or not to overlay the prompt text on the image\n",
        "\n",
        "# @markdown `display_frames_as_we_get_them` - Displaying frames will make the notebook slightly slower\n",
        "\n",
        "\n",
        "# @markdown `optimal_ordering` - Intelligently permutes animation frames to provide a smoother animation.\n",
        "\n",
        "# @markdown `whisper_seg` - Whether or not to use openai's whisper model for lyric segmentation. This is currently the only option, but that will change in a few days.\n",
        "\n",
        "# @markdown `max_video_duration_in_seconds` - Early stopping if you don't want to generate a video the full duration of the provided audio file. Defaults to 5 minutes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "storyboard.params.max_frames = storyboard.params.fps * storyboard.params.max_video_duration_in_seconds\n",
        "\n",
        "\n",
        "print(f\"Max total frames: {storyboard.params.max_frames}\")\n",
        "#print(f\"Max API requests: {int(max_frames/repeat)}\")\n",
        "\n",
        "if storyboard.params.optimal_ordering:\n",
        "\n",
        "    opt_batch_size = storyboard.params.n_variations\n",
        "    while opt_batch_size > storyboard.params.max_variations_per_opt_pass:\n",
        "        opt_batch_size /= 2\n",
        "    print(f\"Frames per re-ordering batch: {opt_batch_size}\")\n",
        "    storyboard.params.opt_batch_size = opt_batch_size\n",
        "\n",
        "storyboard_fname = 'storyboard.yaml'\n",
        "with open(storyboard_fname,'wb') as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8OaQYVfYgBH-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# @title # 3. ðŸ“¥ Download audio from youtube\n",
        "\n",
        "storyboard_fname = 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "\n",
        "video_url = storyboard.params.video_url\n",
        "\n",
        "if video_url:\n",
        "    # check if user provided an audio filepath (or we already have one from youtube) before attempting to download\n",
        "    if storyboard.params.get('audio_fpath') is None:\n",
        "        helper = YoutubeHelper(video_url)\n",
        "\n",
        "        input_audio = helper.info['requested_downloads'][-1]['filepath']\n",
        "        !ffmpeg -y -i \"{input_audio}\" -acodec libmp3lame audio.mp3\n",
        "\n",
        "        # to do: write audio and subtitle paths/meta to storyboard\n",
        "        storyboard.params.audio_fpath = 'audio.mp3'\n",
        "\n",
        "        if False:\n",
        "            subtitle_format = helper.info['requested_subtitles']['en']['ext']\n",
        "            subtitle_fpath = helper.info['requested_subtitles']['en']['filepath']\n",
        "\n",
        "            if subtitle_format == 'srv2':\n",
        "                with open(subtitle_fpath, 'r') as f:\n",
        "                    srv2_xml = f.read() \n",
        "                token_start_times = srv2_to_token_timestamps(srv2_xml)\n",
        "                # to do: handle timedeltas...\n",
        "                #storyboard.params.token_start_times = token_start_times\n",
        "\n",
        "            elif subtitle_format == 'vtt':\n",
        "                captions = webvtt.read(subtitle_fpath)\n",
        "                token_start_times = vtt_to_token_timestamps(captions)\n",
        "                # to do: handle timedeltas...\n",
        "                #storyboard.params.token_start_times = token_start_times\n",
        "\n",
        "            # If unable to download supported subtitles, force use whisper\n",
        "            else:\n",
        "                storyboard.params.whisper_seg = True\n",
        "\n",
        "# force use\n",
        "storyboard.params.whisper_seg = True\n",
        "\n",
        "with open(storyboard_fname,'wb') as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)\n",
        "\n",
        "whisper_seg = storyboard.params.whisper_seg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "73lfb0gZvGW5"
      },
      "outputs": [],
      "source": [
        "# @title # 4. ðŸ’¬ Transcribe and segment speech using whisper\n",
        "\n",
        "storyboard_fname = 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "\n",
        "if whisper_seg:\n",
        "    !pip install git+https://github.com/openai/whisper\n",
        "    from vktrs.asr import whisper_lyrics\n",
        "\n",
        "    prompt_starts = whisper_lyrics(audio_fpath=storyboard.params.audio_fpath)\n",
        "\n",
        "    #storyboard.prompt_starts = prompt_starts\n",
        "    # to do: deal with these td objects\n",
        "    #with open('storyboard.yaml') as fp:\n",
        "    #    OmegaConf.save(config=storyboard, f=fp.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_WH4yvk5_UDp"
      },
      "outputs": [],
      "source": [
        "# @title # 5. ðŸ§® Math\n",
        "\n",
        "### This cell computes how many frames are needed for each segment\n",
        "### based on the start times for each prompt\n",
        "\n",
        "import datetime as dt\n",
        "fps = storyboard.params.fps\n",
        "\n",
        "ifps = dt.timedelta(seconds=1/fps)\n",
        "\n",
        "# estimate video end\n",
        "video_duration = dt.timedelta(seconds=helper.info['duration'])\n",
        "\n",
        "# dummy prompt for last scene duration\n",
        "prompt_starts.append({'td':video_duration})\n",
        "\n",
        "# make sure we respect the duration of the previous phrase\n",
        "frame_start=dt.timedelta(seconds=0)\n",
        "prompt_starts[0]['anim_start']=frame_start\n",
        "for i, rec in enumerate(prompt_starts[1:], start=1):\n",
        "  rec_prev = prompt_starts[i-1]\n",
        "  k=0\n",
        "  while rec_prev['anim_start'] + k*ifps < rec['td']:\n",
        "    k+=1\n",
        "  k-=1\n",
        "  rec_prev['frames'] = k\n",
        "  rec_prev['anim_duration'] = k*ifps\n",
        "  frame_start+=k*ifps\n",
        "  rec['anim_start']=frame_start\n",
        "\n",
        "# make sure we respect the duration of the previous phrase\n",
        "# to do: push end time into a timedelta and consider it... somewhere near here\n",
        "for i, rec1 in enumerate(prompt_starts):\n",
        "    rec0 = prompt_starts[i-1]\n",
        "    rec0['duration'] = rec1['td'] - rec0['td']\n",
        "\n",
        "# drop the dummy frame\n",
        "prompt_starts = prompt_starts[:-1]\n",
        "\n",
        "# to do: given a 0 duration prompt, assume its duration is captured in the next prompt \n",
        "#        and guesstimate a corrected prompt start time and duration \n",
        "\n",
        "\n",
        "### checkpoint the processing work we've done to this point\n",
        "\n",
        "import copy\n",
        "\n",
        "prompt_starts_copy = copy.deepcopy(prompt_starts)\n",
        "\n",
        "for rec in prompt_starts_copy:\n",
        "    for k,v in list(rec.items()):\n",
        "        if isinstance(v, dt.timedelta):\n",
        "            rec[k] = v.total_seconds()\n",
        "\n",
        "        # flush image objects if they're there, they anger omegaconf\n",
        "        if k in ('frame0','variations','images', 'images_raw'):\n",
        "            rec.pop(k)\n",
        "\n",
        "storyboard.prompt_starts = prompt_starts_copy\n",
        "\n",
        "# to do: deal with these td objects\n",
        "storyboard_fname = 'storyboard.yaml'\n",
        "with open(storyboard_fname) as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Sh514DGj_sua"
      },
      "outputs": [],
      "source": [
        "# @title # 6. ðŸ™­ Generate init images\n",
        "\n",
        "import copy\n",
        "import datetime as dt\n",
        "from omegaconf import OmegaConf\n",
        "from pathlib import Path\n",
        "import random\n",
        "import string\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import PIL\n",
        "\n",
        "from vktrs.tsp import (\n",
        "    tsp_permute_frames,\n",
        "    batched_tsp_permute_frames,\n",
        ")\n",
        "\n",
        "from vktrs.utils import (\n",
        "    add_caption2image,\n",
        "    save_frame,\n",
        "    remove_punctuation,\n",
        ")\n",
        "\n",
        "\n",
        "storyboard_fname = 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "\n",
        "prompt_starts = storyboard.prompt_starts\n",
        "use_stability_api = storyboard.params.use_stability_api\n",
        "\n",
        "\n",
        "if use_stability_api:\n",
        "    from vktrs.api import get_image_for_prompt\n",
        "else:\n",
        "    from vktrs.hf import HfHelper\n",
        "    helper = HfHelper()\n",
        "    get_image_for_prompt = helper.get_image_for_prompt\n",
        "\n",
        "\n",
        "def get_variations_w_init(prompt, init_image, **kargs):\n",
        "    return list(get_image_for_prompt(prompt=prompt, init_image=init_image, **kargs))\n",
        "\n",
        "def get_close_variations_from_prompt(prompt, n_variations=2, image_consistency=.7):\n",
        "    \"\"\"\n",
        "    prompt: a text prompt\n",
        "    n_variations: total number of images to return\n",
        "    image_consistency: float in [0,1], controls similarity between images generated by the prompt.\n",
        "                        you can think of this as controlling how much \"visual vibration\" there will be.\n",
        "                        - 0=regenerate each iandely identical\n",
        "    \"\"\"\n",
        "    images = list(get_image_for_prompt(prompt))\n",
        "    for _ in range(n_variations - 1):\n",
        "        img = get_variations_w_init(prompt, images[0], start_schedule=(1-image_consistency))[0]\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "\n",
        "\n",
        "theme_prompt = storyboard.params.theme_prompt\n",
        "\n",
        "display_frames_as_we_get_them = storyboard.params.display_frames_as_we_get_them\n",
        "\n",
        "height = storyboard.params.height\n",
        "width = storyboard.params.width\n",
        "\n",
        "# to do: move this up to run params\n",
        "proj_name = 'test'\n",
        "\n",
        "print(\"Ensuring each prompt has an associated image\")\n",
        "for idx, rec in enumerate(prompt_starts):\n",
        "    print(\n",
        "        f\"[{rec['anim_start']} | {rec['ts']}] [{rec['duration']} | {rec['anim_duration']}] - {rec['frames']} - {rec['prompt']}\"\n",
        "    )\n",
        "    lyric = rec['prompt']\n",
        "    prompt = f\"{lyric}, {theme_prompt}\"\n",
        "    if rec.get('frame0_fpath') is None:\n",
        "        init_image = list(get_image_for_prompt(\n",
        "              prompt, \n",
        "              height=height,\n",
        "              width=width,\n",
        "              )\n",
        "          )[0]\n",
        "        rec['frame0_fpath'] = save_frame(\n",
        "            init_image,\n",
        "            idx,\n",
        "            root_path=Path('./frames') / proj_name,\n",
        "            name=proj_name, ## to do.... uh... i dunno\n",
        "            )\n",
        "\n",
        "        if display_frames_as_we_get_them:\n",
        "            print(lyric)\n",
        "            display(init_image)\n",
        "\n",
        "########################\n",
        "# update config\n",
        "\n",
        "prompt_starts_copy = copy.deepcopy(prompt_starts)\n",
        "\n",
        "for rec in prompt_starts_copy:\n",
        "    for k,v in list(rec.items()):\n",
        "        if isinstance(v, dt.timedelta):\n",
        "            rec[k] = v.total_seconds()\n",
        "        # flush images for now\n",
        "        if k in ('frame0','variations','images', 'images_raw'):\n",
        "            rec.pop(k)\n",
        "\n",
        "storyboard.prompt_starts = prompt_starts_copy\n",
        "\n",
        "# to do: deal with these td objects\n",
        "storyboard_fname = 'storyboard.yaml'\n",
        "with open(storyboard_fname) as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4AgMgvYusAo4"
      },
      "outputs": [],
      "source": [
        "# @title # 7. ðŸš€ Generate animation frames\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "\n",
        "import copy\n",
        "import datetime as dt\n",
        "from itertools import cycle\n",
        "\n",
        "# reload config\n",
        "storyboard_fname = 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "prompt_starts = OmegaConf.to_container(storyboard.prompt_starts, resolve=True)\n",
        "\n",
        "add_caption = storyboard.params.get('add_caption')\n",
        "optimal_ordering = storyboard.params.optimal_ordering\n",
        "display_frames_as_we_get_them = storyboard.params.display_frames_as_we_get_them\n",
        "image_consistency = storyboard.params.image_consistency\n",
        "max_frames = storyboard.params.max_frames\n",
        "max_variations_per_opt_pass = storyboard.params.max_variations_per_opt_pass\n",
        "n_variations = storyboard.params.n_variations\n",
        "\n",
        "\n",
        "# load init_images and generate variations as needed\n",
        "# to do: use SDK args to request multiple images in single request...\n",
        "frames = []\n",
        "print(\"Fetching variations\")\n",
        "for idx, rec in enumerate(prompt_starts):\n",
        "    images = []\n",
        "\n",
        "    if rec.get('images_fpaths') is None:\n",
        "        init_image = Image.open(rec['frame0_fpath'])\n",
        "        n_variations = rec.get('variations', storyboard.params.n_variations)\n",
        "        n_variations = min(n_variations, rec['frames']) # don't generate variations we won't use\n",
        "        for _ in range(n_variations - 1):\n",
        "            img = get_variations_w_init(prompt, init_image, start_schedule=(1-image_consistency))[0]\n",
        "            images.append(img)\n",
        "\n",
        "        # to do: collect images in a separate object to facilitate storyboard updates\n",
        "        rec['variations'] = images\n",
        "        images = [init_image] + images\n",
        "\n",
        "        rec['variations_fpaths'] = [\n",
        "            save_frame(\n",
        "                img,\n",
        "                idx,\n",
        "                root_path=Path('./frames') / proj_name,\n",
        "                #name=proj_name, ## need to make sure each image gets a unique name\n",
        "            ) for j, img in enumerate(rec['variations'])\n",
        "        ]\n",
        "\n",
        "        # to do: persist the ordering in the storyboard\n",
        "        if optimal_ordering:\n",
        "            images = batched_tsp_permute_frames(\n",
        "                images,\n",
        "                max_variations_per_opt_pass\n",
        "            )\n",
        "        rec['images'] = rec['images_raw'] = images\n",
        "\n",
        "        if add_caption:\n",
        "            rec['images'] = [add_caption2image(im, rec['prompt']) for im in rec['images']]\n",
        "        \n",
        "        rec['images_fpaths'] = [\n",
        "            save_frame(\n",
        "                img,\n",
        "                idx,\n",
        "                root_path=Path('./frames') / proj_name,\n",
        "                #name=proj_name, ## need to make sure each image gets a unique name\n",
        "            ) for j, img in enumerate(rec['images'])\n",
        "        ]\n",
        "    else:\n",
        "        # load frames if we've already generated them\n",
        "        for im_fpath in rec['images_fpaths']:\n",
        "            im = Image.open(im_fpath)\n",
        "            images.append(im)\n",
        "        rec['images'] = images\n",
        "\n",
        "    if display_frames_as_we_get_them:\n",
        "        print(rec['prompt'])\n",
        "        for im in rec['images']:\n",
        "            display(im)\n",
        "\n",
        "    #images *= repeat\n",
        "    sequence = []\n",
        "    frame_factory = cycle(rec['images'])\n",
        "    while len(sequence) < rec['frames']:\n",
        "        sequence.append(next(frame_factory))\n",
        "    frames.extend(sequence)\n",
        "    if len(frames) >= max_frames:\n",
        "        break\n",
        "\n",
        "########################\n",
        "# update config\n",
        "\n",
        "prompt_starts_copy = copy.deepcopy(prompt_starts)\n",
        "\n",
        "for rec in prompt_starts_copy:\n",
        "    for k,v in list(rec.items()):\n",
        "        if isinstance(v, dt.timedelta):\n",
        "            rec[k] = v.total_seconds()\n",
        "        # flush images for now\n",
        "        if k in ('frame0','variations','images', 'images_raw'):\n",
        "            rec.pop(k)\n",
        "\n",
        "storyboard.prompt_starts = prompt_starts_copy\n",
        "\n",
        "# to do: deal with these td objects\n",
        "storyboard_fname = 'storyboard.yaml'\n",
        "with open(storyboard_fname) as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cEwFI6kA_2SH"
      },
      "outputs": [],
      "source": [
        "# @title # 6. ðŸŽ¥ Compile your video!\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "# reload config\n",
        "storyboard_fname = 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "\n",
        "fps = storyboard.params.fps\n",
        "input_audio = storyboard.params.audio_fpath\n",
        "output_filename = storyboard.params.output_filename\n",
        "\n",
        "\n",
        "# to do: read frames and variations back into memory. This should be the last cell that gets run, so we need to \n",
        "# update state wrt any user interventions in the storyboard object. actually, should probably do the text overlay step here\n",
        "\n",
        "\n",
        "cmd_in = ['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-']\n",
        "cmd_out = ['-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '1', '-preset', 'veryslow', '-shortest', output_filename]\n",
        "\n",
        "if input_audio:\n",
        "  cmd_in += ['-i', str(input_audio)]\n",
        "\n",
        "cmd = cmd_in + cmd_out\n",
        "\n",
        "p = Popen(cmd, stdin=PIPE)\n",
        "#for im in tqdm(chain(frames)):\n",
        "for im in tqdm(frames):\n",
        "  im.save(p.stdin, 'PNG')\n",
        "p.stdin.close()\n",
        "\n",
        "print(\"Encoding video...\")\n",
        "p.wait()\n",
        "print(\"Video complete.\")\n",
        "print(f\"Video saved to: {output_filename}\")\n",
        "\n",
        "# to do: optionally compress for download\n",
        "# !tar -czvf output.tar.gz output.mp4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('dmarx-je5LfYh2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "57881a85d677a34ea29564e0084ef84f4058c4e30a2bb466eb0e0b908d0628df"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
