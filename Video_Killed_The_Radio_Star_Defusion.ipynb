{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgXxoDhMAiti"
      },
      "source": [
        "# Video Killed The Radio Star ...Diffusion.\n",
        "\n",
        "Notebook by David Marx ([@DigThatData](https://twitter.com/digthatdata))\n",
        "\n",
        "Shared under MIT license\n",
        "\n",
        "\n",
        "## FAQ\n",
        "\n",
        "**What is this?**\n",
        "\n",
        "Point this notebook at a youtube url and it'll make a music video for you.\n",
        "\n",
        "**How does this animation technique work?**\n",
        "\n",
        "For each text prompt you provide, the notebook will...\n",
        "\n",
        "1. Generate an image based on that text prompt (using stable diffusion)\n",
        "2. Use the generated image as the `init_image` to recombine with the text prompt to generate variations similar to the first image. This produces a sequence of extremely similar images based on the original text prompt\n",
        "3. Images are then intelligently reordered to find the smoothest animation sequence of those frames\n",
        "3. This image sequence is then repeated to pad out the animation duration as needed\n",
        "\n",
        "The technique demonstrated in this notebook was inspired by a [video](https://www.youtube.com/watch?v=WJaxFbdjm8c) created by Ben Gillin.\n",
        "\n",
        "**How are lyrics transcribed?**\n",
        "\n",
        "This notebook uses openai's recently released 'whisper' model for performing automatic speech recognition. \n",
        "OpenAI was kind enough to offer several different sizes of this model which each have their own pros and cons. \n",
        "This notebook uses the largest whisper model for transcribing the actual lyrics. Additionally, we use the \n",
        "smallest model for performing the lyric segmentation. Neither of these models is perfect, but the results \n",
        "so far seem pretty decent.\n",
        "\n",
        "The first draft of this notebook relied on subtitles from youtube videos to determine timing, which was\n",
        "then aligned with user-provided lyrics. Youtube's automated captions are powerful and I'll update the\n",
        "notebook shortly to leverage those again, but for the time being we're just using whisper for everything\n",
        "and not referencing user-provided captions at all.\n",
        "\n",
        "**Something didn't work quite right in the transcription process. How do fix the timing or the actual lyrics?**\n",
        "\n",
        "The notebook is divided into several steps. Between each step, a \"storyboard\" file is updated. If you want to\n",
        "make modifications, you can edit this file directly and those edits should be reflected when you next load the\n",
        "file. Depending on what you changed and what step you run next, your changes may be ignored or even overwritten.\n",
        "Still playing with different solutions here.\n",
        "\n",
        "**Can I provide my own images to 'bring to life' and associate with certain lyrics/sequences?**\n",
        "\n",
        "Yes, you can! As described above: you just need to modify the storyboard. Will describe this functionality in\n",
        "greater detail after the implementation stabilizes a bit more.\n",
        "\n",
        "**This gave me an idea and I'd like to use just a part of your process here. What's the best way to reuse just some of the machinery you've developed here?**\n",
        "\n",
        "Most of the functionality in this notebook has been offloaded to library I published to pypi called `vktrs`. I strongly encourage you to import anything you need \n",
        "from there rather than cutting and pasting function into a notebook. Similarly, if you have ideas for improvements, please don't hesitate to submit a PR!\n",
        "\n",
        "**How can I support your work or work like it?**\n",
        "\n",
        "This notebook was made possible thanks to ongoing support from [stability.ai](https://stability.ai/). The best way to support my work is to share it with your friends, [report bugs](https://github.com/dmarx/video-killed-the-radio-star/issues/new), [suggest features](https://github.com/dmarx/video-killed-the-radio-star/discussions) or to donate to open source non-profits :) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oPbeyWtesAoh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title # 0. üõ†Ô∏è Setup\n",
        "!pip install vktrs[api,hf]\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper\n",
        "\n",
        "# these are only needed for hf\n",
        "!pip install \"ipywidgets>=7,<8\"\n",
        "!sudo apt -qq install git-lfs\n",
        "!git config --global credential.helper store\n",
        "\n",
        "!pip install panel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZnTe8clZuZuj"
      },
      "outputs": [],
      "source": [
        "# @markdown # üìä Check GPU Status\n",
        "\n",
        "from vktrs.utils import gpu_info\n",
        "\n",
        "gpu_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cM8cux9b7F4v"
      },
      "outputs": [],
      "source": [
        "# @title # 1. üîë Provide your API Key\n",
        "# @markdown Running this cell will prompt you to enter your API Key below. \n",
        "\n",
        "# @markdown To get your API key, visit https://beta.dreamstudio.ai/membership\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown A note on security best practices: **don't publish your API key.**\n",
        "\n",
        "# @markdown We're using a form field designed for sensitive data like passwords.\n",
        "# @markdown This notebook does not save your API key in the notebook itself,\n",
        "# @markdown but instead loads your API Key into the colab environment. This way,\n",
        "# @markdown you can make changes to this notebook and share it without concern\n",
        "# @markdown that you might accidentally share your API Key. \n",
        "# @markdown \n",
        "\n",
        "use_stability_api = False # @param {type:'boolean'}\n",
        "mount_gdrive = False # @param {type:'boolean'}\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ['XDG_CACHE_HOME'] = os.environ.get(\n",
        "    'XDG_CACHE_HOME',\n",
        "    str(Path('~/.cache').expanduser())\n",
        ")\n",
        "if mount_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    Path('/content/drive/MyDrive/AI/models/.cache/').mkdir(parents=True, exist_ok=True) \n",
        "    # This rm+ln solution is not great. Be careful not to run this locally. \n",
        "    # Low risk, but could be annoying    \n",
        "    !rm -rf /root/.cache\n",
        "    !ln -sf /content/drive/MyDrive/AI/models/.cache/ /root/\n",
        "    # Following line will be sufficient pending merge of https://github.com/openai/whisper/pull/257\n",
        "    os.environ['XDG_CACHE_HOME']='/content/drive/MyDrive/AI/models/.cache'\n",
        "\n",
        "if use_stability_api:\n",
        "    import os, getpass\n",
        "    os.environ['STABILITY_KEY'] = getpass.getpass('Enter your API Key')\n",
        "else:\n",
        "    try:\n",
        "        from google.colab import output\n",
        "        output.enable_custom_widget_manager()\n",
        "    except ImportError:\n",
        "        # assume local use\n",
        "        pass\n",
        "    \n",
        "    from huggingface_hub import notebook_login\n",
        "\n",
        "    # to do: if gdrive mounted, check for API token... somewhere on drive?\n",
        "    # looks like we should be able to find the token through an environment variable\n",
        "    notebook_login()\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from pathlib import Path\n",
        "\n",
        "model_dir_str=str(Path(os.environ['XDG_CACHE_HOME']))\n",
        "proj_root_str = '${active_project}'\n",
        "if mount_gdrive:\n",
        "    proj_root_str = '/content/drive/MyDrive/AI/VideoKilledTheRadioStar/${active_project}'\n",
        "    #model_dir_str = '/content/drive/MyDrive/AI/models' # hf prob needs addl subfolder\n",
        "\n",
        "# notebook config\n",
        "cfg = OmegaConf.create({\n",
        "    'active_project':'default_workspace',\n",
        "    'project_root':proj_root_str,\n",
        "    'gdrive_mounted':mount_gdrive,\n",
        "    'use_stability_api':use_stability_api,\n",
        "    'model_dir':model_dir_str,\n",
        "    'output_dir':'${active_project}/frames'\n",
        "})\n",
        "\n",
        "#Path(cfg.project_root).mkdir(parents=True, exist_ok=True)\n",
        "#with open(Path(cfg.project_root) / 'config.yaml','w') as fp:\n",
        "# This is just the WORKSPACE config. Better to just leave it in cwd\n",
        "with open('config.yaml','w') as fp:\n",
        "    OmegaConf.save(config=cfg, f=fp.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9zT0u4-q_fMF"
      },
      "outputs": [],
      "source": [
        "# @title # 2. üìã Audio processing parameters\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from pathlib import Path\n",
        "\n",
        "workspace = OmegaConf.load('config.yaml')\n",
        "OmegaConf.resolve(workspace)\n",
        "use_stability_api = workspace.use_stability_api\n",
        "model_dir = workspace.model_dir\n",
        "\n",
        "root = workspace.project_root\n",
        "root = Path(root)\n",
        "root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# this needs to not be in the same cell as the login.\n",
        "# some sort of stupid race condition.\n",
        "if not use_stability_api:\n",
        "    # init hf here to download models\n",
        "    from vktrs.hf import HfHelper\n",
        "    try:\n",
        "        hf_helper = HfHelper(\n",
        "            download=False,\n",
        "            model_path=str(Path(model_dir) / 'huggingface' / 'diffusers')\n",
        "        )\n",
        "    except:\n",
        "        hf_helper = HfHelper(\n",
        "            download=True,\n",
        "            model_path=str(Path(model_dir) / 'huggingface' / 'diffusers')\n",
        "        )\n",
        "\n",
        "import datetime as dt\n",
        "from itertools import chain, cycle\n",
        "import json\n",
        "import os\n",
        "\n",
        "import re\n",
        "import string\n",
        "from subprocess import Popen, PIPE\n",
        "import textwrap\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import tokenizations\n",
        "import webvtt\n",
        "\n",
        "\n",
        "# to do: use project name to name file\n",
        "# to do: separate global params defined here from the storyboard object.\n",
        "#        users will not anticipate that updates here will destroy their work\n",
        "storyboard = OmegaConf.create()\n",
        "\n",
        "storyboard.params = dict(\n",
        "\n",
        "    # all this does is make it so each of the following lines can be preceded with a comma\n",
        "    # otw the first parameter would be offset from the other in the colab form\n",
        "    _=\"\"\n",
        "\n",
        "    , video_url = 'https://www.youtube.com/watch?v=REojIUxX4rw' # @param {type:'string'}\n",
        "    , audio_fpath = '' # @param {type:'string'}\n",
        "    , whisper_seg = True # @param {type:'boolean'}\n",
        "\n",
        "    #, use_stability_api = use_stability_api\n",
        ")\n",
        "\n",
        "\n",
        "if not storyboard.params.audio_fpath:\n",
        "    storyboard.params.audio_fpath = None\n",
        "\n",
        "\n",
        "# @markdown `video_url` - URL of a youtube video to download as a source for audio and potentially for text transcription as well.\n",
        "\n",
        "# @markdown `whisper_seg` - Whether or not to use openai's whisper model for lyric segmentation. This is currently the only option, but that will change in a few days.\n",
        "\n",
        "\n",
        "##################\n",
        "# markdown `audio_fpath` - Optionally provide an audio file instead of relying on a youtube download. Name it something other than 'audio.mp3', \n",
        "# markdown                 otherwise it might get overwritten accidentally.\n",
        "##################\n",
        "\n",
        "\n",
        "storyboard_fname = root / 'storyboard.yaml'\n",
        "with open(storyboard_fname,'wb') as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8OaQYVfYgBH-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# @title # 3. üì• Download audio from youtube\n",
        "\n",
        "from vktrs.utils import get_audio_duration_seconds\n",
        "from vktrs.youtube import (\n",
        "    YoutubeHelper,\n",
        "    parse_timestamp,\n",
        "    vtt_to_token_timestamps,\n",
        "    srv2_to_token_timestamps,\n",
        ")\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from pathlib import Path\n",
        "\n",
        "workspace = OmegaConf.load('config.yaml')\n",
        "OmegaConf.resolve(workspace)\n",
        "root = Path(workspace.project_root)\n",
        "\n",
        "storyboard_fname = root / 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "\n",
        "video_url = storyboard.params.video_url\n",
        "\n",
        "if video_url:\n",
        "    # check if user provided an audio filepath (or we already have one from youtube) before attempting to download\n",
        "    if storyboard.params.get('audio_fpath') is None:\n",
        "        helper = YoutubeHelper(\n",
        "            video_url,\n",
        "            ydl_opts = {\n",
        "                'outtmpl':{'default':str( root / f\"ytdlp_content.%(ext)s\" )},\n",
        "                'writeautomaticsub':True,\n",
        "                'subtitlesformat':'srv2/vtt'\n",
        "                },\n",
        "        )\n",
        "\n",
        "        # estimate video end\n",
        "        video_duration = dt.timedelta(seconds=helper.info['duration'])\n",
        "        storyboard.params['video_duration'] = video_duration.total_seconds()\n",
        "\n",
        "        audio_fpath = str( root / 'audio.mp3' )\n",
        "        input_audio = helper.info['requested_downloads'][-1]['filepath']\n",
        "        !ffmpeg -y -i \"{input_audio}\" -acodec libmp3lame {audio_fpath}\n",
        "\n",
        "        # to do: write audio and subtitle paths/meta to storyboard\n",
        "        storyboard.params.audio_fpath = audio_fpath\n",
        "\n",
        "        if False:\n",
        "            subtitle_format = helper.info['requested_subtitles']['en']['ext']\n",
        "            subtitle_fpath = helper.info['requested_subtitles']['en']['filepath']\n",
        "\n",
        "            if subtitle_format == 'srv2':\n",
        "                with open(subtitle_fpath, 'r') as f:\n",
        "                    srv2_xml = f.read() \n",
        "                token_start_times = srv2_to_token_timestamps(srv2_xml)\n",
        "                # to do: handle timedeltas...\n",
        "                #storyboard.params.token_start_times = token_start_times\n",
        "\n",
        "            elif subtitle_format == 'vtt':\n",
        "                captions = webvtt.read(subtitle_fpath)\n",
        "                token_start_times = vtt_to_token_timestamps(captions)\n",
        "                # to do: handle timedeltas...\n",
        "                #storyboard.params.token_start_times = token_start_times\n",
        "\n",
        "            # If unable to download supported subtitles, force use whisper\n",
        "            else:\n",
        "                storyboard.params.whisper_seg = True\n",
        "\n",
        "\n",
        "# estimate video end\n",
        "if storyboard.params.get('video_duration') is None:\n",
        "    # estimate duration from audio file\n",
        "    audio_fpath = storyboard.params['audio_fpath']\n",
        "    storyboard.params['video_duration'] = get_audio_duration_seconds(audio_fpath)\n",
        "\n",
        "if storyboard.params.get('video_duration') is None:\n",
        "    raise RuntimeError('unable to determine audio duration. was a video url or path to a file supplied?')\n",
        "\n",
        "# force use\n",
        "storyboard.params.whisper_seg = True\n",
        "\n",
        "with open(storyboard_fname,'wb') as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)\n",
        "\n",
        "whisper_seg = storyboard.params.whisper_seg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "73lfb0gZvGW5"
      },
      "outputs": [],
      "source": [
        "# @title # 4. üí¨ Transcribe and segment speech using whisper\n",
        "\n",
        "import gc\n",
        "from omegaconf import OmegaConf\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import tokenizations\n",
        "from vktrs.utils import remove_punctuation\n",
        "import whisper\n",
        "\n",
        "workspace = OmegaConf.load('config.yaml')\n",
        "OmegaConf.resolve(workspace)\n",
        "root = Path(workspace.project_root)\n",
        "\n",
        "storyboard_fname = root / 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "\n",
        "whisper_seg = storyboard.params.whisper_seg\n",
        "\n",
        "if whisper_seg:\n",
        "    from vktrs.asr import (\n",
        "        #whisper_lyrics,\n",
        "        #whisper_transcribe,\n",
        "        #whisper_align,\n",
        "        whisper_transmit_meta_across_alignment,\n",
        "        whisper_segment_transcription,\n",
        "    )\n",
        "\n",
        "    #prompt_starts = whisper_lyrics(audio_fpath=storyboard.params.audio_fpath)\n",
        "\n",
        "    audio_fpath = storyboard.params.audio_fpath\n",
        "    #whispers = whisper_transcribe(audio_fpath)\n",
        "\n",
        "    segmentation_model = 'tiny'\n",
        "    transcription_model = 'large'\n",
        "\n",
        "    storyboard.params.whisper = dict(\n",
        "        segmentation_model = segmentation_model\n",
        "        ,transcription_model = transcription_model\n",
        "    )\n",
        "\n",
        "    whispers = {\n",
        "        #'tiny':None, # 5.83 s\n",
        "        #'large':None # 3.73 s\n",
        "    }\n",
        "    # accelerated runtime required for whisper\n",
        "    # to do: pypi package for whisper\n",
        "\n",
        "    # to do: use transcripts we've already built if we have them\n",
        "    #scripts = storyboard.params.whisper.get('transcriptions')\n",
        "    \n",
        "    for k in set([segmentation_model, transcription_model]):\n",
        "        #if k in scripts:\n",
        "\n",
        "        options = whisper.DecodingOptions(\n",
        "            language='en',\n",
        "        )\n",
        "        # to do: be more proactive about cleaning up these models when we're done with them\n",
        "        model = whisper.load_model(k).to('cuda')\n",
        "        start = time.time()\n",
        "        print(f\"Transcribing audio with whisper-{k}\")\n",
        "        \n",
        "        # to do: calling transcribe like this unnecessarily re-processes audio each time.\n",
        "        whispers[k] = model.transcribe(audio_fpath) # re-processes audio each time, ~10s overhead?\n",
        "        print(f\"elapsed: {time.time()-start}\")\n",
        "        del model\n",
        "        gc.collect()\n",
        "    \n",
        "    #######################\n",
        "    # save transcriptions #\n",
        "    #######################\n",
        "\n",
        "    transcriptions = {}\n",
        "    transcription_root = root / 'whispers'\n",
        "    transcription_root.mkdir(parents=True, exist_ok=True)\n",
        "    for k in whispers:\n",
        "        outpath = str( transcription_root / f\"{k}.vtt\" )\n",
        "        transcriptions[k] = outpath\n",
        "        with open(outpath,'w') as f:\n",
        "            # to do: upstream PR to control verbosity\n",
        "            whisper.utils.write_vtt(\n",
        "                whispers[k][\"segments\"], # ...really?\n",
        "                file=f\n",
        "            )\n",
        "    storyboard.params.whisper.transcriptions = transcriptions\n",
        "\n",
        "    #tiny2large, large2tiny, whispers_tokens = whisper_align(whispers)\n",
        "    # sanitize and tokenize\n",
        "    whispers_tokens = {}\n",
        "    for k in whispers:\n",
        "        whispers_tokens[k] = [\n",
        "        remove_punctuation(tok) for tok in whispers[k]['text'].split()\n",
        "        ]\n",
        "\n",
        "    # align sequences\n",
        "    tiny2large, large2tiny = tokenizations.get_alignments(\n",
        "        whispers_tokens[segmentation_model], #whispers_tokens['tiny'],\n",
        "        whispers_tokens[transcription_model] #whispers_tokens['large']\n",
        "    )\n",
        "    #return tiny2large, large2tiny, whispers_tokens\n",
        "\n",
        "    token_large_index_segmentations = whisper_transmit_meta_across_alignment(\n",
        "        whispers,\n",
        "        large2tiny,\n",
        "        whispers_tokens,\n",
        "    )\n",
        "    prompt_starts = whisper_segment_transcription(\n",
        "        token_large_index_segmentations,\n",
        "    )\n",
        "\n",
        "\n",
        "    #return prompt_starts\n",
        "    #storyboard.prompt_starts = prompt_starts\n",
        "    # to do: deal with these td objects\n",
        "    with open(storyboard_fname) as fp:\n",
        "        OmegaConf.save(config=storyboard, f=fp.name)\n",
        "\n",
        "######################################################\n",
        "\n",
        "# title # 4.b (optional) Review/Modify transcription\n",
        "\n",
        "# markdown Run this cell for an opportunity to review and modify the\n",
        "# markdown transcription.\n",
        "\n",
        "import pandas as pd\n",
        "import panel as pn\n",
        "\n",
        "# https://panel.holoviz.org/reference/widgets/Tabulator.html\n",
        "pn.extension('tabulator') # I don't know that specifying 'tabulator' here is even necessary...\n",
        "\n",
        "tabulator_formatters = {\n",
        "    #'float': {'type': 'progress', 'max': 10},\n",
        "    'bool': {'type': 'tickCross'}\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(prompt_starts).rename(\n",
        "    columns={\n",
        "        'ts':'Timestamp (sec)',\n",
        "        'prompt':'Lyric',\n",
        "    }\n",
        ")\n",
        "\n",
        "if 'td' in df:\n",
        "  del df['td']\n",
        "\n",
        "import copy\n",
        "df_pre = copy.deepcopy(df)\n",
        "pn.widgets.Tabulator(df, formatters=tabulator_formatters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_WH4yvk5_UDp"
      },
      "outputs": [],
      "source": [
        "# @title # 5. üßÆ Math\n",
        "\n",
        "# update prompt_starts if any changes were made above\n",
        "import numpy as np\n",
        "if not np.all(df_pre.values == df.values):\n",
        "    df_pre = copy.deepcopy(df)\n",
        "    for i, rec in enumerate(prompt_starts):\n",
        "        rec['ts'] = df['Timestamp (sec)']\n",
        "        rec['td'] = dt.timedelta(rec['ts'])\n",
        "        rec['prompt'] = df['Lyric']\n",
        "\n",
        "############################################\n",
        "\n",
        "workspace = OmegaConf.load('config.yaml')\n",
        "OmegaConf.resolve(workspace)\n",
        "root = Path(workspace.project_root)\n",
        "\n",
        "storyboard_fname = root / 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "\n",
        "### This cell computes how many frames are needed for each segment\n",
        "### based on the start times for each prompt\n",
        "\n",
        "import datetime as dt\n",
        "#fps = storyboard.params.fps\n",
        "\n",
        "\n",
        "# @markdown `fps` - Frames-per-second of generated animations\n",
        "\n",
        "fps = 12 # @param {type:'integer'}\n",
        "storyboard.params.fps = fps\n",
        "\n",
        "ifps = dt.timedelta(seconds=1/fps)\n",
        "\n",
        "# estimate video end\n",
        "video_duration = storyboard.params['video_duration']\n",
        "\n",
        "# dummy prompt for last scene duration\n",
        "prompt_starts.append({'td':dt.timedelta(seconds=video_duration)})\n",
        "\n",
        "# make sure we respect the duration of the previous phrase\n",
        "frame_start=dt.timedelta(seconds=0)\n",
        "prompt_starts[0]['anim_start']=frame_start\n",
        "for i, rec in enumerate(prompt_starts[1:], start=1):\n",
        "  rec_prev = prompt_starts[i-1]\n",
        "  k=0\n",
        "  while (rec_prev['anim_start'] + k*ifps) < rec['td']:\n",
        "    k+=1\n",
        "  k-=1\n",
        "  rec_prev['frames'] = k\n",
        "  rec_prev['anim_duration'] = k*ifps\n",
        "  frame_start+=k*ifps\n",
        "  rec['anim_start']=frame_start\n",
        "\n",
        "# make sure we respect the duration of the previous phrase\n",
        "# to do: push end time into a timedelta and consider it... somewhere near here\n",
        "for i, rec1 in enumerate(prompt_starts):\n",
        "    rec0 = prompt_starts[i-1]\n",
        "    rec0['duration'] = rec1['td'] - rec0['td']\n",
        "\n",
        "# drop the dummy frame\n",
        "prompt_starts = prompt_starts[:-1]\n",
        "\n",
        "# to do: given a 0 duration prompt, assume its duration is captured in the next prompt \n",
        "#        and guesstimate a corrected prompt start time and duration \n",
        "\n",
        "\n",
        "### checkpoint the processing work we've done to this point\n",
        "\n",
        "import copy\n",
        "\n",
        "prompt_starts_copy = copy.deepcopy(prompt_starts)\n",
        "\n",
        "for rec in prompt_starts_copy:\n",
        "    for k,v in list(rec.items()):\n",
        "        if isinstance(v, dt.timedelta):\n",
        "            rec[k] = v.total_seconds()\n",
        "\n",
        "        # flush image objects if they're there, they anger omegaconf\n",
        "        if k in ('frame0','variations','images', 'images_raw'):\n",
        "            rec.pop(k)\n",
        "\n",
        "storyboard.prompt_starts = prompt_starts_copy\n",
        "\n",
        "# to do: deal with these td objects\n",
        "#storyboard_fname = 'storyboard.yaml'\n",
        "with open(storyboard_fname) as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Sh514DGj_sua"
      },
      "outputs": [],
      "source": [
        "# @title # 6. üé® Generate init images\n",
        "\n",
        "import copy\n",
        "import datetime as dt\n",
        "from omegaconf import OmegaConf\n",
        "from pathlib import Path\n",
        "import random\n",
        "import string\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import PIL\n",
        "\n",
        "from vktrs.tsp import (\n",
        "    tsp_permute_frames,\n",
        "    batched_tsp_permute_frames,\n",
        ")\n",
        "\n",
        "from vktrs.utils import (\n",
        "    add_caption2image,\n",
        "    save_frame,\n",
        "    remove_punctuation,\n",
        ")\n",
        "\n",
        "\n",
        "workspace = OmegaConf.load('config.yaml')\n",
        "OmegaConf.resolve(workspace)\n",
        "root = Path(workspace.project_root)\n",
        "\n",
        "storyboard_fname = root / 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "\n",
        "prompt_starts = storyboard.prompt_starts\n",
        "use_stability_api = workspace.use_stability_api\n",
        "model_dir = workspace.model_dir\n",
        "\n",
        "if use_stability_api:\n",
        "    from vktrs.api import get_image_for_prompt\n",
        "else:\n",
        "    from vktrs.hf import HfHelper\n",
        "    helper = HfHelper(\n",
        "        download=False,\n",
        "        model_path=str(Path(model_dir) / 'huggingface' / 'diffusers')\n",
        "        )\n",
        "\n",
        "    # I give up.\n",
        "    def get_image_for_prompt(*args, **kargs):\n",
        "        result = helper.get_image_for_prompt(*args, **kargs)\n",
        "        return result.images\n",
        "    \"\"\"\n",
        "    def get_image_for_prompt(*args, **kargs):\n",
        "        nsfw_regens = storyboard.params.nsfw_regens\n",
        "        if nsfw_regens < 0:\n",
        "            return helper.get_image_for_prompt(*args, **kargs).images\n",
        "        while nsfw_regens > 0:\n",
        "            result = helper.get_image_for_prompt(*args, **kargs)\n",
        "            #if not any(result.nsfw_content_detected):\n",
        "            if hasattr(result, 'nsfw_content_detected') is None:\n",
        "                return result.images\n",
        "            print(\"NSFW filter triggered. Attempting to regenerate images...\")\n",
        "            nsfw_regens -= 1\n",
        "        raise RuntimeError(\n",
        "            \"Regenerations maxed out. Halting progress.\"\n",
        "            \"Please modify your prompt or disable the nsfw classifier.\"\n",
        "            \"The classifier can be disabled by setting the regens \"\n",
        "            \"parameter to a negative value.\"\n",
        "        )\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "def get_variations_w_init(prompt, init_image, **kargs):\n",
        "    return list(get_image_for_prompt(prompt=prompt, init_image=init_image, **kargs))\n",
        "\n",
        "def get_close_variations_from_prompt(prompt, n_variations=2, image_consistency=.7):\n",
        "    \"\"\"\n",
        "    prompt: a text prompt\n",
        "    n_variations: total number of images to return\n",
        "    image_consistency: float in [0,1], controls similarity between images generated by the prompt.\n",
        "                        you can think of this as controlling how much \"visual vibration\" there will be.\n",
        "                        - 0=regenerate each iandely identical\n",
        "    \"\"\"\n",
        "    images = list(get_image_for_prompt(prompt))\n",
        "    for _ in range(n_variations - 1):\n",
        "        img = get_variations_w_init(prompt, images[0], start_schedule=(1-image_consistency))[0]\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "\n",
        "d_ = dict(\n",
        "    _=''\n",
        "    , theme_prompt = \"extremely detailed, painted by ralph steadman and radiohead, beautiful, wow\" # @param {type:'string'}\n",
        "\n",
        "    , height = 512 # @param {type:'integer'}\n",
        "    , width = 512 # @param {type:'integer'}\n",
        "    , display_frames_as_we_get_them = True # @param {type:'boolean'}\n",
        "\n",
        "    #, nsfw_regens = 3 # @param {type:'integer'}\n",
        ")\n",
        "\n",
        "\n",
        "# @markdown `theme_prompt` - Text that will be appended to the end of each lyric, useful for e.g. applying a consistent aesthetic style\n",
        "\n",
        "# @markdown `display_frames_as_we_get_them` - Displaying frames will make the notebook slightly slower\n",
        "\n",
        "\n",
        "regenerate_init_images = False\n",
        "if d_['theme_prompt'] != storyboard.params.get('theme_prompt'):\n",
        "    regenerate_init_images = True\n",
        "\n",
        "storyboard.params.update(d_)\n",
        "\n",
        "if regenerate_init_images:\n",
        "    for rec in prompt_starts:\n",
        "        rec['frame0_fpath'] = None\n",
        "        rec['variations_fpaths'] = None\n",
        "        rec['images_fpaths'] = None\n",
        "\n",
        "theme_prompt = storyboard.params.theme_prompt\n",
        "display_frames_as_we_get_them = storyboard.params.display_frames_as_we_get_them\n",
        "height = storyboard.params.height\n",
        "width = storyboard.params.width\n",
        "\n",
        "\n",
        "# to do: move this up to run params\n",
        "#proj_name = 'test'\n",
        "proj_name = workspace.active_project\n",
        "\n",
        "print(\"Ensuring each prompt has an associated image\")\n",
        "for idx, rec in enumerate(prompt_starts):\n",
        "    print(\n",
        "        f\"[{rec['anim_start']} | {rec['ts']}] [{rec['duration']} | {rec['anim_duration']}] - {rec['frames']} - {rec['prompt']}\"\n",
        "    )\n",
        "    lyric = rec['prompt']\n",
        "    prompt = f\"{lyric}, {theme_prompt}\"\n",
        "    if rec.get('frame0_fpath') is None:\n",
        "        init_image = list(get_image_for_prompt(\n",
        "              prompt, \n",
        "              height=height,\n",
        "              width=width,\n",
        "              )\n",
        "          )[0]\n",
        "        rec['frame0_fpath'] = save_frame(\n",
        "            init_image,\n",
        "            idx,\n",
        "            #root_path=Path('./frames') / proj_name,\n",
        "            #name=proj_name, ## to do.... uh... i dunno\n",
        "            root_path = root / 'frames', # to do: this field should accept a string as well\n",
        "            name='anchor',\n",
        "            )\n",
        "\n",
        "        if display_frames_as_we_get_them:\n",
        "            print(lyric)\n",
        "            display(init_image)\n",
        "\n",
        "########################\n",
        "# update config\n",
        "\n",
        "prompt_starts_copy = copy.deepcopy(prompt_starts)\n",
        "\n",
        "for rec in prompt_starts_copy:\n",
        "    for k,v in list(rec.items()):\n",
        "        if isinstance(v, dt.timedelta):\n",
        "            rec[k] = v.total_seconds()\n",
        "        # flush images for now\n",
        "        if k in ('frame0','variations','images', 'images_raw'):\n",
        "            rec.pop(k)\n",
        "\n",
        "storyboard.prompt_starts = prompt_starts_copy\n",
        "\n",
        "# to do: deal with these td objects\n",
        "#storyboard_fname = 'storyboard.yaml'\n",
        "with open(storyboard_fname) as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4AgMgvYusAo4"
      },
      "outputs": [],
      "source": [
        "# @title # 7. üöÄ Generate animation frames\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "\n",
        "import copy\n",
        "import datetime as dt\n",
        "from itertools import cycle\n",
        "\n",
        "# reload config\n",
        "workspace = OmegaConf.load('config.yaml')\n",
        "OmegaConf.resolve(workspace)\n",
        "root = Path(workspace.project_root)\n",
        "\n",
        "storyboard_fname = root / 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "\n",
        "prompt_starts = OmegaConf.to_container(storyboard.prompt_starts, resolve=True)\n",
        "\n",
        "\n",
        "# `nsfw_regens` - Max number of times to attempt regenerating an image after triggering the NSFW classifier (huggingface only, see [Open RAIL-M restrictions](https://huggingface.co/spaces/CompVis/stable-diffusion-license))\n",
        "\n",
        "# @markdown `n_variations` - How many unique variations to generate for a given text prompt. This determines the frequency of the visual \"pulsing\" effect\n",
        "\n",
        "# @markdown `image_consistency` - controls similarity between images generated by the prompt.\n",
        "# @markdown - 0: ignore the init image\n",
        "# @markdown - 1: true as possible to the init image\n",
        "\n",
        "# @markdown `add_caption` - Whether or not to overlay the prompt text on the image\n",
        "\n",
        "# @markdown `optimal_ordering` - Intelligently permutes animation frames to provide a smoother animation.\n",
        "\n",
        "# @markdown `max_video_duration_in_seconds` - Early stopping if you don't want to generate a video the full duration of the provided audio. Default = 5min.\n",
        "\n",
        "\n",
        "d_ = dict(\n",
        "    _=''\n",
        "\n",
        "    , n_variations=5 # @param {type:'integer'}\n",
        "    , image_consistency=0.8 # @param {type:\"slider\", min:0, max:1, step:0.01}  \n",
        "    , add_caption = False # @param {type:'boolean'}\n",
        "    , optimal_ordering = True # @param {type:'boolean'}\n",
        "    , max_video_duration_in_seconds = 300 # @param {type:'integer'}\n",
        "\n",
        "    # this parameter is currently not exposed in the form\n",
        "    , max_variations_per_opt_pass = 15\n",
        ")\n",
        "\n",
        "storyboard.params.update(d_)\n",
        "storyboard.params.max_frames = storyboard.params.fps * storyboard.params.max_video_duration_in_seconds\n",
        "\n",
        "print(f\"Max total frames: {storyboard.params.max_frames}\")\n",
        "#print(f\"Max API requests: {int(max_frames/repeat)}\")\n",
        "\n",
        "if storyboard.params.optimal_ordering:\n",
        "\n",
        "    opt_batch_size = storyboard.params.n_variations\n",
        "    while opt_batch_size > storyboard.params.max_variations_per_opt_pass:\n",
        "        opt_batch_size /= 2\n",
        "    print(f\"Frames per re-ordering batch: {opt_batch_size}\")\n",
        "    storyboard.params.opt_batch_size = opt_batch_size\n",
        "\n",
        "\n",
        "add_caption = storyboard.params.get('add_caption')\n",
        "optimal_ordering = storyboard.params.optimal_ordering\n",
        "display_frames_as_we_get_them = storyboard.params.display_frames_as_we_get_them\n",
        "image_consistency = storyboard.params.image_consistency\n",
        "max_frames = storyboard.params.max_frames\n",
        "max_variations_per_opt_pass = storyboard.params.max_variations_per_opt_pass\n",
        "n_variations = storyboard.params.n_variations\n",
        "theme_prompt = storyboard.params.get('theme_prompt')\n",
        "\n",
        "\n",
        "# load init_images and generate variations as needed\n",
        "# to do: use SDK args to request multiple images in single request...\n",
        "frames = []\n",
        "print(\"Fetching variations\")\n",
        "for idx, rec in enumerate(prompt_starts):\n",
        "    images = []\n",
        "    images_fpaths = rec.get('images_fpaths')\n",
        "    curr_variation_count = 0 if images_fpaths is None else len(images_fpaths)\n",
        "    if curr_variation_count < n_variations:\n",
        "        lyric = rec['prompt']\n",
        "        prompt = f\"{lyric}, {theme_prompt}\"\n",
        "\n",
        "        init_image = Image.open(rec['frame0_fpath'])\n",
        "        n_variations = rec.get('n_variations', storyboard.params.n_variations)\n",
        "        n_variations = min(n_variations, rec['frames']) # don't generate variations we won't use\n",
        "        n_variations -= curr_variation_count  # only generate variations we need\n",
        "        for _ in range(n_variations - 1):\n",
        "            img = get_variations_w_init(prompt, init_image, start_schedule=(1-image_consistency))[0]\n",
        "            images.append(img)\n",
        "\n",
        "        # to do: collect images in a separate object to facilitate storyboard updates\n",
        "        rec['variations'] = images\n",
        "        images = [init_image] + images\n",
        "\n",
        "        rec['variations_fpaths'] = [\n",
        "            save_frame(\n",
        "                img,\n",
        "                idx,\n",
        "                root_path= root / 'frames', #Path('./frames') / proj_name,\n",
        "                #name=proj_name, ## need to make sure each image gets a unique name\n",
        "            ) for j, img in enumerate(rec['variations'])\n",
        "        ]\n",
        "\n",
        "        # to do: persist the ordering in the storyboard\n",
        "        if optimal_ordering:\n",
        "            images = batched_tsp_permute_frames(\n",
        "                images,\n",
        "                max_variations_per_opt_pass\n",
        "            )\n",
        "        rec['images'] = rec['images_raw'] = images\n",
        "\n",
        "        if add_caption:\n",
        "            rec['images'] = [add_caption2image(im, rec['prompt']) for im in rec['images']]\n",
        "        \n",
        "        rec['images_fpaths'] = [\n",
        "            save_frame(\n",
        "                img,\n",
        "                idx,\n",
        "                root_path=Path('./frames') / proj_name,\n",
        "                #name=proj_name, ## need to make sure each image gets a unique name\n",
        "            ) for j, img in enumerate(rec['images'])\n",
        "        ]\n",
        "    else:\n",
        "        # load frames if we've already generated them\n",
        "        for im_fpath in rec['images_fpaths']:\n",
        "            im = Image.open(im_fpath)\n",
        "            images.append(im)\n",
        "        rec['images'] = images\n",
        "\n",
        "    if display_frames_as_we_get_them:\n",
        "        print(rec['prompt'])\n",
        "        for im in rec['images']:\n",
        "            display(im)\n",
        "\n",
        "    #images *= repeat\n",
        "    sequence = []\n",
        "    frame_factory = cycle(rec['images'])\n",
        "    while len(sequence) < rec['frames']:\n",
        "        sequence.append(next(frame_factory))\n",
        "    frames.extend(sequence)\n",
        "    if len(frames) >= max_frames:\n",
        "        break\n",
        "\n",
        "########################\n",
        "# update config\n",
        "\n",
        "prompt_starts_copy = copy.deepcopy(prompt_starts)\n",
        "\n",
        "for rec in prompt_starts_copy:\n",
        "    for k,v in list(rec.items()):\n",
        "        if isinstance(v, dt.timedelta):\n",
        "            rec[k] = v.total_seconds()\n",
        "        # flush images for now\n",
        "        if k in ('frame0','variations','images', 'images_raw'):\n",
        "            rec.pop(k)\n",
        "\n",
        "storyboard.prompt_starts = prompt_starts_copy\n",
        "\n",
        "# to do: deal with these td objects\n",
        "#storyboard_fname = 'storyboard.yaml'\n",
        "with open(storyboard_fname) as fp:\n",
        "    OmegaConf.save(config=storyboard, f=fp.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cEwFI6kA_2SH"
      },
      "outputs": [],
      "source": [
        "# @title # 8. üé• Compile your video!\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "# reload config\n",
        "workspace = OmegaConf.load('config.yaml')\n",
        "OmegaConf.resolve(workspace)\n",
        "root = Path(workspace.project_root)\n",
        "\n",
        "storyboard_fname = root / 'storyboard.yaml'\n",
        "storyboard = OmegaConf.load(storyboard_fname)\n",
        "\n",
        "fps = storyboard.params.fps\n",
        "input_audio = storyboard.params.audio_fpath\n",
        "\n",
        "output_filename = 'output.mp4' # @param {type:'string'}\n",
        "output_filename = str( root / output_filename )\n",
        "storyboard.params.output_filename = output_filename\n",
        "\n",
        "# to do: read frames and variations back into memory. This should be the last cell that gets run, so we need to \n",
        "# update state wrt any user interventions in the storyboard object. actually, should probably do the text overlay step here\n",
        "\n",
        "\n",
        "cmd_in = ['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-']\n",
        "cmd_out = ['-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '1', '-preset', 'veryslow', '-shortest', output_filename]\n",
        "\n",
        "if input_audio:\n",
        "  cmd_in += ['-i', str(input_audio)]\n",
        "\n",
        "cmd = cmd_in + cmd_out\n",
        "\n",
        "p = Popen(cmd, stdin=PIPE)\n",
        "#for im in tqdm(chain(frames)):\n",
        "for im in tqdm(frames):\n",
        "  im.save(p.stdin, 'PNG')\n",
        "p.stdin.close()\n",
        "\n",
        "print(\"Encoding video...\")\n",
        "p.wait()\n",
        "print(\"Video complete.\")\n",
        "print(f\"Video saved to: {storyboard.params.output_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title # 9. üì∫ Enjoy your animation!\n",
        "\n",
        "output_filename = storyboard.params.output_filename\n",
        "\n",
        "download_video = False # @param {type:'boolean'}\n",
        "compress_video = False # @param {type:'boolean'}\n",
        "\n",
        "# @markdown Compressing to `*.tar.gz`` format can reduce filesize, which in turn reduces\n",
        "# @markdown your download time. You may need to install additional software\n",
        "# @markdown to \"decompress\" the file after downloading to view your video.\n",
        "\n",
        "\n",
        "#  NB: only embed short videos\n",
        "embed_video_in_notebook = False\n",
        "\n",
        "if compress_video:\n",
        "    uncompressed_fname = output_filename\n",
        "    output_filename = f\"{output_filename}.tar.gz\"\n",
        "    print(f\"Compressing to: {output_filename}\")\n",
        "    !tar -czvf {output_filename} {uncompressed_fname}\n",
        "\n",
        "if download_video:\n",
        "    from google.colab import files\n",
        "    files.download(output_filename)\n",
        "\n",
        "if embed_video_in_notebook:\n",
        "    from IPython.display import display, Video\n",
        "    display(Video(output_filename, embed=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öñÔ∏è I put on my robe and lawyer hat\n",
        "\n",
        "### Notebook license\n",
        "\n",
        "This notebook and the accompanying [git repository](https://github.com/dmarx/video-killed-the-radio-star/) and its contents are shared under the MIT license.\n",
        "\n",
        "<!-- Note to self: lawyers should really be forced to use some sort of markup or pseudocode to eliminate ambiguity \n",
        "\n",
        "...oh shit, if laws were actually described in code, we could just run queries against it\n",
        "-->\n",
        "\n",
        "```\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2022 David Marx\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "```\n",
        "\n",
        "### DreamStudio API TOS\n",
        "\n",
        "The default behavior of this notebook uses the [DreamStudio](https://beta.dreamstudio.ai/) API to generate images. Users of the DreamStudio API are subject to the DreamStudio usage terms: https://beta.dreamstudio.ai/terms-of-service\n",
        "\n",
        "### Stable Diffusion\n",
        "\n",
        "As of the date of this writing (2022-09-29), all publicly available model checkpoints are subject to the restrictions of the Open RAIL license: https://huggingface.co/spaces/CompVis/stable-diffusion-license. \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
